\documentclass[a4paper, 10pt]{article}
%\usepackage[utf8]{inputenc}			
\usepackage[ngerman]{babel}		% for german	
\usepackage{graphicx}
\usepackage{psfrag}								
\usepackage{parskip}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{empheq}
\usepackage{titlesec}	
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{epstopdf}
\usepackage{dsfont}
\usepackage{sectsty}
\usepackage{ latexsym }
\allsectionsfont{\bfseries\sffamily}

\addtolength{\textwidth}{2.1cm}
\addtolength{\topmargin}{-1.4cm}
\addtolength{\oddsidemargin}{-1.1 cm}
\definecolor{leichtgrau}{gray}{0.91}
\setlength{\parindent}{0pt}

% \lstset{language = C,
	% basicstyle=\footnotesize,       
	% numbers=left,                  
	% numberstyle=\footnotesize,      
	% stepnumber=2,
	% numbersep=5pt,
	% backgroundcolor=\color{leichtgrau},
	% frame=single,
% }

% Definition von römischen Zahlen
\makeatletter
\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand{\Rmnum}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
%\renewcommand{\familydefault}{\sfdefault}
\title{MIMO Skript\,-\,Wintersemester 2012/13 \\ Kapitel 2\,-\,4}
\date{}

\begin{document}
\maketitle
\tableofcontents
\setcounter{section}{1}
\section{Point\,-\,to\,-\,Point MIMO Systems}
\subsection{MIMO Channel Capacity}
Inhalt: s. Skript in VL ausgeteilt
\subsection{SIMO Systems}
\paragraph{Remarks}
\begin{itemize}
	\item In SIMO Systems only \underline{coding} and \underline{diversity} \underline{gains} can be exploited (no multiplexing gains)
	\item To realize these gains diversity combining has to be performed
	\item Diversity combining schemes vary in complexity and performance
	\item There are \underline{many} diversity combining schemes. Here we consider:
	\begin{itemize}
		\item Maximal ratio combining (MRC)
		\item Equal gain combining (EGC)
		\item Selection combining (SC)
	\end{itemize}
	\item Diversity combining problem
	\begin{figure}[ht]
		\centering
		\psfrag{h_1}[hl][hl]{$h_1$}
		\psfrag{n_1}[hl][hl]{$n_1$}
		\psfrag{w_1}[hl][hl]{$h_1$}
		\psfrag{h_NR}[hl][hl]{$h_{N_R}$}
		\psfrag{n_NR}[hl][hl]{$n_{N_R}$}
		\psfrag{w_NR}[hl][hl]{$w_{N_R}$}
		\psfrag{x_dach}[hl][hl]{$\hat{x}$}
		\psfrag{x}[hl][hl]{$x$}		
		\includegraphics[width=0.8\textwidth]{Multi_Channel}
		\caption{Block Diagramm for SIMO}
		\label{fig:Multi_Channel}	
	\end{figure}		
	
	\begin{itemize}
		\item how to choose combining weights $w_n$?
		\item what performance (e.g. error rate, outage probability) is achieved?
		\item what \underline{diversity} and coding/combining gain is achieved?
	\end{itemize}
\end{itemize}

\begin{figure}
\begin{minipage}[hbt]{7cm}
	
		\centering
		\psfrag{GC_SNR_GD}[hl][hl]{$(G_c\cdot\text{SNR})^{-G_d}$}
		\includegraphics[width=7cm]{SIMO_BER_SNR_Kurve}
		\caption{Exemplary BER for SIMO}
		\label{fig:SIMO_BER_SNR_Kurve}
	
\end{minipage}
\hfill
\begin{minipage}[hbt]{5cm}
	\centering
		\begin{itemize}
			\item $G_c$ : Coding gain
			\item $G_d$ : Diversity gain
		\end{itemize}
\end{minipage}
\end{figure}

\subsubsection{Preliminaries}
Consider an equivalent system:
\begin{align*}
	y &= hx +n;\\
	\mathcal{E}\{|x^2|\} &= \mathcal{E}_s; & \mathcal{E}\{|n^2|\} &= \sigma_n^2; & \mathcal{E}\{|h|^2\} &= 1
\end{align*}
\begin{itemize}
	\item Instantaneous SNR: $\gamma_t = \frac{\mathcal{E}_s}{\sigma_n^2}\cdot |h|^2$
\item Average SNR: $\bar{\gamma}_t = \mathcal{E}\{\gamma_t\} = \frac{\mathcal{E}_s}{\sigma_n^2}$
\end{itemize}
\paragraph{Bit and Symbol Error Rate}
\begin{itemize}
	\item The Bit and Symbol Error Rate of many modulation schemes can be expressed for given $\gamma_t$ as:
\end{itemize}
\begin{align*}
	P_e(\gamma_t) = aQ\bigl( \sqrt{b\gamma_t}\bigr)
\end{align*}
where:
\begin{itemize}
	\item $Q(x) = \frac{1}{\sqrt{2\pi}}\cdot\int_{x}^{\infty} e^{-\frac{t^2}{2}}~dt$
	\item $P_e(\gamma_t)$ may be exact result or approximation
	\item BPSK: exact with $a = 1, b = 2$
	\item M-ary QAM: tight approximation with $a = 4\bigl (1-\frac{1}{\sqrt{M}}\bigr ), b =  \frac{3}{M - 1}$
\end{itemize}
\begin{math} \bigl ({\small Einschub: Gray-Code: {BER} = \frac{1}{\log_2M} \cdot {SER} }\bigr ) \end{math}
\begin{itemize}
	\item Alternative representation of Q\;-\;function:
			\begin{align*} 
			Q(x) = \frac{1}{\pi}\int_{0}^{\frac{\pi}{2}} e^{-\frac{x^2}{2\sin^2\theta}}~d\theta
			\end{align*}
			$\rightarrow$ Integral limits are fixed and do not depend on integration variables!
	\item Average error probability
		\begin{align*} 
		P_e = \mathcal{E}\bigl \{P_e(\gamma_t)\bigr \} = \int_{0}^{\infty}aQ\bigr (\sqrt{bx}\bigl )p_{\gamma_t}(x)~\mathrm{d}x
		\end{align*}
		\begin{itemize}
			\item Integral may be difficult to solve analytically
			\item Integral has infinite support $\rightarrow$ numerical evaluation difficult
		\end{itemize}
	\item Using alternative representation of Q-function we get:
		\begin{align*}
			P_e &= \int_{0}^{\infty}\frac{a}{\pi}\int_{0}^{\frac{\pi}{2}}e^{-\frac{bx}{2\sin^2\theta}}p_{\gamma_t}(x)~d\theta ~dx\\
			&= \frac{a}{\pi}\int_{0}^{\frac{\pi}{2}}\int_{0}^{\infty}p_{\gamma_t}(x)e^{-\frac{b}{2\sin^2\theta}x}~dx~d\theta &= \frac{a}{\pi}\int_{0}^{\frac{\pi}{2}}M_{\gamma_t}\bigl ( \frac{b}{2\sin^2\theta} \bigr )~d\theta
		\end{align*}
where:
		\begin{itemize}
				\item $M_{\gamma_t}(s) = \int_{0}^{\infty}p_{\gamma_t}(x)e^{-sx}~dx$ is the Laplace transform of $p_{\gamma_t}$	
				\item $M_{\gamma_t}(-s)$ is the so called Moment Generation Function (MGF) of $p_{\gamma_t}$
				\item Here, we will also refer to $M_{\gamma_t}(s)$ as MGF
				\item $M_{\gamma_t}(s)$ is sometimes easier to obtain than $p_{\gamma_t}$
				\item The above integral can be easily evaluated numerically because of the finite integral limits
			\end{itemize}
\end{itemize}
\paragraph{Outage probability}
\begin{itemize}
	\item The outage probability is the probability that the channel cannot support a certain rate, R, i.e. (where \begin{math} \gamma_T \end{math} is the threshold SNR):
		\begin{align*}
			C = \log_2(1+\gamma_t) < R \quad \leftrightarrow \quad \gamma_t < 2^R - 1 \hat{=} \gamma_T
		\end{align*}\\
		Thus, the outage probability is given by:
		\begin{align*}
			P_{out} &= P_0 \{\gamma_t <\gamma_T\}  \,= \int_{0}^{\gamma_T}p_{\gamma_t}(x)~dx
		\end{align*}
\item Using the inverse Laplace Transform\\
\begin{minipage}[hbt]{7cm}
	\centering
		\begin{align*}
			p_{\gamma  _t}(x) = \frac{1}{2\pi j}\int_{c-j\omega}^{c+j\omega}M_{\gamma _t}(s)e^{sx}~dx
		\end{align*}
	where \begin{math} c > 0\end{math} is a small constant that lies in the region of convergence of the integral, we obtain:
\end{minipage}
\hfill
\begin{minipage}[hbt]{5cm}
	\centering
	\includegraphics[width=2cm]{SIMO_Konstante_c}
\end{minipage}

\begin{itemize}
	\item 1.
			\begin{align*}
				P_{out} = \frac{1}{2\pi j}\int_{c-j\omega}^{c+j\omega}M_{\gamma _t}(s)\int_{0}^{\gamma _T}e^{sx}~dx~ds = \frac{1}{2\pi j}\int_{c-j\omega}^{c+j\omega}M_{\gamma _t}(s)e^{\gamma _Ts}~\frac{ds}{s}
			\end{align*}
			\begin{math}\bigl (\end{math}lower integral limit is 0 since \begin{math}p_{\gamma _t}(0) = 0 \bigr )\end{math}
	\item and 2.:
		\begin{align*}
			p_{\gamma _t} (x) &= \int_{0}^{x} p_{\gamma _t}(t) ~dt = 0\\		
		\text{for } x &= 0 \text{ note: } p_{\gamma _t}(x) \xleftrightarrow [transform]{Laplace} \frac{1}{s}M_{\gamma _t}(s)
		\end{align*}
	\end{itemize}
\end{itemize}
\paragraph{General combining scheme}
\begin{align*}
	y &= \Bigl (\sum_{n = 1}^{N_R}h_nw_n\Bigr )x + \sum_{n = 1}^{N_R}w_nn_n \\
	\gamma _t &= \frac{\mathcal{E}_s \Bigl | \sum_{n = 1}^{N_R}h_nw_n\Bigr |^2 }{\sigma _n^2 \sum_{n = 1}^{N_R}|w_n|^2}
\end{align*}
where \begin{math} w_n\end{math} depends on the particular combining scheme.
\subsubsection{MRC (Maximum Ratio Combining)}
\begin{itemize}
	\item what weight \begin{math}w_n\end{math} maximize \begin{math}\gamma _t\end{math}?
	\begin{itemize}
		\item Cauchy-Schwarz inequality
		\begin{align*}
			\Bigl | \sum_{n = 1}^{N_R}h_nw_n\Bigr |^2 \leq \sum_{n = 1}^{N_R}|h_n|^2 \cdot \sum_{n = 1}^{N_R}|w_n|^2
		\end{align*}
	where equality holds if and only if \begin{math}w_n = c\cdot h_n^*\end{math} for some non-zero constant \begin{math}c\end{math}.
		\item for \begin{math}w_n = h_n^*\end{math}, we obtain
		\begin{align*}
			\gamma _t = \frac{\mathcal{E}_s}{\sigma _n^2}\cdot \frac{\Bigl ( \sum_{n = 1}^{N_R}|h_n|^2\Bigr ) ^2}{\sum_{n = 1}^{N_R}|h_n|^2} = \frac{\mathcal{E}_s}{\sigma _n^2}\sum_{n = 1}^{N_R}|h_n|^2
		\end{align*}
		\item \begin{math}w_n = h_n^*\, \forall \, n\end{math} are the MRC combining weights.	
	\end{itemize}
	\item For performance analysis we assume \underline{independent identically distributed (IID)} Rayleigh fading
		\begin{align*}
			\rightarrow \mathcal{E}\{|h_n|^2\} &= 1; \quad \bar{\gamma} = \frac{\mathcal{E}_s}{\sigma	_n^2}; \quad \gamma _n = \frac{\mathcal{E}_s}{\sigma _n^2}|h_n|^2\\
			p_{\gamma}(x) &= \frac{1}{\bar{\gamma}}e^{-\frac{x}{\bar{\gamma}}}; \quad x\geq 0\\
			M_{\gamma}(s) &= \frac{1}{1+s\bar{\gamma}}
		\end{align*}		
	\item Error rate
			\begin{align*}
				\gamma _t = \sum_{n = 1}^{N_R}\gamma _n
			\end{align*}
			\begin{math}\rightarrow \end{math} sum of IID random variables (r.v.s.)
			\begin{align*}
				M_{\gamma _t}(s) = \Bigl (M_\gamma (s)\Bigr )^{N_R} = \frac{1}{(1 + s\bar{\gamma})^{N_R}} = \frac{1}{\bar{\gamma}^{N_R}}\cdot \frac{1}{(s + \frac{1}{\bar{\gamma}})^{N_R}}
			\end{align*}
			inverse Laplace-transform (from tables)
			\begin{align*}
				p_{\gamma _t}(x) = \frac{1}{\bar{\gamma}^{N_R}}\cdot \frac{x^{N_R-1}}{(N_R - 1)!}e^{-\frac{x}{\bar{\gamma}}};\quad x\geq 0
			\end{align*}
	\item Direct approach 
		\begin{align*}
			P_e &= \int_{0}^{\infty}a\cdot Q\bigl (\sqrt{ax} \bigr )p_{\gamma _t}(x)~dx = a\Bigl (\frac{1 - \mu}{2}\Bigr )^{N_R} \cdot \sum_{n = 0}^{N_R - 1}\begin{pmatrix} N_R - 1 + n \\ n\end{pmatrix}\Bigl (\frac{1 + \mu}{2}\Bigr )^n \\ \text{where}\; \mu &= \sqrt{\frac{b\bar{\gamma}}{2 + b\bar{\gamma}}}
		\end{align*}
		\item MGF approach
			\begin{align*}
				P_e &= \frac{a}{\pi}\int_{0}^{\frac{\pi}{2}}M_{\gamma _t}\Bigl ( \frac{b}{2\sin ^2\theta}\Bigr )~d\theta \\
				&= \frac{a}{\pi}\int_{0}^{\frac{\pi}{2}}\frac{1}{\bar{\gamma}^{N_R}\bigl (\frac{b}{2\sin ^2\theta} + \frac{1}{\bar{\gamma}}\bigr )^{N_R}}~d\theta \quad \text{(numerisch berechnen!)}
			\end{align*}
			\item with high SNR: \quad \begin{math}\bar{\gamma}\rightarrow \infty \Longleftrightarrow \frac{1}{\bar{\gamma}}\rightarrow 0 \end{math} and with MGF approach this leads to Average Error Probability $P_e$:
	\begin{align*}
		P_e &= \frac{a}{\pi}\cdot\frac{1}{\bar{\gamma}^{N_R}}\cdot \Bigl (\frac{2}{b}\Bigr )^{N_R}\int_{0}^{\frac{\pi}{2}}\sin ^{2N_R}\theta ~d\theta \\
		&= \frac{a}{2^{N_R+1}\cdot b^{N_R}}\begin{pmatrix}2N_R \\ N_R\end{pmatrix}\frac{1}{\bar{\gamma}^{N_R}}\quad \text{as } \bar{\gamma} \rightarrow \infty \\ 
		&\stackrel{!}{=} \Bigl (\frac{1}{G_c\bar{\gamma}}\Bigr )
	\end{align*}	
	where: 
	\begin{itemize}
		\item $ \int_{0}^{\frac{\pi}{2}}\sin ^{2N_R}\theta ~d\theta = \frac{\pi}{2^{N_R + 1}}\cdot \begin{pmatrix} 2N_R \\ N_R \end{pmatrix}$
		\item Diversity gain: $ G_d = N_R $ 
		\item Combining/Coding gain: $ G_c = 2b\biggl (\frac{a}{2}\begin{pmatrix}2N_R \\ N_R\end{pmatrix}\biggr )^{-\frac{1}{N_R}} $
	\end{itemize}		
	\begin{figure}[ht]
		\centering
		\psfrag{G_C}[hl][hl]{$G_c$}
		\psfrag{gamma_Mittelwert}[hl][hl]{$\bar{\gamma}$}
		\includegraphics[width = 0.7\textwidth]{SIMO_BER_gamma_Kurve}
		\caption{BER for average SNR $\bar{\gamma_t}$}
		\label{fig:SIMO_BER_gamma_Kurve}
	\end{figure}
	\item MRC exploits the maximal possible diversity
	\item Diversity gain is not affected by correlation as the branches are not \underline{fully} correlated
	\item Diversity gain depends on fading distribution 
\end{itemize}
\paragraph{Outage probability}
\begin{align*}
	P_{out} &= \int_{0}^{\gamma _T}p_{\gamma _t}(x)~dx = \frac{1}{\bar{\gamma}^{N_R}}\int_{0}^{\gamma _T}\frac{x^{N_R-1}}{(N_R - 1)!}e^{-\frac{x}{\bar{\gamma}}}~dx \\ &= 1- e^{-\frac{\gamma _T}{\bar{\gamma}}}\cdot \sum_{n = 1}^{N_R}\frac{\bigl (\frac{\gamma _T}{\bar{\gamma}}\bigr )^n}{(n - 1)!}
\end{align*}
\begin{itemize}
	\item Approximation (Taylor series): $\bar{\gamma}\rightarrow \infty : -e^{-\frac{x}{\bar{\gamma}}} = 1 - \frac{x}{\bar{\gamma}} + O(\frac{1}{\bar{\gamma}})$\; where a function $f(x) \text{ is } O(x) \text{ if } \lim\limits_{x\to\infty}\frac{f(x)}{x} = 0$.
\begin{align*}
	\Rightarrow P_{out}=\frac{1}{\gamma^{N_R}}\int\limits_0^{\gamma_T}\frac{x^{N_R-1}}{(N_R-1)!}\left(1-\frac{x}{\bar{\gamma}}+O\left(\frac{1}{\bar{\gamma}}\right)\right)
\end{align*}
	\item Diversity and coding gain can also be defined for $P_{out}$
\end{itemize}

\subsubsection{EGC (Equal Gain Combining)}

\paragraph{Combining Weights}
\begin{itemize}
	\item For MRC, both, the amplitudes and phases of the channel gains $h_n=|h_n|e^{j\varphi_n}$ have to be known (or estimated in practice)
	\item In EGC it is assumed that only the phases are known and weights $w_n=e^{-j\varphi_n}$ are used.
\begin{align*}
	\Rightarrow \gamma_t &=\dfrac{\mathcal{E}_s}{\sigma_n^2}\dfrac{\left|\sum\limits_{n=1}^{N_R}|h_n|e^{j\varphi_n}e^{-j\varphi_n}\right|^2}{\sum\limits^{N_R}_{n=1}\left|e^{-j\varphi_n}\right|^2}
	=\frac{\mathcal{E}_s}{\sigma_n^2}\frac{1}{N_R}\left(\sum\limits^{N_R}_{n=1}|h_n|\right)^2\\
	&=\frac{1}{N_R}\left(\sum\limits_{n=1}^{N_R}\sqrt{\gamma_n}\right)^2;\text{  with  }\gamma_n=\frac{\mathcal{E}_s}{\sigma_n^2}|h_n|^2
\end{align*}
\end{itemize}
\paragraph{Performance Analysis}
\begin{itemize}
	\item IID case \\
	$\Rightarrow$ $\sqrt{\gamma_n}$ is Rayleigh distributed\\
	$\Rightarrow$ Exact analysis is much more difficult than for MRC $\Rightarrow$ see book by Simon\;\&\;Alouini p.341
	\item Approximate result
\begin{align*}
	P_e=\frac{a}{2}\left[1-\sqrt{\frac{2b\bar{\gamma}}{5+2b\bar{\gamma}}}\sum\limits_{n=0}^{N_R-1}\frac{\left(\! \begin{array}{c} 2n \\ n \end{array} \!\right) }{4^n(1+\frac{2}{5}b\bar{\gamma})^n}\right]
\end{align*}
	\item high SNR\\
	$\Rightarrow$ use high SNR analysis of Wang\;\&\;Giannakis, 2003\\
	$\Rightarrow$ at high SNR, only pdf of $\gamma_n$ around $0$ is relevant for performance
\begin{align*}
	\Rightarrow \overset{\text{Rayleigh}}{p_\gamma(x)} = \frac{1}{\bar{\gamma}}e^{-\frac{x}{\bar{\gamma}}}\overset{\text{Taylor Serie}}{=}\frac{1}{\bar{\gamma}}+O\left(\frac{1}{\bar{\gamma}}\right)\text{ as } x \to 0
\end{align*}
	\item need pdf $\gamma _t$: ($\gamma _n$ bekannt, $\rightarrow$ ges.: Wurzel, etc.)\\ (cumulative distribution function of $\sqrt{\gamma} $ (cdf))
	\begin{align*}
	% 1. Formel
		P_{\sqrt{\gamma}}(x) &= \text{Pr}\bigl\{\sqrt{\gamma}\leq x \bigr\} = \text{Pr}\bigl\{\gamma\leq x^2 \bigr\} = P_{\gamma}(x^2) = \text{cdf of}\;\gamma\\
	% 2. Formel
		\rightarrow p_{\sqrt{\gamma}}(x) &= \frac{d}{dx}P_{\sqrt{\gamma}}(x) = 2x\cdot p_{\gamma}(x^2) = \frac{2x}{\bar{\gamma}} + O\bigl(\frac{1}{\bar{\gamma}}\bigr)\\
	\end{align*}
	\item Laplace Transformation to MGF
	\begin{align*}
	% 3. Formel
		\rightarrow M_{\sqrt{\gamma}}(s) &= \mathcal{L}\bigl\{p_{\sqrt{\gamma}}(x)\bigr\} = \frac{2}{\bar{\gamma}}\cdot \frac{1}{s^2} + O\bigl(\frac{1}{\bar{\gamma}}\bigr)\\
	% 4. Formel
		\sqrt{\gamma _t} &= \sum_{n = 1}^{N_R}\frac{\sqrt{\gamma _n}}{N_R}\\
	% 5. Formel
		 M_{\sqrt{\gamma _t}}(s) &= \mathcal{E}\Bigl\{\mathrm{exp}({-s\sqrt{\gamma _t}})\Bigr\} = \mathcal{E}\Bigl\{\mathrm{exp}({-\frac{s}{\sqrt{N_R}}\cdot \sum_{n = 1}^{N_R}\sqrt{\gamma _n}})\Bigr\} = \Bigl(\mathcal{E}\Bigl\{\mathrm{exp}(-\frac{s}{\sqrt{N_R}}\cdot \sqrt{\gamma _n}\Bigr\}\Bigr)^{N_R}\\
	% 6. Formel
		 &= \Bigl( M_{\sqrt{\gamma}}\bigl(\frac{s}{\sqrt{N_R}}\bigr) \Bigr)^{N_R} = \Bigl(\frac{2}{\bar{\gamma}}\cdot \frac{N_R}{s^2}\Bigr)^{N_R} + O\Bigl(\frac{1}{\bar{\gamma}^{N_R}}\Bigr)\\
	\end{align*}
	\item inverse Laplace Transform
	\begin{align*}
	% 7. Formel
		p_{\sqrt{\gamma _t}}(x) &= \mathcal{L} ^{-1}\Bigl\{M_{\sqrt{\gamma _t}}(s)\Bigr\} = \Bigl(\frac{2N_R}{\bar{\gamma}}\Bigr)^{N_R}\cdot \frac{x^{2N_R-1}}{(2N_R-1)!} + O\Bigl(	\frac{1}{\bar{\gamma}^{N_R}}\Bigr)\\
	% 8. Formel
		P_{\gamma _t}(x) &= \text{Pr}\bigl\{\gamma _t\leq x	\bigr\} =  \text{Pr}\bigl\{\sqrt{\gamma _t}\leq \sqrt{x}\bigr\} = P_{\sqrt\gamma _t}(\sqrt{x}) \rightarrow \text{cdf of } \sqrt{\gamma _t}\\
	% 9. Formel
		p_{\gamma _t}(x) &= \frac{d}{dx}P_{\gamma _t}(x) = \frac{1}{2\sqrt{x}}\cdot p_{\gamma _t}(\sqrt{x}) = \frac{1}{2}\Bigl(\frac{2N_R}{\bar{\gamma}} \Bigr)^{N_R}\cdot \frac{x^{N_R-1}}{(2N_R-1)!} + O\bigl(\bar{\gamma}^{-N_R}\bigr)\\
	% 10. Formel
		\rightarrow M_{\gamma _t}(s) &= \mathcal{L}\bigl\{p_{\gamma _t}(x)\bigr\} = \frac{1}{2}\Bigl( \frac{2N_R}{\bar{\gamma}}\Bigr)^{N_R}\cdot \frac{(N_R - 1)!}{(2N_R -1)!~b^{N_R}} + O\bigl(\bar{\gamma}^{-N_R}\bigr)\\
	% 11. Formel
	\end{align*}
	\item Error Probability:
\begin{align*}
        P_e&=\frac{a}{\pi}\int\limits_0^{\frac{\pi}{2}}M_{\gamma_t}\left(\frac{b}{2\sin^2(\theta)}\right)\mathrm{d}\theta\\
        &=\frac{a}{\pi}\frac{1}{2}\left(\frac{2N_R}{\bar{\gamma}}\right)^{N_R}\frac{(N_R-1)!}{(2N_R-1)!}
        \frac{2^{N_R}}{b^{N_R}}
        \underbrace{\int\limits_0^{\frac{\pi}{2}}\sin^{2N_R}(\theta)\mathrm{d}\theta}_
        {\frac{\pi}{2^{2N_R+1}}\binom{2N_R}{N_R}=\frac{\pi(2N_R)!}{2^{2N_R+1}(N_R!)^2}}
        +O\left(\frac{1}{\bar{\gamma}^{N_R}}\right)\\
        &= \frac{aN_R^{N_R}}{2b^{N_R}N_R!}\frac{1}{\bar{\gamma}^{N_R}}+O\left(\frac{1}{\bar{\gamma}^{N_R}}\right)
        \overset{!}{=}\left(\frac{1}{G_c\cdot\bar{\gamma}}\right)^{G_d}
\end{align*}

\begin{itemize}
		\item[] $\Longrightarrow \text{Diversity gain: } G_d = N_R$
		\item[] $\Longrightarrow \text{Combining gain: } G_c = \frac{b}{N_R}\Bigl( \frac{2N_R!}{a}\Bigr)^{\frac{1}{N_R}}$
		\item[] vergleiche auch Blatt mit Kurven \Rmnum{3} und \Rmnum{4}
	\end{itemize}
\end{itemize}
A similar asymptotic analysis can be conducted for the outage probability.
\subsubsection{SC (Selection Combining)}
\paragraph{Combining weights}
	\begin{itemize}
		\item only the strongest branch is chosen
		\item strongest branch: $\hat{n} = \underset{n}{\text{argmax}} \gamma _n \longrightarrow \gamma _t = \gamma _{\hat{n}}$
		\item only on RF receiver chain required $\rightarrow$\; saves hardware complexity 
	\end{itemize}
\paragraph{Performance analysis}
	\begin{itemize}
		\item cdf of: $\gamma _t$
			\begin{align*}
				P_{\gamma _t}(x) &= \text{Pr}\bigl\{\gamma _{\hat{n}} \leq x\bigr\} = \text{Pr}\bigl\{\gamma _1 \leq x \cap \gamma _2 \leq x \cap \dots \gamma _{N_R} \leq x\bigr\}\\
				&\overset{(IID)}{=} \Bigl(\text{Pr}\bigl\{\gamma _n \leq x\bigr\}\Bigr)^{N_R} = \Bigl( P_{\gamma}(x)\Bigr)^{N_R}
			\end{align*}
		\item pdf:
			\begin{align*}
				p_{\gamma _t}(x) &= \frac{d}{dx}P_{\gamma _t}(x) = N_R\bigl(P_{\gamma}(x)\bigr)^{N_R-1}\cdot p_{\gamma}(x)\\
				\text{where: }\qquad p_{\gamma} (x) &= \frac{1}{\bar{\gamma}}e^{-\frac{x}{\bar{\gamma}}};\quad x\geq 0\\
				P_{\gamma}(x) &= \int_{0}^{x}p_{\gamma}(x)~dx = 1 - e^{-\frac{x}{\bar{\gamma}}};\quad x\geq 0\\
				\rightarrow p_{\gamma _t}(x) &= \frac{N_R}{\bar{\gamma}}\bigl( 1 - e^{-\frac{x}{\bar{\gamma}}}\bigr)^{N_R-1}e^{-\frac{x}{\bar{\gamma}}};\quad x\geq 0
			\end{align*}
	\end{itemize}
\paragraph{Error probability}
	\begin{itemize}
		\item direct approach $\rightarrow$ closed-form solution possible
		\item MGF approach (with Binomial expansion):
			\begin{align*}
				p_{\gamma _t}(x) &= \frac{N_R}{\bar{\gamma}}e^{-\frac{x}{\bar{\gamma}}}\sum_{n = 0}^{N_R - 1}\begin{pmatrix} N_R - 1 \\ n\end{pmatrix} 1^{N_R - 1 - n}\Bigl( -e^{-\frac{x}{\bar{\gamma}}}\Bigr)^n \\ 
				&= \frac{N_R}{\bar{\gamma}}\sum_{n = 0}^{N_R - 1}\begin{pmatrix}N_R - 1 \\ n\end{pmatrix}\cdot (-1)^{n}e^{-\frac{x(n + 1)}{\bar{\gamma}}};\quad	x \geq 0 \\ \\
				M_{\gamma_t}(s) &= \frac{N_R}{\bar{\gamma}}\sum_{n = 0}^{N_R - 1}\begin{pmatrix}N_R - 1\\n\end{pmatrix}(-1)^{n}\frac{1}{s + \frac{n+1}{\bar{\gamma}}}\\ \\
				P_e &= \frac{a}{\pi}\int_{0}^{\frac{\pi}{2}}M_{\gamma _t}\Bigl(\frac{b}{2\sin^2\theta}\Bigr)~d\theta = \frac{aN_R}{\pi\bar{\gamma}}\sum_{n = 0}^{N_R - 1}\begin{pmatrix}N_R - 1\\ n\end{pmatrix} (-1)^{n}\int_{0}^{\frac{\pi}{2}}\frac{\mathrm{d}\theta}{\frac{b}{2\sin^2\theta} + \frac{n + 1}{\bar{\gamma}}}\\ &\rightarrow\text{can be evaluated numerically}				
			\end{align*}
		\item high SNR approach $\Rightarrow \; \bar{\gamma}\rightarrow\infty$
			\begin{align*}
			p_{\gamma_t}
			&=\frac{N_R}{\bar{\gamma}}
			\left[1-\mathrm{exp}\left(-\frac{x}{\bar{\gamma}}\right)\right]^{N_R-1}\mathrm{exp}\left(-\frac{x}{\bar{\gamma}}\right)\\
			&\overset{\bar{\gamma} \rightarrow \infty}{=} \frac{N_R}{\bar{\gamma}}
			\left[1-\left(1-\frac{x}{\bar{\gamma}}+O\left(\bar{\gamma}^{-1}\right)\right)\right]^{N_R-1}
			\left(1-\frac{x}{\bar{\gamma}}+O\left(\bar{\gamma}^{-1}\right)\right)\\
			&=\frac{N_R}{\bar{\gamma}^{N_R}}x^{N_R-1}+o\left(\bar{\gamma}^{-N_R}\right)\\ \\
			M_{\gamma_t}(s)
			&=\frac{N_R}{\bar{\gamma}^{N_R}}\frac{(N_R-1)!}{s^{N_R}}+O\left(\bar{\gamma}^{-N_R}\right)\\
			\Bigl[\rightarrow P_e
			&=\frac{a}{\pi}\int\limits^{\frac{\pi}{2}}_0 M_{\gamma_t}\left(\frac{b}{2\sin^2(\theta)}\right)\mathrm{d}\theta\Bigr]\\
			&=\frac{a(2N_R)!}{b^{N_R}2^{N_R+1}N_R!}\frac{1}{\bar{\gamma}^{N_R}}+O(\bar{\gamma}^{-N_R})
			\end{align*}
		\begin{itemize}
			\item[] $\Longrightarrow \text{Diversity gain: } G_d = N_R$
			\item[] $\Longrightarrow \text{Combining gain: } G_c = 2b\left(\frac{2N_R!}{a(2N_R)!}\right)^{\frac{1}{N_R}}$
			%\item[] vergleiche auch Blatt mit Kurven \rom{III} und \rom{IV}
		\end{itemize}
	\end{itemize}

\paragraph{Outage Probability}
		\begin{align*}
			P_{out}&=\mathrm{Pr}\{\gamma_{\hat{n}} \leq \gamma_T \}=P_{\gamma_{\hat{n}}}(\gamma_T)=\left[1-\mathrm{exp}\left(-\frac{\gamma_T}{\bar{\gamma}}\right)\right]^{N_R}\\
			\text{high SNR: } P_{out}&=\left(\frac{\gamma_T}{\bar{\gamma}}\right)^{N_R}+O\left(\bar{\gamma}^{-N_R}\right)
		\end{align*}

\subsubsection{Comparison}
	\begin{itemize}
		\item Diversity Gain:\\
			MRC, EGC and SC all achieve the maximum possible diversity gain of $G_d=N_R$
		\item Combining Gain:\\
			The combining gains of MRC, EGC and SC are different
			\begin{itemize}
			\item MRC/EGC:
			\begin{align*}
			\frac{G_C^{EGC}}{G_C^{MRC}}=\frac{\frac{1}{2b}\left(\frac{a}{2}\binom{2N_R}{N_R}\right)^{\frac{1}{N_R}}}{\frac{N_R}{b}\left(\frac{a}{2}\frac{1}{N_R!}\right)^{\frac{1}{N_R}}}
			=\frac{[(2N_R)!]^{\frac{1}{N_R}}}{2N_R(N_R)^{\frac{1}{N_R}}}\leq 1\\
			\end{align*}
			(independent of a or b which are modulation parameters, only depends on number of antennas)
			\begin{align*}
				N_R \gg 1: \qquad N_R!\approx \sqrt{2\pi}e^{-N_R}N_R^{N_R+\frac{1}{2}}\qquad(Stirling)
			\end{align*}
			\begin{align*}
				\left.\frac{G_c^{EGC}}{G_c^{MRC}}\right|_{N_R\gg1}
				=\frac{\left(\sqrt{2\pi}e^{-2N_R}(2N_R)^{2N_R+\frac{1}{2}}\right)^{\frac{1}{N_R}}}{2N_R\left(\sqrt{2\pi}e^{-N_R}N_R^{N_R+\frac{1}{2}}\right)^{\frac{1}{N_R}}}
				=\frac{2\cdot2^{\frac{1}{2N_R}}}{2}\overset{N_R\rightarrow\infty}{\rightarrow}\frac{2}{e}\equiv -1.3\mathrm{dB}
			\end{align*}
			\item MRC/SC:
			\begin{align*}
				\frac{G_c^{SC}}{G_c^{MRC}}
				&=\frac{2b\left(\frac{a}{2}\binom{2N_R}{N_R}\right)^{\frac{1}{N_R}}}{2b\left(\frac{a}{2}\frac{(2N_R)!}{N_R!}\right)^{\frac{1}{N_R}}}
				=\frac{1}{(N_R!)^{\frac{1}{N_R}}} \leq 1\\
				\left.\frac{G_c^{SC}}{G_c^{MRC}}\right|_{N_R\gg1}&=\frac{1}{\sqrt{2\pi}^{\frac{1}{N_R}}e^{-1}N_R^{1+\frac{1}{2N_R}}}\overset{\rightarrow}{N_R\rightarrow\infty}\frac{e}{N_R}
			\end{align*}
			$\rightarrow$ loss increases with $N_R$
			\end{itemize}
		\item Ergebnis:
		\begin{itemize}
			\item unterschiedliche Kurven in Diagramm \glqq Combiner Performance Comparison, BPSK\grqq\ ergeben sich durch $G_c$
			\item alle Kurven haben die gleiche Steigung $\Rightarrow G_d $ ist \"uberall gleich
			\item nur Abst\"ande sind unterschiedlich $\Rightarrow G_c $ wird in MRC besser \glqq genutzt\grqq\ als in EGC und SC
		\end{itemize}
	\end{itemize}
	\newpage
\subsection{MISO Systems}
\paragraph{Remarks}
	\begin{itemize}
		\item Similar to SIMO systems, in MISO systems only coding and diversity gains can be obtained.
		\item To realize these gains, a careful transmitter design is necessary
		\item System design depends on whether or not channel state information (\textbf{CSI}) is available at transmitter
	\end{itemize}
\subsubsection{Naive Approach}
\begin{itemize}	
	\item Assume we simply send the same signal over all $N_T$ transmit antennas
\end{itemize}
\begin{figure}[ht]
	\centering
	\input{MISO_NAIVE.pstex_t}
	\caption{Block Diagramm of naive MISO}
	\label{fig:MISO_NAIVE.pstex_t}
\end{figure}

\begin{itemize}
	\item Transmit power: $\mathcal{E}\left\{\left|\frac{1}{\sqrt{N_T}}x\right|^2+,\dots,\left|\frac{1}{\sqrt{N_T}}x\right|^2\right\}=\mathcal{E}\left\{N_T\frac{1}{N_T}|x|^2\right\}=\mathcal{E}_s$
	\item Received signal: $y=\frac{1}{\sqrt{N_T}}\sum\limits_{n=1}^{N_T}h_n\cdot x+n$
	\item Rayleigh fading: $h_n$ are zero mean complex gaussian random variables\\
	$\rightarrow$ $h$ is also zero mean complex gaussian
	\item i.i.d.:
	\begin{itemize}
		\item $\mathcal{E}\{|h_n|^2\}=1\;\forall n$
		\item $\mathcal{E}\{|h|^2\}=\frac{1}{N_T}\mathcal{E}\left\{\left|\sum\limits_{n=1}^{N_T}h_n\right|^2\right\}=\frac{1}{N_T}\mathcal{E}\left\{\sum\limits^{N_T}_{n=1}|h_n|^2\right\}=1$
		\item statistical properties of h are independent of $N_T$
		\item the multiple transmit antennas have no benefit at all
		\item \underline{more sophisticated transmitter designs necessary}
	\end{itemize}
\end{itemize}
\subsubsection{Full CSI Available at the Transmitter}
\begin{itemize}
	\item $h_n, n \in \{1,\dots,N_T\}$ is known at the transmitter
	\item Perform ``precoding'' (beamforming) with coefficients $w_n$
\end{itemize}
\begin{figure}[ht]
	\input{MISO_CSI.pstex_t}	
	\caption{Block Diagramm of MISO with CSI}
	\label{fig:MISO_CSI.pstex_t}
\end{figure}

\begin{itemize}
	\item Transmit Power: Two constraints maybe considered
	\begin{itemize}
		\item Average transmit power constraint
		\begin{align*}
		P_{av}=\mathcal{E}\left\{\sum\limits^{N_T}_{n=1}|w_n^*x|^2\right\}=\sum\limits_{n=1}^{N_T}|w_n|^2\underbrace{\mathcal{E}\{|x|^2\}}_{\mathcal{E}_s}=\mathcal{E}_s \Rightarrow \sum\limits^{N_T}_{n=1}|w_n|^2=1
		\end{align*}
		\item Power constraint for each transmit antenna
		\begin{align*}
		\rightarrow |w_n|=\frac{1}{\sqrt{N_T}} \qquad \rightarrow P_{av}=\mathcal{E}_s
		\end{align*}
	\end{itemize}
	\item Recveived signal: $y=\underbrace{\sum\limits^{N_T}_{n=1}w^*_nh_n}_{h}x+n$ (equivalent SISO channel)
\end{itemize}
\paragraph{Maximum Ratio Transmission (MRT)}
\begin{itemize}
	\item we have only the average power constraint: $ \sum_{n = 1}^{N_T} |w_n|^2 = 1 $
	\item SNR:  $\gamma	 _t = \frac{\mathcal{E}_s|h|^2}{\sigma _n^2} = \frac{\mathcal{E}_s\left|\sum_{n = 1}^{N_T}w_n^*\cdot h_n \right|^2}{\sigma _n^2} $ 
	\item Maximize SNR under constraint $ \sum_{n = 1}^{N_T}|w_n|^2 = 1 $
	\item constraint optimization problem $\rightarrow $ Lagrange method
		\begin{align*}
			L = \frac{\mathcal{E}_s}{\sigma _n^2}\left| \sum_{n = 1}^{N_T}w_n^*\cdot h_n \right |^2 + \lambda\left ( \sum_{n = 1}^{N_T}|w_n|^2 - 1\right ); \quad \text{where: } \lambda = \text{Lagrange Multiplier}
		\end{align*}
		$\Rightarrow$  Wirtinger Kalk\"ul: treat  $z$  and  $z^*$  as independent variables for differentiation:
		\begin{align*}
			\frac{\partial z^*}{\partial z} &= 0;\quad  \frac{\partial |z|^2}{\partial z} = \frac{\partial z\cdot z^*}{\partial z} = z^*\\
			\frac{\partial x^2}{\partial x} &= 2x; \quad \frac{\partial (z^*)^2}{\partial z^*} = 2\cdot z^*;  \frac{\partial |z|^2}{\partial z} = z^*
		\end{align*}
		\begin{align*}
			\frac{\partial L}{\partial w_m^*} = \frac{\mathcal{E}_s}{\sigma _n^2}\Bigl ( \sum_{n = 1}^{N_T}w_n^*\cdot h_n\Bigr )^*h_m + \lambda w_m
		\end{align*}
		\begin{itemize}
			\item[$\rightarrow$] $   w_m =\underset{\text{const., independent of } m:=c}{\frac{\mathcal{E}_s}{\sigma _n^2 \cdot \lambda}\Bigl ( \sum\limits_{n = 1}^{N_T}w_n^*h_n \Bigr )^*}h_m $
			\item[$\rightarrow$] $  w_m = c\cdot h_m$
			\item[$\rightarrow$] $ \sum_{n = 1}^{N_T} |w_n|^2 = 1 \rightarrow c^2 = \frac{1}{\sum\limits_{n = 1}^{N_T}|h_n|^2}  $
			\item[$\rightarrow$] $  w_n = \frac{h_n}{\sqrt{\sum_{n = 1}^{N_T}|h_n|^2}} \equiv $ MRT gains 
		\end{itemize}
		\item[$\rightarrow$] SNR $ = \frac{\mathcal{E}_s}{\sigma _n^2}\Bigl | \sum_{n = 1}^{N_T}\frac{|h_n|^2}{\sqrt{\sum_{m = 1}^{N_T}|h_m|^2}}\Bigr |^2 = \frac{\mathcal{E}_s}{\sigma _n^2}\sum_{n = 1}^{N_T}|h_n|^2 $
		\item[$\Rightarrow$] same SNR as for maximum ration combining (MRC)
		\item[$\Rightarrow$] MRT with $N_T$ transmit antennas achieves the same performance as MRC with $N_T$ receive antennas
		\item[$\Rightarrow$] MRT/MRC can be extended to $ N_T\times N_R $ MIMO systems
		\begin{itemize}
			\item[$\rightarrow$] has the same performance as MRC with $N_T\cdot N_R$ receive antennas and one transmit antenna
		\end{itemize}
	\end{itemize}
\paragraph{Equal Gain Transmission (EGT)}
\begin{itemize}
	\item we employ gains: $w_n = \frac{1}{\sqrt{N_T}}\cdot \frac{h_n}{|h_n|} \rightarrow |w_n| = \frac{1}{\sqrt{N_T}} $
	\item SNR:
	\begin{align*}
		\gamma _t 
		&= \frac{\mathcal{E}_s}{\sigma _n^2}\left|\sum_{n = 1}^{N_T} w_n^*h_n \right |^2\\
		&= \frac{\mathcal{E}_s}{\sigma _n^2}\left|\sum_{n = 1}^{N_T}\frac{1}{\sqrt{N_T}}\cdot \frac{|h_n|^2}{|h_n|} \right|^2 = \frac{1}{N_T}\cdot \frac{\mathcal{E}_s}{\sigma _n^2} \left|\sum_{n = 1}^{N_T}|h_n| \right|^2\\
		\gamma _n 
		&= \frac{\mathcal{E}_s}{\sigma _n^2}|h_n|^2\\
		&\text{same SNR as for EGC} \rightarrow \gamma _t = \frac{1}{N_T} \left| \sum_{n = 1}{N_T}\sqrt{\gamma _n} \right|^2
	\end{align*}
	\item[$\rightarrow$] EGC with $ N_T$ transmit antennas achieves the same performance as EGC with $ N_T$ receive antennas
\end{itemize}
\paragraph{Transmit Antennas Selection}
\begin{itemize}
	\item select antenna with maximum channel gain for transmission:
		\begin{align*} 
			w_n = 
			\begin{cases}
\frac{h_n}{|h_n|},  & \text{if } n = \hat{n}\\
0, & \text{otherwise}
\end{cases} \text{where } \hat{n} = \underset{n}{\mathrm{argmax}}|h_n|
		\end{align*}
	\item antenna selection with $ N_T$ transmit antennas achieves the same performance as \textit{Selection Combining} with $ N_T$ receive antennas
\end{itemize}
\subsubsection{No CSI at Transmitter - Space\,-\,Time\,-\,Coding}
\begin{itemize}
	\item $h_n, n\in\{1,\dots ,N_T\}$, is only known at the receiver
	\item ``Space-time-coding'' has to be employed to realize diversity gain
	\item $T\times N_T$ matrics $\mathbf{X}$ are transmitted in $T$ symbol intervals over $N_T$ antennas
	\item $\mathbf{X}$ is drawn from a matrix alphabet $\mathcal{X}$
	\item Example:
	\begin{align*}
		\mathbf{X}=\begin{pmatrix} x_{1,1} & x_{1,2} & \cdots & x_{1,N_T} \\
					   x_{2,1} & x_{2,2} & \cdots & x_{2,N_T} \\
					   \vdots  & \vdots  & \ddots & \vdots    \\
				 	   x_{T,1} & x_{T,2} & \cdots & x_{T,N_T}
			\end{pmatrix}
	\end{align*}
	\item We distinguish:
		\begin{itemize}
			\item Space-time-block-codes (STBCs)\\
			$\rightarrow$ $\mathbf{X}$ is obtained by mapping $K$ scalar symbols $s_k,\; k=1, \dots , K$ from a scalar alphabet $\mathcal{A}$ to matrix $\mathbf{X}$
			\item Space-time-trellis-codes (STTCs)\\
			$\rightarrow$ $\mathbf{X}$ is obtained from scalar symbols $s_k$ through a trellis encoding process.\\
			{\small[see: Tarokh, Seshadri, Calderbank: Space-time-codes for high datarate wireless communication: Performance criterions and coder construction; IEEE Trans. Inf. Theory 1998]}
			\item here: We concentrate on space-time-block-codes (STBCs), but many results can be easily extended to space-time-trellis-codes
		\end{itemize}
	\item STBCs:
		\begin{itemize}
			\item  $K$ $M$-ary scalar symbols (e.g. $M$-PSK symbols) are mapped to STBC matrices $\mathbf{X}$\\ 
			$\mathbf{S}=[s_1,\dots ,s_K] \rightarrow \mathbf{X}$\\
			$s_k\in \mathcal{A} \rightarrow x\in\mathcal{X}$ with $|\mathcal{X}|=M^K$
			\item Example: ``Alamouti''-Code\\
			\begin{align*}
				\mathbf{X}=\frac{1}{\sqrt{2}}
				\begin{pmatrix}
					s_1 & s_2 \\
					-s_2^* & s_1^*
				\end{pmatrix}
			\end{align*}
			{\small[Alamouti: A simple transmit diversity technique for wireless communication, IEEE JSAC 1998]}
		\end{itemize}
\end{itemize}
\paragraph{Optimal Detection}
\begin{itemize}
	\item Signal model:
	\begin{align*}
	\begin{pmatrix} y_1 \\ \vdots \\ y_T \end{pmatrix}&=\mathbf{X}\begin{pmatrix} h_1 \\ \vdots \\ h_{N_T} \end{pmatrix} + \begin{pmatrix} n_1 \\ \vdots \\ n_T \end{pmatrix}\\
	\mathbf{y}&=\mathbf{X}\cdot\mathbf{h}+\mathbf{n}
	\end{align*}
	\item Optimal detection - ML-detection
	\begin{itemize}
		\item $\mathbf{h}$ is known at receiver
		\item $\mathbf{n}$ is AWGN with $\mathcal{E}\{\mathbf{n}\cdot\mathbf{n^H}\}=\sigma_{\mathbf{n}}^2\cdots \mathbf{I}_{T\times T}$
		\begin{align*}
		p(\mathbf{y}|\mathbf{X})
		&=\frac{1}{\pi^T|\sigma_n^2\mathbf{I}_{T\times T}|}\mathrm{exp}\left(-(\mathbf{y - Xh})^H(\sigma_n^2\mathbf{I}_{T\times T})^{-1}(\mathbf{y-Xh})\right)\\
		&=\frac{1}{\pi^T\sigma_n^{2T}}\mathrm{exp}\left(-\frac{1}{\sigma_n^2}(\mathbf{y-Xh})^H(\mathbf{y-Xh})\right)=\frac{1}{\pi^T\sigma_n^{2T}}\mathrm{exp}\left(||\mathbf{y-Xh}||^2\right)
		\end{align*}
		$\rightarrow$ the optimal estimate $\hat{\mathbf{X}}$ or equivalently the optimal estimate $\hat{\mathbf{s}}$ can be obtained as
		\begin{align*}
			\hat{\mathbf{s}}=\underset{\mathbf{s}\in\mathcal{A}^K}{\mathrm{argmax}}\;p(\mathbf{y|X})=\underset{\mathbf{s}\in\mathcal{A}^K}{\mathrm{argmin}}||\mathbf{y-Xh}||^2
		\end{align*}
		\item Disadvantage: In general, metric $||\mathbf{y-Xh}||^2$ has to be calculated $M^K$ times\\
		$\rightarrow$ complexity increases exponentially with $K$
	\end{itemize}
\end{itemize}
\paragraph{Types of STBCs}
\begin{itemize}
	\item Orthogonal STBCs (OSTBCs)
	\begin{itemize}
		\item OSTBCs are a special class of STBCs which allow independent detection of each $s_k \rightarrow$ only $K\cdot M$ metrics have to be evaluated
		\item Rate STBCs: $R_{STBC} = \frac{K}{T}$
		\item Examples:
		\begin{itemize}
			\item Alamouti Code  $(K = 2, T = 2) \rightarrow R_{STBC} = 1$
			\begin{align*}
				\textbf{X} &= \frac{1}{\sqrt{2}}\underset{\longleftrightarrow N_T}{\begin{pmatrix} s_1 & s_2 \\ -s_2^* & s_1^*	\end{pmatrix}}\updownarrow T\\
				\rightarrow &\text{ only ``full rate ´´ OSTBC for complex } s_k
			\end{align*}
			\item $N_T = 3, K = 3, T = 4$
			\begin{align*}
				\textbf{X} = \frac{1}{\sqrt{3}}\begin{pmatrix*}[r]	s_1 & s_2 & s_3 \\ -s_2^*  & s_1^* & 0 \\ s_3^* & 0 & -s_3^*\\0 & -s_3^* & s_2^*\end{pmatrix*} \rightarrow R_{STBC} = \frac{K}{T} = \frac{3}{4}
			\end{align*}
		\end{itemize}
		\item Orthogonality: $\textbf{X}^H\textbf{X} = {const}\cdot \textbf{I}_{N_T\times N_T}$
		\item Independent detection of $s_1$ \& $s_2$ for Alamouti Code
		\begin{align*}
				\begin{pmatrix*}[r]y_1 \\y_2 
			\end{pmatrix*} &= \frac{1}{\sqrt{2}}
			\begin{pmatrix*}[r]	s_1 & s_2 \\-s_2^* & s_1^*
			\end{pmatrix*}
			\begin{pmatrix*}[r]h_1\\h_2
			\end{pmatrix*} + 
			\begin{pmatrix*}[r]n_1\\n_2
			\end{pmatrix*}\\
			\rightarrow 
			\underbrace{
			\begin{pmatrix*}[r]y_1\\y_2
			\end{pmatrix*}}_{\tilde y} &= \frac{1}{\sqrt{2}}
			\underbrace{
			\begin{pmatrix*}[r]h_1 & h_2\\h_2^* & -h_1^*
			\end{pmatrix*}}_{\tilde F}
			\underbrace{
			\begin{pmatrix*}[r]s_1\\s_2
			\end{pmatrix*}}_{s} + 
			\underbrace{
			\begin{pmatrix*}[r]n_1\\n_2^*
			\end{pmatrix*}}_{\tilde n}
		\end{align*}
		$\Bigl ( $ Anmerkung: \underline{nur } $\begin{pmatrix}s_1 \\s_2 \end{pmatrix} $ gew\"unscht, nicht: $s_1^*, s_2^* \Bigr ) $
		\begin{align*}
			\textbf{F}^H\textbf{F} = \frac{1}{2}\begin{pmatrix}h_1^* & h_2\\h_2^* & -h_1\end{pmatrix}\begin{pmatrix}h_1 & h_2\\h_2^* & -h_1^* \end{pmatrix} = \frac{1}{2}\begin{pmatrix} |h_1|^2 + |h_2|^2 & 0\\0 & |h_1|^2 + |h_2|^2\end{pmatrix}		 								
		\end{align*}
		\begin{itemize}
			\item[$\rightarrow$] $\frac{\sqrt{2}}{\sqrt{|h_1|^2 + |h_2|^2}}\cdot \textbf{F} $ is unitary matrix 
			\item[$\rightarrow$] $\frac{2}{|h_1|^2 + |h_2|^2}\cdot \textbf{F}^H\cdot \tilde{\textbf{y}} = \textbf{s} + \frac{2}{|h_1|^2 + |h_2|^2}\cdot \textbf{F}^H\cdot\tilde{\textbf{n}}$ \\  $\bigl ( \frac{2}{|h_1|^2 + |h_2|^2}\cdot \textbf{F}^H\cdot\tilde{\textbf{n}}$ is AWGN vector with covariance matrix $\frac{2\sigma_n^2}{|h_1|^2 + |h_2|^2}\cdot \textbf{I}_{T\times T} \bigr)$
			\item[$\rightarrow$] ML decision: $\hat{\textbf{s}} = \underset{\textbf{s}}{\text{argmin}}\bigl | \bigl | \frac{2}{|h_1|^2 + |h_2|^2}\cdot \textbf{F}^H\cdot \tilde{\textbf{y}} - \textbf{s}\bigr |\bigr |^2 $
			\item[$\rightarrow$] independent ML decoding 
				\begin{align*}
					\hat{s}_1 &= \underset{s_1}{\text{argmin}} \Bigl |s_1 - \frac{h_1^*y_1 + h_2y_2^*}{\frac{1}{\sqrt{2}}(|h_1|^2 + |h_2|^2)}\Bigr |\\ \hat{s}_2 & = \underset{s_2}{\text{argmin}}\Bigl |s_2 - \frac{h_1^*y_1 - h_2y_2^*}{\frac{1}{\sqrt{2}}(|h_1|^2 + |h_2|^2)}\Bigr|
				\end{align*}
		\end{itemize}
		\item independent decoding property can be proved for all OSTBCs
		\item low complexity is at the expense of a rate-loss compared to other STBCs for $N_T > 2$
		\begin{itemize}
			\item[$\rightarrow$] Frequenzhopping
			\item[$\rightarrow$] keine Kanalinformation aus vorher empfangenen Symbolen m\"oglich $\Rightarrow$ Kanal \"andert sich st\"andig: nur Entscheidung, ob Rauschen oder Signal $+$ Rauschen	
		\end{itemize}
		\end{itemize}
		\item{Performance Analysis of Alamouti Code}
		\begin{itemize}
		\item Decision-variables after combining
		\begin{align*}
			r_1 &= \sqrt{2}\frac{h_1^*y_1 + h_2y_2^*}{|h_1|^2 + |h_2|^2}\\
			r_2 &= \sqrt{2}\frac{h_1^*y_1 - h_2y_2^*}{|h_1|^2 + |h_2|^2}
\end{align*} 
because of symmetry it suffices to consider $r_1$
		\begin{align*}
			r_1 &= \sqrt{2}\frac{h_1^*\bigl(\frac{1}{\sqrt{2}}s_1h_1 + \frac{1}{\sqrt{2}}h_2s_2 + n_1 \bigr) + h_2\bigl(-\frac{1}{\sqrt{2}}h_2s_1^* + \frac{1}{\sqrt{2}}h_1s_2^* + n_2\bigr)^*}{|h_1|^2 + |h_2|^2}\\ 
			&= \sqrt{2}\frac{\frac{1}{\sqrt2} \bigl(|h_1|^2 + |h_2|^2\bigr) s_1 + h_1^*n_1 + h_2n_2^*} {|h_1|^2 + |h_2|^2}\\
			&= 1\cdot s_1 + n_{eq}
		\end{align*}
		where 
		\begin{align*}
			n_{eq} &= \sqrt{2}\;\frac{h_1^*n_1 + h_2n_2^*}{|h_1|^2 + |h_2|^2}\\
			\text{SNR} \rightarrow \gamma _t  &= \frac{\mathcal{E}_s\cdot 1^2}{\sigma _{eq}^2}\quad \text{with}\quad \mathcal{E}\bigl\{ |s_1|^2\bigr\} = \mathcal{E}_s\\ 	
			\sigma _{eq}^2 &= 2\frac{|h_1|^2\sigma_n^2 + |h_2|^2\sigma_n^2}{\bigl(|h_1|^2 + |h_2|^2\bigr)^2} = \frac{2\sigma _n^2}{|h_1|^2 + |h_2|^2}
		\end{align*}
		\item[$\rightarrow$] $\gamma _t = \frac{1}{2}\frac{\mathcal{E}_s}{\sigma_n^2}\bigl ( |h_1|^2 + |h_2|^2\bigr)$
		\item[$\rightarrow$] $\text{SNR}_{\text{Alamouti}} = \frac{1}{2}\text{SNR}_{\text{MRC}} = \frac{1}{2}\text{SNR}_{\text{MRT}}$
		\item[$\rightarrow$] Alamouti code has diversity gain $G_d = 2$
		\item[$\rightarrow$] Transmission with Alamouti STBC requires 3dB higher SNR to achieve same performance as MRT $\rightarrow$  3dB loss in coding gain $G_c$
		\item[$\rightarrow$] Lack of CSI knowledge at transmitter ``costs'' 3dB in power efficiency
		\item[$\rightarrow$] General: 
		\begin{itemize}
			\item[$\cdot$] OSTBCs achieve a diversity gain of $G_d = N_T$ if only one receive antenna is available
			\item[$\cdot$] if $ N_R $ receive antennas are available, MRC can be used at the receiver to yield a diversity gain of $\underline{G_d = N_TN_R}$
		\end{itemize}
	\end{itemize}
\item Other STBCs:
	\begin{itemize}
		\item Quasi orthogonal STBCs
		\begin{itemize}
			\item higher rate than OSTBCs
			\item only subset of symbols have to be decoded jointly
			\item Example: $K = N_T = T = 4$
			\begin{align*}
				\textbf{X} = \frac{1}{2}
				\begin{pmatrix*}[r]
					s_1 & s_2 & s_3 & s_4\\
					-s_2^* & s_1^* & -s_4^* & s_3^*\\
					-s_3^* & -s_4^* & s_1^* & s_2^*\\
					s_4 & -s_3 & -s_2 & s_1
				\end{pmatrix*}
			\end{align*}
			\item Anmerkung 1:  $\textbf{X} $ ist \"ahnlich zu Alamouti Code
			\item Anmerkung 2: $\textbf{X}^H\textbf{X}$: viele Nicht-diagonal Elemente sind Null; die, die ungleich Null sind, zeigen, welche Symbole gemeinsam entschl\"usselt werden m\"ussen 		
		\end{itemize}
		\item Golden Code for $ N_T = N_R = 2$: achieves a rate of $R_{STBC} = 2 $ and full diversity of $ G_d = N_T, N_R = 4$
		\item Differential STBCs: $\textbf{X}_k = \textbf{X}_{k-1}\cdot \textbf{D}_k $. $\textbf{X}_k $ is transmitted, $\textbf{D}_k $ is transmitted 
		\item Linear dispersion codes: designed to achieve high mutual information
		\item noncoherent STBCs (On-Off-Keying)
	\end{itemize}
\end{itemize}
\paragraph{Space Time Code Design}
Given: 
\begin{itemize}
	\item Code  $\mathscr{X} = \bigl\{\textbf{X}_1,\dots ,\textbf{X}_{|\mathscr{X}|}\bigr\}$
	\item Channel: IID Rayleigh-fading: 
	\begin{itemize}
		\item $h_n \sim \mathcal{CN}(0,1);\quad n \in \{1, 2, \dots , N_T \} $
		\item AWGN $ n \sim \mathcal{CN}(0,\sigma _n^2) $
	\end{itemize}
\end{itemize}
Problem: How should we design codebook $\mathscr{X}$?
\begin{itemize}
	\item Need to derive error rate for general codebooks $\mathscr{X}$!
	\begin{itemize}
		\item Codeword error rate\\
		\begin{align*}
			P_e=\frac{1}{|\mathscr{X}|}\sum\limits_{i=1}^{|\mathscr{X}|}\mathrm{Pr}\{\mathbf{x}_i\neq\hat{\mathbf{x}}_i\}
		\end{align*}
		where $\hat{\mathbf{x}}_i$ is the detected codeword and we assume that all codewords are equally likely
	\end{itemize}
\underline{Problem:} $\mathrm{Pr}\{\mathbf{x}_i\neq\hat{\mathbf{x}}_i\}$ is not tractable in general
	\item  Use union bound to upper bound $\mathrm{Pr}\{\mathbf{x}_i\neq\hat{\mathbf{x}}_i\}$ as upper sum over \underline{pairwise error probabilities}(PEP)
	$\mathrm{Pr}\{\mathbf{x}_i\rightarrow\mathbf{x}_j\}$ where it is assumed that $\mathbf{x}_i$ was transmitted and $\mathbf{x}_i$ and $\mathbf{x}_j$ are the
	only codewords in the codebook
	\begin{align*}
	\boxed{P_e\leq \frac{1}{|\mathscr{X}|}\sum\limits_{i=1}^{|\mathscr{X}|}\sum\limits_{j=1}^{|\mathscr{X}|}\mathrm{Pr}\{\mathbf{x}_i\rightarrow\mathbf{x}_j\} \text{ where } j \neq i}
	\end{align*}
\underline{Calculation of PEPs}\\
Recall: $\hat{\mathbf{x}}=\underset{\mathbf{x}\in\mathscr{X}}{\mathrm{argmin}}||\mathbf{y-xh}||^2$\\
Now, $\mathbf{x}_i$ and $\mathbf{x}_j$ are the only alternatives and an error is made if $||\mathbf{y-x_{\mathnormal{i}}h}||^2>||\mathbf{y-x_{\mathnormal{j}}h}||^2$ since $\mathbf{x}_i$ was sent but $\mathbf{x}_j$ was detected
	\begin{align*}
	\rightarrow	||\mathbf{x_{\mathnormal{i}}h+n-x_{\mathnormal{i}}h}||^2 
	&>||\mathbf{x_{\mathnormal{i}}h+n-x_{\mathnormal{j}}h}||^2\\
	||\mathbf{n}||^2
	&>||\mathbf{(x_{\mathnormal{i}}-x_{\mathnormal{j}})h+n}||^2\\
	\rightarrow	||\mathbf{n}||^2
	&>\underbrace{\mathbf{h^{\mathnormal{H}}(x_{\mathnormal{i}}-x_{\mathnormal{j}})^{\mathnormal{H}}(x_{\mathnormal{i}}-x_{\mathnormal{j}})h}}_{\Delta}
			+\mathbf{h^{\mathnormal{H}}(x_{\mathnormal{i}}-x_{\mathnormal{j}})n+n^{\mathnormal{H}}(x_{\mathnormal{i}}-x_{\mathnormal{j}})h+||n||^{\mathnormal{2}}}
	\end{align*}
	\begin{align*}
		\rightarrow \underbrace{\mathbf{-h^{\mathnormal{H}}(x_{\mathnormal{i}}-x_{\mathnormal{j}})^{\mathnormal{H}}n-n^{\mathnormal{H}}(x_{\mathnormal{i}}-x_{\mathnormal{j}})h}}_{z}>\Delta
	\end{align*}
for given $\mathbf{h}$, $z$ is a gaussian random variable
\begin{align*}
	\sigma_z^2&=\mathcal{E}\{|z|^2\}=\mathcal{E}\{2\mathbf{h}^H(\mathbf{x}_i-\mathbf{x}_j)\overbrace{\mathbf{nn}^H}^{\sigma_n^2\mathbf{I}}(\mathbf{x}_i-\mathbf{x}_j)\mathbf{h}
	+2\mathbf{h}^H(\mathbf{x}_i-\mathbf{x}_j)^H\overbrace{\mathbf{nn}^T}^{=0}(\mathbf{x}_i-\mathbf{x}_j)^*\mathbf{h}^*\}\\
		&=2\sigma_n^2\Delta+0
\end{align*}
\begin{align*}
	\mathrm{Pr}\{\mathbf{x}_i\rightarrow\mathbf{x}_j\}&=\int\limits_{\Delta}^{\infty}\frac{1}{\sqrt{2\pi}\sigma_z}\mathrm{exp}\left(-\frac{z^2}{2\sigma_z^2}\right)\mathrm{d}z, \; t=\frac{z}{\sigma_z}\\
	&=\frac{1}{\sqrt{2\pi}}\int\limits_{\frac{\Delta}{\sigma_z}}^{\infty}e^{-\frac{t^2}{2}}\mathrm{d}t=Q\left(\frac{\Delta}{\sigma_z}\right)=Q\left(\frac{\Delta}{\sqrt{2\sigma_n^2\Delta}}\right)\\
	&=Q\left(\sqrt{\frac{\Delta}{2\sigma_n^2}}\right)
\end{align*}
	\item $\mathrm{Pr}\{\mathbf{x}_i\rightarrow\mathbf{x}_j\}=\mathcal{E}\left\{Q\left(\sqrt{\frac{\Delta}{2\sigma_n^2}}\right)\right\}$
	\begin{itemize}
	\item to avoid cumbersome Q-function we use Chernoff bound:
	\begin{align*}	
		\boxed{Q(x)\leq\frac{1}{2}e^{-\frac{x^2}{2}}}\\
	\end{align*}
	\begin{align*}
		\mathrm{Pr}\{\mathbf{x}_i\rightarrow\mathbf{x}_j\}\leq \frac{1}{2}\mathcal{E}_h\left\{\mathrm{exp}\left(-\frac{\mathbf{h}^H\mathbf{Qh}}{4\sigma_n^2}\right)\right\}\\
		\text{where } \mathbf{Q}=(\mathbf{x}_i-\mathbf{x}_j)^H(\mathbf{x}_i-\mathbf{x}_j)
	\end{align*}
	\end{itemize}
	\item Eigendecomposition: $\mathbf{Q=U}^H\mathbf{\Lambda U}$ with $\mathbf{\Lambda} = \mathrm{diag}\{\lambda_1,\dots,\lambda_r, 0,\dots,0\}\qquad r=\mathrm{rank}\{\mathbf{Q}\}$\\

	\item Elements $\mathbf{h}$ are i.i.d. Gaussian
	\begin{itemize}	
		\item $\mathbf{\underline{\beta}=Uh}$ has also i.i.d. Gaussian random variables as elements since $\mathbf{U}$ is unitary matrix
		\item $\mathbf{h}^H\mathbf{Qh}=\underbrace{\mathbf{h}^H\mathbf{U}^H}_{\underline{\beta}^*}\mathbf{\Lambda}\underbrace{\mathbf{Uh}}_{\underline{\beta}}=\sum\limits_{i=1}^r\lambda_i|\beta_i|^2$ 
		with $\underline{\beta}=[\beta_1,\dots,\beta_{N_T}]$
	\end{itemize}
	\begin{align*}
	\mathrm{Pr}\{\mathbf{x}_i\rightarrow\mathbf{x}_j\}&=\frac{1}{2}\mathcal{E}_{\underline{\beta}}\left\{\mathrm{exp}\left(-\frac{\sum\limits_{i=1}^r\lambda_i|\beta_i|^2}{4\sigma_n^2}\right)\right\}\\
	&=\frac{1}{2}\mathcal{E}_{\underline{\beta}}\left\{\prod\limits_{i=1}^r e^{-\frac{\lambda_i}{4\sigma_n^2}|\beta_i|^2}\right\}\\
	&=\frac{1}{2}\prod\limits_{i=1}^r\mathcal{E}_{\beta_i}\left\{e^{-\frac{\lambda_i}{4\sigma_n^2}|\beta_i|^2}\right\}\\
	&=\frac{1}{2}\prod\limits_{i=1}^r \mathcal{E}_{|\beta_i|^2}\left\{e^{-\frac{\lambda_i}{4\sigma_n^2}|\beta_i|^2}\right\}\mathrel{\widehat{=}}\text{MGF of exponentially distributed variable } \alpha_i=|\beta_i|^2\\
	\end{align*}
	$\rightarrow P_{\alpha_i}(x)=e^{-x},\; x\geq 0$
	\begin{align*}
	\rightarrow \mathrm{Pr}\{\mathbf{x}_i\rightarrow \mathbf{x}_j\}&\leq \frac{1}{2}\prod\limits^r_{i=1}\frac{1}{1+\frac{\lambda_i}{4\sigma_n^2}}\\
	&\leq\prod\limits^r_{i=1}\frac{1}{\frac{\lambda_i}{4\sigma_n^2}}=2^{2r-1}\frac{1}{\prod\limits^r_{i=1}\lambda_i}\left(\underbrace{\frac{1}{\sigma_n^2}}_{\mathrel{\widehat{=}}SNR}\right)^{-r}
	\end{align*}
	\item upper bound on $P_e$: 
	\begin{align*}
		\lambda_n(i,j) &= n\text{th eigenvalue of }(\mathbf{x}_i-\mathbf{x}_j)^H(\mathbf{x}_i-\mathbf{x}_j)\\
		r(i,j) &= \text{ rank of }(\mathbf{x}_i-\mathbf{x}_j)^H(\mathbf{x}_i-\mathbf{x}_j)
	\end{align*}
	\begin{align*}
	\rightarrow P_e \leq \frac{1}{\mathscr{|X|}} \sum\limits_{i=1}^{\mathscr{|X|}}\sum\limits_{j=1}^{\mathscr{|X|}}2^{2r(i,j)-1}\frac{1}{\prod\limits_{n=1}^{r(i,j)}\lambda_n(i,j)}\left(\frac{1}{\sigma_n^2}\right)^{-r(i,j)}
	\end{align*}
	\item generally loose bound but offers significant insight for code design
\end{itemize}
\vspace{1.6cm}
{\large\underline{Two criteria:}}\\
	\begin{description}
		\item[\underline{Rank criterion:}]The diversity gain of a ST code is given by \\
		$G_d=\underset{i,j}{\mathrm{min}}(r(i,j))=\underset{i,j}{\mathrm{min}}\;\mathrm{rank}\left((\mathbf{x}_i-\mathbf{x}_j)^H(\mathbf{x}_i-\mathbf{x}_j)\right)$\\
		$\rightarrow$ Design code such that minimum rank of all possible matrices $(\mathbf{x}_i-\mathbf{x}_j)^H(\mathbf{x}_i-\mathbf{x}_j)$ is maximized
		\begin{align*}
			T \updownarrow  \overset{\overset{N_T}{\longleftrightarrow}}{\mathbf{X}_i} \Rightarrow r(i,j)=N_T\qquad \forall i\neq j
		\end{align*}
		\item[\underline{Determinant criterion:}]To maximize the coding gain among all codes with $r(i,j)=N_T$, we need to maximize 
		$\mathrm{max}\;\underset{i,j}{\mathrm{min}}\prod\limits_{n=1}^{N_T}\lambda_n(i,j)=\mathrm{max}\;\underset{i,j}{\mathrm{min}}\left|(\mathbf{x}_i-\mathbf{x}_j)^H(\mathbf{x}_i-\mathbf{x}_j)\right| \qquad \forall i \neq j$
	\end{description}
\begin{itemize}
	\item Rank and determinant criterion can be used for the search for good space-time block codes and space-time trellis codes. These two criteria were first derived by Tarokh, et. al. 1998.
	\item diversity increases to $N_TN_R$ if $N_K$ receive antennas are available
	\item Example: see B\"aro, Bauch, Hansmann: Improved codes for space-time trellis coded modulation. IEEE Comm. Letters, 2000.
\end{itemize}
\subsubsection{Partial or Imperfect CSI at the Transmitter}
\begin{itemize}
	\item In practice, the CSI cannot be perfect. Channel estimation, quantization and noisy feedback channels introduce errors.
	\item If the system is optimized for perfect CSI (\textit{e.g.} using MRT or EGT), the performance for imperfect CSI may be worse than for a system designed for no CSI(\textit{e.g.} space-time coding)
	\item In this case, it is advantageous to use a hybrid approach and combine beamforming and space-time coding.
\end{itemize}
\begin{figure}[ht]
	\centering
	\input{STBEAM.pstex_t}
	\caption{Block Diagramm of MISO with Beamforming}
	\label{fig:STBEAM.pstex_t}
\end{figure}
\begin{itemize}
	\item $\mathbf{W}$ is the beamforming matrix which depends on the reliability of the CSI
	\item CSI is modeled as
	\begin{align*}
	\hat{h}_i=\rho h_i+\sqrt{1-\rho^2}e_i
	\end{align*}
	where:
		\begin{itemize}
			\item $\hat{h}_i$ is the CSI estimate
			\item $\rho$ is the correlation between $\hat{h}_i$ and $h_i$
			\item $e_i$ is the CSI error modeled as AWGN
		\end{itemize}
	extreme cases:
		\begin{itemize}
			\item $\rho = 0$ : $\hat{h}_i$ independent of $h_i$ $\rightarrow$ no CSI ($\mathbf{W}=\mathbf{I}$)
			\item $\rho = 1$ : $\hat{h}_i=h_i$ $\rightarrow$ perfect CSI ($\mathbf{W}$ performs MRT)
		\end{itemize}
	\item $\mathbf{W}$ can be optimized under the assumptions for given $\rho$ and $\hat{h}_i$\\
	$\rightarrow$ see for details: J\"ongren, Skorglund and Ottersten: "Combining Beamforming and Orthogonal Space-time Block Coding", IEEE on IT, 2002.
\end{itemize}
\newpage
\subsection{MIMO Systems without CSI at the transmitter}
\begin{itemize}
	\item We consider $N_T\times N_R$ MIMO system and assume that the channel matrix $\mathbf{H}$ is not known at the transmitter\\
	$\rightarrow$ no CSI at the transmitter (CSIT)
	\item signal model:
	\begin{align*}
		N_R \updownarrow\mathbf{y}=N_R \updownarrow \overset{\overset{N_T}{\longleftrightarrow}}{\mathbf{H}}\mathbf{x}\updownarrow N_T+\mathbf{n} \updownarrow N_R
	\end{align*}

\begin{figure}[ht]
	\centering
	\input{SIGMOD.pstex_t}
	\caption{Block Diagramm of MISO without CSI}
	\label{fig:SIGMOD.pstex_t}
\end{figure}

	\item $x_n$ are $M$-ary i.i.d. scalar symbols taken \textit{e.g} from an $M$-PSK or $M$-QAM symbol alphabet $\mathscr{A}$
	\item This scheme is often called ``spatial multiplexing''
	\item We transmit $N_T$ symbols per symbol interval\\
	$\rightarrow$ rate $R=\log_2(M)\cdot N_T$ for uncoded transmission
	\item Problem: How to detect $\mathbf{x}$ at the receiver considering
		\begin{itemize}
			\item performance and
			\item complexity?
		\end{itemize}
\end{itemize}
\subsubsection{Optimum Detection}
\begin{itemize}
	\item Elements of $\mathbf{n}$ are gaussian random variables with variance $\sigma_n^2$
	\item $\mathbf{H}$ is known at the receiver
	\begin{align*}
		p(\mathbf{y}|\mathbf{x})&=\frac{1}{\pi^{N_R}\sigma_n^2\mathbf{I}_{N_R \times N_R}} \exp \left(-(\mathbf{y}-\mathbf{Hx})^H(\sigma_n^2\mathbf{I}_{N_R \times N_R})^{-1}(\mathbf{y}-\mathbf{Hx})\right)\\
		&=\frac{1}{\pi^{N_R}\sigma_n^{2N_R}}\exp\left(-\frac{1}{\sigma_n^2}||\mathbf{y}-\mathbf{xH}||^2\right)
	\end{align*}
	\item ML-Detection
	\begin{align*}
		\hat{x}=\underset{\mathbf{x}\in \mathscr{A}^{N_T}}{\mathrm{argmin}}||\mathbf{y}-\mathbf{xH}||^2=\underset{\mathbf{x}\in \mathscr{A}^{N_T}}{\mathrm{argmax}}\quad p(\mathbf{y}|\mathbf{x})
	\end{align*}
	\begin{itemize}
		\item[$\rightarrow$] $M^{N_T}$ metric calculations $\rightarrow$ complexity is exponential in $N_T$!!
		\item[$\rightarrow$] in general too complex in practice
	\end{itemize}
	\item Performance 
	\begin{itemize}
		\item consider worst case pairwise error probability (PEP) to evaluate \underline{diversity gain}
		\item PEP $\rightarrow$ $x_i$ is transmitted but $x_j\neq x_i$ is detected\\
		this happens if $||\mathbf{y}-\mathbf{Hx}_i||^2 > ||\mathbf{y}-\mathbf{Hx}_j||^2$\\
		$\rightarrow\; ||\mathbf{n}||^2>||\mathbf{H}(\mathbf{x}_i-\mathbf{x}_j)+\mathbf{n}||^2$
		\item the ``worst case'' is if $\mathbf{x}_i$ \& $\mathbf{x}_j$ differ only in one element \textit{i.e.},
		\begin{align*}
			 \mathbf{x}_i-\mathbf{x}_j=(x_{ni}-x_{nj})= \begin{bmatrix*} 0 \\ 0 \\ 0 \\ \vdots \\  1 \\ \vdots \\ 0 \\ 0  \end{bmatrix*}\leftarrow \text{``1'' in position } n
		\end{align*}
		where $\mathbf{x}_i=[x_{1i},x_{2i},\dots,x_{N_Ti}]$
		\item $||\mathbf{n}||^2>||\underbrace{\mathbf{h}_n}_{\text{$n$th column of $\mathbf{H}$}}\underbrace{(x_{ni}-x_{nj})}_{\Delta x_n(i,j)}+\mathbf{n}||^2$
		\item $||\mathbf{n}||^2> \mathbf{h}_n^H\mathbf{n} \Delta x^*_n(i,j)+\mathbf{n}^H\mathbf{h}_n\Delta x_n(i,j)+||\mathbf{n}||^2+||\mathbf{h}_n||^2-|\Delta x_n(i,j)|^2$
		\begin{align*}
			||\mathbf{h}_n||^2|\Delta x_n(i,j)|^2<\underbrace{-\mathbf{h}_n^H\mathbf{n}\Delta x_n(i,j)-\mathbf{n}^H\mathbf{h}_n \Delta x_n(i,j)}_
			{\text{Gaussian random variable with variance $\sigma_{eq}^2=2\sigma_n^2|\Delta x_n(i,j)|^2||\mathbf{h}_n||^2$}}
		\end{align*}
		\item $\mathrm{Pr}\{\mathbf{x}_i\rightarrow \mathbf{x}_j\|\mathbf{H}\}=Q\left(\sqrt{\frac{||\mathbf{h}_n||^2|\Delta x_n(i,j)|^2}{2\sigma_n^2}}\right)$
		\item $\mathrm{Pr}\{\mathbf{x}_i\rightarrow \mathbf{x}_j\}=\mathcal{E}\left\{Q\left(\sqrt{\frac{||\mathbf{h}_n||^2|\Delta x_n(i,j)|^2}{2\sigma_n^2}}\right)\right\}$\\
		$\rightarrow$ use same approach as for space-time code design to get diversity order\\
		or : SNR is 
		\begin{align*}
			\gamma_t = \frac{||\mathbf{h}_n||^2|\Delta x_n(i,j)|^2}{2\sigma_n^2} = \frac{|\Delta x_n(i,j)|^2}{2\sigma_n^2}(|h_{1n}|^2+|h_{2n}^2+\dots+|h_{N_Rn}|^2)
		\end{align*}
		\item same form as SNR of MRC with $N_R$ receive antennas
		\item diversity gain of spatial multiplexing wit ML-decoding is
		\begin{align*}
			G_d=N_R
		\end{align*}
		\item diversity of $N_T$ transmit antennas is not exploited with spatial multiplexing
		\item to exploit this additional gain, coding across space is required (at the expense of rate)\\
			(Hier geh\"oren die detection performance kurven f\"ur BPSK hin)
	\end{itemize}
\end{itemize}
\subsubsection{Linear Receivers}
\begin{itemize}
	\item How can we avoid the complexity associated wit the joint detection of the elements of $\mathbf{x}$?
	\item Idea: Employ linear filter (matrix) to seperate  the elements of $\mathbf{x}$
	\item Requires: $N_T \leq N_R$
	\item We form
	\begin{align*}
		\mathbf{r}=N_T\updownarrow &\overset{\overset{N_R}{\longleftrightarrow}}{\mathbf{F}}\mathbf{y}=[r_1,\dots,r_{N_T}]^T\\
	&\text{where $\mathbf{F}$ is the filter matrix and $\mathbf{y}$ is the received vector}
	\end{align*}
	such that $x_n$ can be obtained from
	\begin{align*}
		\hat{x}_n&=\underset{x_n \in \mathscr{A}}{\mathrm{argmin}} \; |r_i-x_n|^2
		&\text{where } \mathbf{F} \in \mathds{C}^{N_T\times N_R}
	\end{align*}
	\item Two popular design criteria for $\mathbf{F}$
	\begin{itemize}
		\item Zero-forcing (ZF) criterion
		\item minimum mean squared error (MMSE) crtiterion
	\end{itemize}
\end{itemize}
\paragraph{ZF Detection}
\begin{align*}
	\mathbf{r}=\mathbf{Fy}=\mathbf{F}(\mathbf{Hx}+\mathbf{n})=\mathbf{FHx}+\mathbf{Fn}
\end{align*}
ZF $\leftrightarrow$ we require $\mathbf{FH}=\mathbf{I}_{N_T \times N_T}$
\begin{itemize}
	\item noise covariance matrix
	\begin{align*}
		\Phi_{ee}=\mathcal{E}\{\mathbf{Fn}(\mathbf{Fn})^H\}=\sigma_n^2\mathbf{FF}^H
	\end{align*}
	\item $N_T=N_R \; \rightarrow \mathbf{FH}=\mathbf{I}_{N_T \times N_T} \; \rightarrow \mathbf{F}=\mathbf{H}^{-1}$ assuming $\mathbf{H}$ is invertible
	\item $\rightarrow$ $N_T \leq N_R$ $\rightarrow$ which one of the many $\mathbf{F}$ that yield $\mathbf{FH}=\mathbf{I}_{N_T \times N_T}$?
	\item choose $\mathbf{F}$ that leads to the smallest noise enhancement
	\item optimal $\mathbf{F}$ is the solution to the following problem:
	\begin{align*}
		\underset{\mathbf{F}}{\mathrm{min}} \;& \mathrm{tr}\{\sigma_n^2\mathbf{FF}^H\}\\
		\text{s.t } &\mathbf{FH}=\mathbf{I}_{N_T \times N_T}\\
		\text{ the constraint} \text{ is equivalent to }
		&\mathrm{tr}\{(\mathbf{FH}-\mathbf{I})(\mathbf{FH}-\mathbf{I})^H\}=0
	\end{align*}
Lagrangian:
	\begin{align*}
		L(\mathbf{F})&=\mathrm{tr}\{\sigma_n^2\mathbf{FF}^H\}+\lambda \mathrm{tr}\{\mathbf{FHH}^H\mathbf{F}-\mathbf{FH}-\mathbf{H}^H\mathbf{F}^H+\mathbf{I}\}\\
			&=\sigma_n^2\mathrm{tr}\{\mathbf{FF}^H\}+\lambda \mathrm{tr}\{\mathbf{FHH}^H\mathbf{F}^H\}-\lambda\mathrm{tr}\{\mathbf{FH}\}
			-\lambda\mathrm{tr}\{\mathbf{H}^H\mathbf{F}^H\}+\lambda N_T
	\end{align*}
	\item use rules for complex matrix differentiation in Table \Rmnum{4} in paper by Hj\"orunges \& Gesbert
	\begin{align*}
		\frac{\delta L(\mathbf{F})}{\delta \mathbf{F}^*}=\sigma_n^2\mathbf{F}+\lambda\mathbf{FHH}^H-\lambda\mathbf{H}^H=0\\
		\rightarrow \mathbf{F}(\sigma_n^2\mathbf{I}+\lambda\mathbf{HH}^H)=\lambda\mathbf{H}^H\\
		\rightarrow \mathbf{F}=\lambda\mathbf{H}^H(\sigma_n^2\mathbf{I}+\lambda\mathbf{HH}^H)^{-1}
	\end{align*}
	use matrix inversion lemma
	\begin{align*}
	(\mathbf{A}+\mathbf{UBV})^{-1}=\mathbf{A}^{-1}-\mathbf{A}^{-1}\mathbf{U}(\mathbf{B}^{-1}+\mathbf{VA}^{-1}\mathbf{U})^{-1}\mathbf{VA}^{-1}
	\end{align*}
	\begin{align*}
	\rightarrow\mathbf{F}&=\lambda\mathbf{H}^H\left[\frac{1}{\sigma_n^2}\mathbf{I}-\frac{1}{\sigma_n^2}\mathbf{H}
	\left[\frac{1}{\lambda}\mathbf{I}+\frac{1}{\sigma_n^2}\mathbf{H}^H\mathbf{H}\right]^{-1}\mathbf{H}^H\frac{1}{\sigma_n^2}\right]\\
	&=\frac{\lambda}{\sigma_n^2}\left[\underbrace{\mathbf{I}}_{\left(\frac{1}{\lambda}\mathbf{I}+\frac{1}{\sigma_n^2}\mathbf{H}^H\mathbf{H}\right)
	\left(\frac{1}{\lambda}\mathbf{I}+\frac{1}{\sigma_n^2}\mathbf{H}^H\mathbf{H}\right)^{-1}}-\frac{1}{\sigma_n^2}\mathbf{H}^H\mathbf{H}
	\left[\frac{1}{\lambda}\mathbf{I}+\frac{1}{\sigma_n^2}\mathbf{H}^H\mathbf{H}\right]\right]\mathbf{H}^H\\
	&=\frac{\lambda}{\sigma_n^2}\left[\frac{1}{\lambda}\mathbf{I}+\frac{1}{\lambda}\mathbf{H}^H\mathbf{H}-\frac{1}{\sigma_n^2}\mathbf{H}^H\mathbf{H}\right]
	\left(\frac{1}{\lambda}\mathbf{I}+\frac{1}{\sigma_n^2}\mathbf{H}^H\mathbf{H}\right)^{-1}\mathbf{H}^H\\
	&=\frac{1}{\sigma_n^2}\left(\frac{1}{\lambda}\mathbf{I}+\frac{1}{\sigma_n^2}\mathbf{H}^H\mathbf{H}\right)\mathbf{H}^H
	\end{align*}
	\item How to choose $\lambda$
	\begin{align*}
	\mathbf{FH}&=\frac{1}{\sigma_n^2}\left(\frac{1}{\lambda}\mathbf{I}+\frac{1}{\sigma_n^2}\mathbf{H}^H\mathbf{H}\right)^{-1}\mathbf{H}^H\mathbf{H}=\mathbf{I}\\
	\frac{1}{\sigma_n^2}\mathbf{H}^H\mathbf{H}&=\frac{1}{\lambda}\mathbf{I}+\frac{1}{\sigma_n^2}\mathbf{H}^H\mathbf{H}\\
	\Rightarrow \lambda \rightarrow \infty
	\end{align*}
	\begin{align*}
	\Rightarrow \boxed{\mathbf{F}=(\mathbf{H}^H\mathbf{H})^{-1}}\;\widehat{=}\text{ Moore-Penrose pseudoinverse}	
	\end{align*}
noise covariance:
\begin{align*}
	\Phi_{ee}=\sigma_n^2\mathbf{FF}^H=\sigma_n^2(\mathbf{H}^H\mathbf{H})^{-1}\underbrace{\mathbf{H}^H\mathbf{H}(\mathbf{H}\mathbf{H})^{-1}}_{\mathbf{I}}
	=\sigma_n^2(\mathbf{H}^H\mathbf{H})^{-1}
\end{align*}
$\Phi_{ee}$ is not in general a diagonal matrix
	\begin{itemize}
		\item effective noise $\mathbf{Fn}$ is spatially correlated
		\item ''equalization´´ of channel leads to coloring of noise
	\end{itemize}
	\item Interpretation:\\
	we have
	\begin{align*}
	\mathbf{FHx} = \begin{bmatrix*} \mathbf{f}_1 \\ \mathbf{f}_2 \\ \vdots \\  \mathbf{f}_{N_T}  \end{bmatrix*} \begin{bmatrix*} \mathbf{h}_1 & \mathbf{h}_2 &\dots & \mathbf{h}_{N_T}\end{bmatrix*}\mathbf{x}=\mathbf{x}	
	\end{align*}
	\begin{itemize}
		\item $\mathbf{f}_i\mathbf{h}_i=1 \quad \mathbf{f}_i\mathbf{h}_j=0 \quad \forall i \neq j$
		\item $\mathbf{f}_i^T$ is orthogonal to $\begin{bmatrix*} \mathbf{h}_1 & \dots \mathbf{h}_{i-1} & \mathbf{h}_{i+1} & \dots & \mathbf{h}_{N_T} \end{bmatrix*}
		\updownarrow N_R$
		\item $\mathbf{f}_i^T$ is confined to an $N_R-(N_T-1)$ dimensional subspace of the $N_R$ dimensional space spanned by $\mathbf{H}$ 
	\end{itemize}
	\item Diversity gain
	\begin{itemize}
		\item e.g. SISO model:  $ r_i = \mathbf{f}_i\cdot\mathbf{h}_i\mathbf{x}_i + \mathbf{f}_i\cdot\mathbf{n}_i $
		\begin{itemize}
			\item[$\rightarrow$] $ \text{SNR}_{\text{eq}} $$= \frac{\mathcal{E}_s|\mathbf{f}_i\mathbf{h}_i|^2}{\sigma_n^2\|\mathbf{f}_i\|^2} = \frac{\mathcal{E}_s}{\sigma_n^2}\left | \tilde{\mathbf{f}}_i\cdot\mathbf{h}_i\right | $,\quad  where $\mathbf{f}_i =\alpha\mathbf{f}_i  $ with $\|\tilde{\mathbf{f}}_i \|^2 = 1$  
			\item[] we can represent $\mathbf{f}_i$ as: $\tilde{\mathbf{f}}_i^T = \alpha\mathbf{M}\boldsymbol{\beta}$,\medskip\\ where: $\mathbf{M} \in \mathbb{C}^{N_R\times(N_R-N_T+1)} \text{ and } \boldsymbol{\beta} \in \mathbb{C}^{(N_R - N_T +1)\times 1} \quad\widehat{=} $ basis of subspace
			\item[$\rightarrow$] $\tilde{\mathbf{f}}_i\textbf{M}_i = \boldsymbol{\beta}^T\mathbf{M}_i^T\mathbf{M}_i $,\medskip\\  where: $\mathbf{M}^H\mathbf{M} = \tilde{\mathbf{M}}_i = \mathbf{I} $ and $ \tilde{\mathbf{M}}_i \rightarrow \mathcal{CN} (0, \sigma_n^2\mathbf{I}_{(N_R - N_T + 1)} $\medskip\\ (since rows of $\mathbf{M}^T$ are orthogonal)
			\item[] 
				\begin{align*}		
					\text{SNR}_{\text{eq}} = \frac{\mathcal{E}_s}{\sigma_n^2}\alpha^2\left| \sum\limits_{j = 1}^{N_R - N_T+1}\boldsymbol{\beta}_{ji}\tilde{\mathbf{h}}_ji \right|^2; \tilde{\mathbf{h}}_i &= \begin{pmatrix}
					\tilde{h}_{1i}, & \tilde{h}_{2i}, & \dots , & \tilde{h}_{N_R - N_T +1}\end{pmatrix}^T\\ \boldsymbol{\beta}_i &= \begin{pmatrix}\beta_{1i}, & \dots , & \beta_{N_R - N_T +1,i}\end{pmatrix}^T										  
				\end{align*}  		
			\item[$\rightarrow$] $\text{SNR}_{\text{eq}} $ includes only $ N_R - N_T + 1$ independent Gaussian RV
			\item[$\rightarrow$] diversity gain is limited to: $\underline{G_d = N_R - N_T + 1} $
		\end{itemize}
		\item[] \underline{Example:} \begin{align*} N_T &= N_R = 3 \\ G_d^{ZF} &= 1 \text{ but } G_d^{ML} = N_R = 3 \end{align*} $ \rightarrow $ huge performance loss because of linear ZF
	\end{itemize}		
\end{itemize}
\paragraph{MMSE detection}
\begin{itemize}
\item ZF criterion may be too strict and leads to noise enhancement
	\begin{itemize}
		\item[$\rightarrow$] maybe it is better to allow some interferences between signals but reduce noise enhancement
		\item[$\rightarrow$] What is the optimal trade-off between interference and noise?
		\item[$\rightarrow$] MMSE criterion
	\end{itemize}
\item MMSE criterion 
	\begin{itemize}
		\item error signal: $ \mathbf{e} = \mathbf{Fy} - \mathbf{x} $
		\item total error variance: $\sigma_e^2 = \mathcal{E}\bigl \{ \| \mathbf{e}\| ^2\bigr\} = \mathcal{E}\bigl \{\text{tr}\{\mathbf{ee}^H\} \bigr\} = \text{tr}\bigl\{\mathcal{E}\{\mathbf{ee}^H\}\bigr\} = \text{tr}\{ \Phi_{ee}\}$ 
		\item $\Phi_{ee}$: error covariance matrix
		\item optimal filter: $\mathbf{F}_{\text{opt}} = \underset{\mathbf{F}}{\mathrm{argmin}}\,\mathrm{tr}\{\Phi_{ee}\} $
	\end{itemize}
	\item Deviation of $\mathbf{F}_{\text{opt}}$
	\begin{itemize}
		\item $\Phi_{ee}= \mathcal{E}\{\mathbf{ee}^H\} = \mathcal{E}\{(\mathbf{Fy} - \mathbf{x})(\mathbf{Fy} - \mathbf{x})^H \} = \mathbf{F}\cdot\boldsymbol{\Phi}_{yy}\cdot\mathbf{F}^H - \mathbf{F}\cdot\boldsymbol{\Phi}_{yx} - \boldsymbol{\Phi}_{xx}\cdot\boldsymbol{\Phi}_{xy}\cdot\mathbf{F}^H + \boldsymbol{\Phi}_{xx} $
		\item[] with:
		\begin{align*}
			\boldsymbol{\Phi}_{yy} &= \mathcal{E}\{ \mathbf{yy}^H\} = \mathcal{E}\{(\mathbf{Hx} + \mathbf{n})(\mathbf{Hx} + \mathbf{n})^H\} = \mathcal{E}_s \cdot\mathbf{H}\mathbf{H}^H + \sigma_n^2\cdot\mathbf{I}_{N_R\times N_T}\\
			\boldsymbol{\Phi}_{yx} &= \mathcal{E}\{ \mathbf{yx}^H\} = \mathcal{E}\{ (\mathbf{Hx} + \mathbf{n})\cdot\mathbf{x}^H\} = \mathcal{E}_s\cdot\mathbf{H}^H = \boldsymbol{\Phi}_{xy}^H\\
			\boldsymbol{\Phi}_{xx} &= \mathcal{E}_s\cdot\mathbf{I}_{N_T\times N_R}
		\end{align*}
		\item $\mathbf{F}_{\text{opt}} \quad \rightarrow \quad \frac{d}{d\mathbf{F}^*}\Bigl ( \text{tr}\{\mathbf{F}\boldsymbol{\Phi}_{yy}\mathbf{F}^H\} - \text{tr}\{\mathbf{F}\boldsymbol{\Phi}_{yx}\} - \text{tr}\{ \boldsymbol{\Phi}_{xy}\mathbf{F}^H\} + \text{tr}\{\boldsymbol{\Phi}_{xx}\}\Bigr )\overset{!}{=} 0 $
		\item[] with Table \Rmnum{4} in paper by Hjoranges $\&$ Gesbert:
		\begin{itemize}
			\item[$\Rightarrow$] $\mathbf{F}\cdot\boldsymbol{\Phi}_{yy} - \boldsymbol{\Phi}_{xy} = 0$  	
			\item[$\Rightarrow$] $ \mathbf{F}_{\text{opt}} = \boldsymbol{\Phi}_{xy}\cdot\boldsymbol{\Phi}_{yy}^{-1} = \mathcal{E}_s\mathbf{H}^H\bigl(\mathcal{E}_s\mathbf{H}\mathbf{H}^H + \sigma_n^2\mathbf{I}\bigr)^{-1} \medskip\\ = \text{(Matrix inversion Lemma)} =\medskip \\ = \bigl( \mathbf{H}^H\mathbf{H} + \frac{\sigma_n^2}{\mathcal{E}_s}\mathbf{I}\bigr)^{-1}\cdot\mathbf{H}^H$
		\end{itemize}
		\item Comparison:
		\begin{align*}
			 \mathbf{F}_{\text{MMSE}} = \bigl( \mathbf{H}^H\mathbf{H} + \frac{\sigma_n^2}{\mathcal{E}_s}\mathbf{I} \bigr)^{-1}\cdot\mathbf{H}^H &\xrightarrow[{\frac{\sigma_n^2}{\epsilon} \to 0}]{}\bigl(\mathbf{H}^H\cdot\mathbf{H}\bigr)^{-1}\mathbf{H}^H = \mathbf{F}_{ZF}\\ &\xrightarrow[\frac{\sigma_n^2}{\mathcal{E}_s} \to\infty]{} \frac{\mathcal{E}_s}{\sigma_n^2} \cdot\mathbf{H}^H = \mathbf{F}_{MF} \quad \widehat{=} \text{matched filter}
		\end{align*}
		\item[$\Rightarrow$] For high SNR, $\frac{\mathcal{E}_s}{\sigma_n^2}$, the MMSE filter approaches the ZF-Filter, for low SNR, it approaches the matched filter. \medskip\\ $\rightarrow$ MMSE receiver yields the same diversity gain as the ZF receiver \medskip\\ $G_d^{MMSE} = G_d^{ZF} = N_R - N_T + 1\leq G_d^{ML} = N_R$
		\item End-to-End Channel: $\mathbf{K} = \mathbf{FH} = \bigl(\mathbf{H}^H\mathbf{H} + \frac{\sigma_n^2}{\mathcal{E}_s}\cdot\mathbf{I}\bigr)^{-1}\cdot\mathbf{H}^H\cdot\mathbf{H} \neq \text{diagonal matrix}$
		\begin{itemize}
			\item[$\Rightarrow$] crosstalk/interference between elements $\mathbf{x}$ in received signal after filtering $\mathbf{r}$.
			\item[] elements of $\mathbf{K}: K_{l,n}$		
		\end{itemize}
		\item Covariance for $\mathbf{F}_{\text{opt}}$				
		\begin{align*}
			\boldsymbol{\Phi}_{ee}  &= \boldsymbol{\Phi}_{xy}\cdot\overbrace{\boldsymbol{\Phi}_{yy}^{-1} \cdot\boldsymbol{\Phi}_{yy}\cdot\boldsymbol{\Phi}_{yy}^{-1}}^{\boldsymbol{\Phi}_{yy}^{-1}}\cdot \boldsymbol{\Phi}_{xy}^H - \boldsymbol{\Phi}_{xy}\cdot\boldsymbol{\Phi}_{yy}^{-1}\cdot\boldsymbol{\Phi}_{yx} - \boldsymbol{\Phi}_{xy}\cdot\boldsymbol{\Phi}_{yy}^{-1}\cdot\boldsymbol{\Phi}_{xy}^H + \boldsymbol{\Phi}_{xx} \\ &= \boldsymbol{\Phi}_{xx} - \overbrace{\boldsymbol{\Phi}_{xy}\cdot\boldsymbol{\Phi}_{yy}^{-1}}^{\mathbf{F}_{\text{opt}}}\cdot\boldsymbol{\Phi}_{yx}\\ &= \mathcal{E}_s\mathbf{I} - \mathcal{E}_s\bigl(\mathbf{H}^H\mathbf{H} + \frac{\sigma_n^2}{\mathcal{E}_s}\cdot\mathbf{I}\bigr)^{-1}\cdot\mathbf{H}^H\mathbf{H}	 = \text{(Matrix inversion Lemma)}\\ &= \sigma_n^2 \bigl(\mathbf{H}^H\mathbf{H} + \frac{\sigma_n^2}{\mathcal{E}_s}\cdot\mathbf{I}\bigr)^{-1}\\
			\boldsymbol{\Phi}_{ee} &= \mathcal{E}_s(\mathbf{I} - \mathbf{K})	
		\end{align*}
  		\begin{itemize}
  			\item[$\rightarrow$] $ 0 \leq K_{m,m} \leq 1 $ since main diagonal elements of $ \boldsymbol{\Phi}_{ee} $ are $ 0 \leq [\boldsymbol{\Phi}_{ee}]_{m,m} \leq \mathcal{E}_s $
  			\item[] siehe auch Abbildung \ref{MMSE}
  		\end{itemize}	
	\end{itemize}
\end{itemize}
	\begin{figure}[h]
		\centering
		\includegraphics[width = \textwidth]{MMSE}
		\caption{MMSE}
		\label{fig:MMSE}
	\end{figure}
	
\paragraph{SNR (biased vs. unbiased)}
\paragraph{a) biased SNR}
\begin{align*}
	\text{SNR}_{\text{bias,m}} &= \frac{\mathcal{E}_s}{[\boldsymbol{\Phi}_{ee}]_{mm}} = \frac{\mathcal{E}_s}{\mathcal{E}_s(1 - K_{mm})} = \frac{1}{1 - K_{mm}}, \quad 1\leq m \leq 4 
\end{align*}
	\textit{Anmerkung: } $\mathbf{K} = \mathbf{F}_{\text{opt}}\cdot\mathbf{H} \rightarrow \text{SNR} = 1 $ \textit{falls } $ \mathbf{K} = \text{zeros()} \Rightarrow \text{woher SNR } = 1 $ \textit{ bei keiner Uebertragung?} $\Rightarrow $ \textit{ nicht vorteilhaft: siehe: b) unbiased SNR}  % \ref{unbiased_SNR}
but: 
\begin{itemize}
	\item $\text{SNR}_{\text{bias,m}} $ does not represent the actual SNR since the main diagonal elements of $ \mathbf{K} $ are smaller than 1
	\item $ \mathbf{r} = \mathbf{FHx} + \mathbf{Fn} = \mathbf{Kx} + \underbrace{\mathbf{F}\mathbf{n} }_{\mathbf{\tilde{n}} = [\tilde{n}_1, \dots,\tilde{n}_{N_T}]^T } $
	\item $r_m = \underbrace{K_{mm}}_{<1}x_m + \underset{ n\neq m}{\sum\limits^{N_T}_{n = 1}}K_{mn}x_n + \tilde{n}_m $
\end{itemize}
\paragraph{b) unbiased SNR}
\label{par:unbiased_SNR}
\begin{itemize}
	\item remove bias: $r_m^\prime = \dfrac{r_m}{K_{mm}} = x_m + \underbrace{\dfrac{\tilde{e}_m}{K_{mm}}}_{e^\prime_m} $
	\item SNR?
	\item scaling matrix: $ \mathbf{C} = \text{diag}\bigl\{\frac{1}{K_{11}}, \frac{1}{K_{22}}, \dots, \frac{1}{K_{N_TN_T}}  \bigr\} $
	\item $\mathbf{r}^\prime = \mathbf{Cr} \rightarrow \mathbf{e}^\prime = [e^\prime_1, \dots, e^\prime_{N_T}]^T = \mathbf{r}^\prime - \mathbf{x}  = \mathbf{Cr} - \mathbf{x} = \mathbf{CFy} - \mathbf{x} $
	\item $\boldsymbol{\Phi}_{e^\prime e^\prime} = \mathcal{E}\Bigl\{\bigr(\mathbf{CFy} - \mathbf{x}\bigr)\bigl(\mathbf{CFy} - \mathbf{x}\bigr)^H\Bigr\} = \mathbf{CF}\boldsymbol{\Phi}_{yy}\mathbf{F}^H\mathbf{C}^H - \mathbf{CF}\boldsymbol{\Phi}_{yx} - \boldsymbol{\Phi}_{xy}\mathbf{F}^H\mathbf{C}^H + \boldsymbol{\Phi}_{xx}$
	\item $ \mathbf{F} = 	\mathbf{F}_{\text{opt}} \rightarrow \mathbf{F}_{\text{opt}}\boldsymbol{\Phi}_{yy} = \boldsymbol{\Phi}_{xy} = \mathcal{E}_s\cdot\mathbf{H}^H $
	\item[$\rightarrow$] $\boldsymbol{\Phi}_{e^\prime e^\prime} = \mathcal{E}_s\mathbf{C}\underbrace{\mathbf{H}^H\mathbf{F}^H}_{\mathbf{K}^H}\mathbf{C}^H - \mathcal{E}_s\mathbf{C}\underbrace{\mathbf{FH}}_{\mathbf{K}} - \mathcal{E}_s\underbrace{\mathbf{H}^H\mathbf{F}^H}_{\mathbf{K}^H}\mathbf{C}  + \mathcal{E}_s\mathbf{I} \\ \quad = \mathcal{E}_s\bigl[\mathbf{I} + (\mathbf{C} - \mathbf{I})\mathbf{K}^H\mathbf{C}^H - \mathbf{CK}\bigr]$  
	\item[] \textit{Anmerkung: nur Hauptdiagonalelemente interessieren, da diese die Varianz darstellen}
	\item[$\rightarrow$] maindiagonal elements of $\boldsymbol{\Phi}_{e^\prime e^\prime} =  $ variances of $ e^\prime _m \quad = \mathcal{E}_s\Bigl ( 1 + \bigl (\frac{1}{K_{mm}} - 1\bigr )   K_{mm}\frac{1}{K_{mm}}  - \frac{1}{K_{mm}}  K_{mm}\Bigr )  = 
	\frac{1 - K_{mm}}{K_{mm}}$
	\item[$\rightarrow$] vgl. Abbildung \ref{MMSE}
	\item[$\rightarrow$]$SNR_{unbiased}=\dfrac{\mathcal{E}_s}{[\boldsymbol{\Phi}_{e^\prime e^\prime}]_{mm}}=\dfrac{\mathcal{E}_s}{\mathcal{E}_s\dfrac{1-K_{mm}}{K_{mm}}}
	=\dfrac{K_{mm}}{1-K_{mm}}, \quad 1\leq m \leq N_T$
	\item[$\rightarrow$]the SNR after bias removal is by ``$1$'' smaller than the biased SNR $\rightarrow$ general result valid for any type of
	MMSE estimation
\end{itemize}
\subsubsection{Decision\,-\,Feadback Equalization (Detection)}
\begin{itemize}
	\item Also known as: 
	\begin{itemize}
		\item BLAST (Bell Laboratories space-time system)
		\item successive interference cancellation		
	\end{itemize}
	\item Problem of linear receiver: Noise enhancementbecause of linear filtering $\rightarrow$ nonlinear filtering processing necessary
\end{itemize}
\paragraph{Basic Idea}
\begin{itemize}
	\item Recall (linear filter): $\mathbf{FH} = \mathbf{I} $ for linear ZF receiver $\rightarrow$ $i$th row of $\mathbf{F}, \mathbf{f}_i, $ is orthogonal to the $j$th column of $\mathbf{H}, \mathbf{h}_j$, if $ i \neq j$ (if $i = j$: inner product = 1)  
	\item we can detect $x_i$, based on $r_i = \mathbf{f}_i\mathbf{y} $
	\item Once we have detected $x_i$, we can substract its contribution from $\mathbf{y}$: $\mathbf{y}_1 = \mathbf{y} - \mathbf{h}_i\hat{x}_i$ ($\hat{x}_i $ is detected symbol, we assume for now, $\hat{x}_i = x_i $) 
	\item $\mathbf{y}_1 $ can be expressed as $ \mathbf{y}_1 = \mathbf{H}_i\mathbf{x}_i + \mathbf{n}$ where 
	\begin{align*}
		\mathbf{H}_i &= \bigl[\mathbf{h}_1, \dots ,\mathbf{h}_{i-1},\mathbf{h}_{i+1},\dots ,\mathbf{h}_{N_T} \bigr] \\
		\mathbf{x}_i &= \bigl[x_1, \dots , x_{i-1}, x_{i+1}, \dots ,x_{N_T}\bigr]
	\end{align*}
	\begin{itemize}
		\item[$\rightarrow$] we have reduced the number of signal streams to $N_T - 1$ ($N_R $ bleibt gleich)
	\end{itemize}
	\item apply now linear ZF filter for symbol to detected next, e.g. $x_j $, where $ j\in \bigl\{1, \dots ,i - 1, i + 1, \dots , N_T\bigr\} $
	\item[$\rightarrow$] $ \mathbf{r}_j = \mathbf{f}_j\mathbf{y}_1 $ where $\mathbf{f}_j $ is the ZF filter for $\mathbf{H}_1$ 
	\item substract contribution of $x_j $ from $\mathbf{y}_1 $: $\mathbf{y}_2 = \mathbf{y}_1 - \mathbf{h}_j x_j $
	\item substract until last symbol is detected
	\item Blockdiagram see figure\ref{DFE} 
	\begin{figure}[h]
		\centering
		\resizebox{\textwidth}{!}{\input{DFE.pstex_t}}
		\caption{DFE Blockdiagram}
		\label{fig:DFE.pstex_t}
	\end{figure}
	\item Observations:
	\begin{itemize}
		\item The order in which the $x_1 $ are detected can be freely chosen and effects the performance $\rightarrow  N_T! $ possible orders $\rightarrow$ cannot explore all of them
		\item Practical approach: Select in each step that $x_i $ for which the noise variance enhancement is minimum, i.e. which has the smallest $\mathcal{E}\Bigl\{|\mathbf{f}_i\mathbf{n}|^2\Bigr\} = \sigma _n^2\||\mathbf{f}_i\||^2 $ 
	\end{itemize}
	\item Diversity order:
	\begin{itemize}
		\item stage 1: $G_d^1 = N_R - N_T + 1 $
		\item stage 2: $G_d^2 = G_d^1 + 1 = N_R - N_T +2 $
		\item[ $\vdots$]
		\item stage $N_T$: $G_d^{N_T} = N_R $
		\item overall: $\underline{G_d = N_R - N_T +1} $
		\item \textit{(Anmerkung: der schlechteste Fall dominiert (Stage 1), weitere koennen nur schwer beeinflussen)}
	\end{itemize}
\end{itemize}
\paragraph{ZF - DFE - Matrix Model}
\begin{itemize}
	\item Signal model: $ \mathbf{y} = \mathbf{Hx} + \mathbf{n} = \underbrace{\mathbf{HP}}_{\tilde{\mathbf{H}}}\cdot\underbrace{\mathbf{P}^{-1}\mathbf{x}}_{\tilde{\mathbf{x}}} + \mathbf{n} $ with permutation matrix $\mathbf{P} $
	\item $\mathbf{P} $ has one \glqq 1\grqq\, per column and row, all other elements are \glqq 0\grqq 
	\item[$\rightarrow$] can change the detection order to maximize performance
	\item note: $\mathbf{P}^T = \mathbf{P}^{-1} $
	\item Example: 
	\begin{align*}
		\mathbf{P} &= \begin{pmatrix*}[r] 0 & 0 & 1\\1 & 0 & 0\\ 0 & 1 & 0	\end{pmatrix*}[r] \rightarrow \mathbf{P}^{-1} = \mathbf{P}^T = \begin{pmatrix*}0 & 1 & 0\\0 & 0 & 1\\ 1 & 0 & 0\end{pmatrix*}\\
		\tilde{\mathbf{x}} &= \mathbf{P}^T\cdot\mathbf{x} = \begin{pmatrix*}[r] 0 & 1 & 0\\ 0 & 0 & 1\\ 1 & 0 & 0\end{pmatrix*}\begin{pmatrix*}[r]x_1 \\x_2\\x_3\end{pmatrix*} = \begin{pmatrix*}[r]x_2\\x_3\\x_1\end{pmatrix*}
	\end{align*}
	\item Blockdiagram see figure \ref{ZF-DFE-Matrix}
	\begin{figure}
		\centering
		\resizebox{\textwidth}{!}{\input{ZF_DFE_Matrix.pstex_t}}
		\caption{ZF-DFE Matrix Model}
		\label{fig:ZF-DFE-Matrix_pstex_t}
	\end{figure}
	\item DFE Filters:
	\begin{itemize}
		\item $\mathbf{F}$ feedworward filter
		\item $\mathbf{B}$ feedback filter
	\end{itemize}
	\item Filter calculation
	\begin{itemize}
		\item Cholesky factorization: $\tilde{\mathbf{H}}^H \tilde{\mathbf{H}} = \mathbf{L}^H\mathbf{DL}$ with diagonal matrix $\mathbf{D}$
			and lower triangular matrix $\mathbf{L}$ ( maindiagonal elements of $\mathbf{L}$ are ``1'')
	\begin{align*}
			\mathbf{L} &= 
			\begin{bmatrix}
				1 & 0 & \cdots & \cdots & 0 \\
				l_{21} & 1 & \ddots & \ddots &	\vdots \\
				l_{31} & l_{32} & 1 & \ddots &\vdots \\
				\vdots & \ddots & \ddots & \ddots & \vdots\\
				\vdots & \cdots & \cdots & \cdots & 1
			\end{bmatrix}
	\end{align*}

		\item $\mathbf{F}=\mathbf{D}^{-1}\mathbf{L}^{-H}\tilde{\mathbf{H}}^H$
		\begin{align*}
			\rightarrow \mathbf{r}=\mathbf{Fy}=\underbrace{\mathbf{D}^{-1}\mathbf{L}^{-H}\underbrace{\tilde{\mathbf{H}}^H\tilde{\mathbf{H}}}_
			{\mathbf{L}^H\mathbf{DL}}}_{\mathbf{L}}\tilde{\mathbf{x}}+\underbrace{\mathbf{D}^{-1}\mathbf{L}^{-H}\tilde{\mathbf{H}}^H\mathbf{n}}_
			{\tilde{\mathbf{n}}}
		\end{align*}
		\item $\mathbf{B}=\mathbf{L-I}=$ lower triangular matrix with maindiagonal elements ``$0$''
	\begin{align*}
			\mathbf{B} &= 
			\begin{bmatrix}
				0 & 0 & \cdots & \cdots & 0 \\
				l_{21} & 0 & \ddots & \ddots &	\vdots \\
				l_{31} & l_{32} & 0 & \ddots &\vdots \\
				\vdots & \ddots & \ddots & \ddots & \vdots\\
				\vdots & \cdots & \cdots & \cdots & 0
			\end{bmatrix}
	\end{align*}

	\end{itemize}
	\item Interpretation:
	\begin{itemize}
		\item after feedforward filter
	\begin{align*}
			\mathbf{r} &= 
			\begin{bmatrix}
				1 & 0 & \cdots & \cdots & 0 \\
				l_{21} & 1 & \ddots & \ddots &	\vdots \\
				l_{31} & l_{32} & 1 & \ddots &\vdots \\
				\vdots & \ddots & \ddots & \ddots & \vdots\\
				\vdots & \cdots & \cdots & \cdots & 1
			\end{bmatrix}
			\begin{bmatrix}
				\tilde{x}_1 \\
				\tilde{x}_2 \\
				\vdots \\
				\tilde{x}_{N_T}
			\end{bmatrix}
			+ \tilde{\mathbf{n}}
	\end{align*}
	\end{itemize}
	\item from feedback filter
	\begin{align*}
			\tilde{\mathbf{r}} &= 
			\begin{bmatrix}
				0 & 0 & \cdots & \cdots & 0 \\
				l_{21} & 0 & \ddots & \ddots &	\vdots \\
				l_{31} & l_{32} & 0 & \ddots &\vdots \\
				\vdots & \ddots & \ddots & \ddots & \vdots\\
				\vdots & \cdots & \cdots & \cdots & 0
			\end{bmatrix}
			\begin{bmatrix}
				\hat{\tilde{x}}_1 \\
				\hat{\tilde{x}}_2 \\
				\vdots \\
				\hat{\tilde{x}}_{N_T}
			\end{bmatrix}
			\text{ $\hat{\tilde{x}}_n$ : decision on $\tilde{x}_n$}\\
			\rightarrow \mathbf{r}-\tilde{\mathbf{r}} &= 
			\begin{bmatrix}
			\tilde{x}_1+0 \\
			l_{21}(\tilde{x}_1-\hat{\tilde{x}}_1)+\tilde{x}_2+0 \\
			l_{31}(\tilde{x}_1-\hat{\tilde{x}}_2)+l_{32}(\tilde{x}_2-\hat{\tilde{x}}_2)+\tilde{x}_3+0 \\
			\vdots
			\end{bmatrix}
			+\tilde{\mathbf{n}}
	\end{align*}
\end{itemize}

\paragraph{MMSE-DFE}
\begin{figure}[h]
	\centering
	\input{MMSE_DFE.pstex_t}
	\caption{MMSE-DFE Blockdiagram}
	\label{fig:MMSE_DFE.pstex_t}
\end{figure}
\begin{itemize}
	\item $\mathbf{B}=\mathbf{L}-\mathbf{I}$ has to be lower triangular matrx with all-zero main diagonal elements because of causality.
	\item $\mathbf{d}=\mathbf{Fy}-\mathbf{B}\hat{\tilde{\mathbf{x}}} \quad \rightarrow \mathbf{e}=\mathbf{d}-\hat{\mathbf{x}}=\mathbf{Fy}-\underbrace{(\mathbf{B}+\mathbf{I})}_{\mathbf{L}}\tilde{\mathbf{x}}$\\
	where we assume $\hat{\tilde{\mathbf{x}}}=\tilde{\mathbf{x}}$ for filter optimization
	\item Optimal $\mathbf{F}$ for given $\mathbf{L}$ \\
	\begin{align*}
		\sigma_e^2=\mathrm{tr}\left\{\underbrace{\mathcal{E}\{\mathbf{ee}^H\} }_{\boldsymbol{\Phi}_{ee}}\right\}
		=\mathrm{tr}\left\{\mathbf{F}\boldsymbol{\Phi}_{yy}\mathbf{F}^H\right\}-\mathrm{tr}\left\{\mathbf{F}\boldsymbol{\Phi}_{y \tilde{x}}\mathbf{L}^H\right\}
		-\mathrm{tr}\left\{\mathbf{L}\boldsymbol{\Phi}_{\tilde{x} y}\mathbf{F}^H\right\}
		+\mathrm{tr}\left\{\mathbf{L}\boldsymbol{\Phi}_{\tilde{x} \tilde{x}}\mathbf{L}^H\right\}
	\end{align*}
	\begin{align*}
		\dfrac{\delta}{\delta \mathbf{F}^*}\sigma_e^2=\mathbf{F}\boldsymbol{\Phi}_{yy}-\mathbf{L}\boldsymbol{\Phi}_{\tilde{x}y}=0 \quad \rightarrow \mathbf{F}
		=\mathbf{L}\boldsymbol{\Phi}_{\tilde{x}y}\boldsymbol{\Phi}_{yy}^{-1}
	\end{align*}
	where
	\begin{align*}
		\boldsymbol{\Phi}_{yy}&=\mathcal{E}_s\tilde{\mathbf{H}}\tilde{\mathbf{H}}^H+\sigma_n^2\mathbf{I}\\
		\boldsymbol{\Phi}_{\tilde{x}y}&=\mathcal{E}_s\tilde{\mathbf{H}}^H
	\end{align*}
	\begin{align*}
		\rightarrow \mathbf{F}=\mathbf{L}\tilde{\mathbf{H}}^H(\tilde{\mathbf{H}}\tilde{\mathbf{H}}^h+\dfrac{\sigma_n^2}{\mathcal{E}_s}\mathbf{I})^{-1}
		=\mathbf{L}\overbrace{(\tilde{\mathbf{H}}^H\tilde{\mathbf{H}}+\dfrac{\sigma_n^2}{\mathcal{E}_s}\mathbf{I})^{-1}\tilde{\mathbf{H}}^H}^{\text{MMSE Linear Equalizer}}
	\end{align*}
	\begin{figure}[h]
		\centering
		\resizebox{\textwidth}{!}{\input{DFE_MMSE_EQUIV.pstex_t}}
		\caption{MMSE-DFE Equivalence}
		\label{fig:MMSE_DFE_Equiv.pstex_t}
	\end{figure}
	\item Optimal $\mathbf{L}$:
	\begin{align*}
		\boldsymbol{\Phi}_{ee}&=\mathbf{L}\boldsymbol{\Phi}_{\tilde{x}y}\boldsymbol{\Phi}_{yy}^{-1}\boldsymbol{\Phi}_{yy}\boldsymbol{\Phi}_{yy}^{-1}\boldsymbol{\Phi}_{\tilde{x}y}^H\mathbf{L}^H
		-\mathbf{L}\boldsymbol{\Phi}_{\tilde{x}y}\boldsymbol{\Phi}_{yy}^{-1}\boldsymbol{\Phi}_{y\tilde{x}}\mathbf{L}^H
		-\mathbf{L}\boldsymbol{\Phi}_{\tilde{x}y}\boldsymbol{\Phi}_{yy}^{-1}\boldsymbol{\Phi}_{\tilde{x}y}^H\mathbf{H}^H+\mathbf{L}\boldsymbol{\Phi}_{\tilde{x}\tilde{x}}\mathbf{L}^H\\ 
		&=\mathbf{L}(\boldsymbol{\Phi}_{\tilde{x}\tilde{x}}-\boldsymbol{\Phi}_{\tilde{x}y}\boldsymbol{\Phi}_{yy}^{-1}\boldsymbol{\Phi}_{\tilde{x}y}^H)\mathbf{L}^H
	\end{align*}
	$=$ MMSE covariance matrix for a linear MMSE filter
	\begin{itemize}
		\item[$\rightarrow$]$\boldsymbol{\Phi}_{ee}=\sigma_n^2\mathbf{L}(\tilde{\mathbf{H}}^H\tilde{\mathbf{H}}+\dfrac{\sigma_n^2}{\mathcal{E}_s}\mathbf{I})^{-1}\mathbf{L}^H$
		\item[$\rightarrow$] need lower triangular matrix $\mathbf{L}$ which minimizes $\mathrm{tr}\{\boldsymbol{\Phi}_{ee}\}$
		\item[$\rightarrow$] the optimum $\mathbf{L}$ whitenes $\boldsymbol{\Phi}_{ee}$ $\rightarrow$  $\boldsymbol{\Phi}_{ee}$ becomes diagonal matrix i.e. $\mathbf{L}$ exploits the correlation after linear MMSE filtering
		to reduce noise variance!
		\item[$\rightarrow$] $\mathbf{L}$ is obtained via Cholesky factorization
		\begin{align*}
			\tilde{\mathbf{H}}^H\tilde{\mathbf{H}}+\dfrac{\sigma_n^2}{\mathcal{E}_s}\mathbf{I}=\mathbf{LDL}^H
		\end{align*}
		\item[$\rightarrow$] $\boldsymbol{\Phi}_{ee}=\sigma_n^2 \mathbf{L}(\mathbf{LDL}^H)^{-1}\mathbf{L}=\sigma_n^2\mathbf{LL}^{-1}\mathbf{D}^{-1}\mathbf{L}^{-H}\mathbf{L}=\sigma_n^2\mathbf{D}^{-1}\widehat{=}$ diagonal matrix
	\end{itemize}
	\item Summary of MMSE calculation
	\begin{itemize}
		\item $\mathbf{B}=\mathbf{L}-\mathbf{I}$ where $\mathbf{H}^H\mathbf{H}+\dfrac{\sigma_n^2}{\mathcal{E}_s}\mathbf{I}=\mathbf{LDL}^H$
		\item $\mathbf{F}=\mathbf{L} \underbrace{(\mathbf{H}^H\mathbf{H}+\dfrac{\sigma_n^2}{\mathcal{E}_s}\mathbf{I})^{-1}\tilde{\mathbf{H}}}_{\mathbf{F}_{Linear}}$
		\item $\mathbf{L}$ is a prediction error filter, which whitenes the error signal $\mathbf{e}$
		\item SNR:
		\begin{align*}
			\mathbf{D}&=\mathrm{diag}\{\xi_1,\dots,\xi_{N_T}\}\\
			\boldsymbol{\Phi}_{ee}&=\mathrm{diag}\{\dfrac{\sigma_n^2}{\xi_1},\dots,\dfrac{\sigma_n^2}{\xi_{N_T}}\}
		\end{align*}
		\item end-to-end channel
		\begin{align*}
			\mathbf{K}&=\mathbf{F}\tilde{\mathbf{H}}\\
				&=\underbrace{\mathbf{L}(\tilde{\mathbf{H}}^H \tilde{\mathbf{H}}+ \dfrac{\sigma_n^2}{\mathcal{E}_s}\mathbf{I})^{-1}\mathbf{L}^H}_{\dfrac{1}{\sigma_n^2}\boldsymbol{\Phi}_{ee}} 
				\mathbf{L}^{-H} \tilde{\mathbf{H}}^H\tilde{\mathbf{H}}\\
				&= \dfrac{1}{\sigma_n^2} \boldsymbol{\Phi}_{ee} \mathbf{L}^{-H}(\tilde{\mathbf{H}}^H\tilde{\mathbf{H}}+\dfrac{\sigma_n^2}{\mathcal{E}_s}\mathbf{I}-\dfrac{\sigma_n^2}{\mathcal{E}_s}\mathbf{I})
				\mathbf{L}^{-1}\mathbf{L}\\
				&= \dfrac{1}{\sigma_n^2}[\underbrace{\mathbf{L}^{-H}(\tilde{\mathbf{H}}^H\tilde{\mathbf{H}}+\dfrac{\sigma_n^2}{\mathcal{E}_s}\mathbf{I})\mathbf{L}^{-1}}_{\sigma_n^2\boldsymbol{\Phi}_{ee}^{-1}}
				\mathbf{L}-\dfrac{\sigma_n^2}{\mathcal{E}_s}\mathbf{L}^{-H}]\\
				&=\mathbf{L}-\dfrac{1}{\mathcal{E}_s}\boldsymbol{\Phi}_{ee}\mathbf{L}^{-H}
		\end{align*}
		$\rightarrow$ main diagonal of $\mathbf{K}$:
		\begin{align*}
			K_{mm}=1-\dfrac{\sigma_n^2}{\mathcal{E}_s\xi_m} < 1
		\end{align*}
		\item biased SNR
		\begin{align*}
			SNR_{m,bias}=\dfrac{\mathcal{E}_S}{[\boldsymbol{\Phi}_{ee}]_{mm}}=\dfrac{\mathcal{E}_s}{\sigma_n^2}\xi_m=\dfrac{1}{1-K_{mm}}, \quad 1 \leq m \leq N_T
		\end{align*}
		\item unbiased SNR
		\begin{align*}
			SNR_{m,unbiased}=SNR_{m,bias}-1=\dfrac{1}{1-K_{mm}}-1=\dfrac{K_{mm}}{1-K_{mm}}, \quad 1 \leq m \leq N_T
		\end{align*}
	\end{itemize}
\end{itemize}

\subsubsection{Sphere Decoding}

\begin{itemize}
	\item Linear and DFE receivers cannot approach performance of ML\,-\,detector
	\item ML\,-\,detector: $ \hat{\mathbf{x}} = \underset{\mathbf{x} \in A^{N_T}}{\text{argmin}}||\mathbf{y} - \mathbf{Hx}||^2 $
	\item[$\rightarrow$] high complexity for brute force search
\end{itemize}
\paragraph{Main Idea}
\begin{itemize}
	\item can we search ML metric in a ``smarter'' way, akin to the smart search of the viterbi algorithm for problems with a trellis structure
	\item we need to find a way to prune/dismiss non\,-\,ML sequences/vectores early on
	\item[$\rightarrow$] this is accomplished by \underline{sphere decoding}	
\end{itemize}
\paragraph{Step 1}
Bring metric into a suitable form:
\begin{itemize}
	\item real represantation: $||\mathbf{y} - \mathbf{Hx}||^2 = ||\tilde{\mathbf{y}} - \tilde{\mathbf{H}}\tilde{\mathbf{x}}||^2 $
	\item where:
	\begin{align*}
		\tilde{\mathbf{y}} &= \begin{bmatrix*} \operatorname{Re}\{\mathbf{y}^T\} & \operatorname{Im}\{\mathbf{y}^T \}	\end{bmatrix*}^T \\
		\tilde{\mathbf{x}} &= \begin{bmatrix*} \operatorname{Re}\{\mathbf{x}^T\} & \operatorname{Im}\{\mathbf{x}^T \}	\end{bmatrix*}^T \\
		\tilde{\mathbf{H}} &= \begin{bmatrix*}[r] \operatorname{Re}\{\mathbf{H}\} & -\operatorname{Im}\{\mathbf{H} \} \\ \operatorname{Im}\{ \mathbf{H}\} &\operatorname{Re}\{\mathbf{H} \}\end{bmatrix*}^T 		
	\end{align*}
	\item[$\rightarrow$] QAM: $\tilde{\mathbf{x}}$ defines points in an $ 2N_T $ dimensional lattice
	\item QL decomposition: $ \tilde{\mathbf{H}} = \mathbf{QL} $ with
	\begin{itemize}
		\item orthogonal matrix $\mathbf{Q} \in\mathbb{R} ^{2N_T\times 2N_T};\quad \mathbf{QQ}^T = \mathbf{I}$ 
		\item and lower triangular matrix $\mathbf{L} \in \mathbb{R} ^{2N_T\times2N_T}$	
	\end{itemize}
	\begin{align*}
		||\tilde{\mathbf{y}} - \mathbf{QL}\tilde{\mathbf{x}}||^2 &= 
		||\mathbf{QQ}^T(\tilde{\mathbf{y}} - \mathbf{QL}\tilde{\mathbf{x}})||^2 + ||(\mathbf{I} - \mathbf{QQ}^T)(\tilde{\mathbf{y}} - \mathbf{QL}\tilde{\mathbf{x}})||^2 \\ 
		&= ||\mathbf{Q}(\mathbf{Q}^T\tilde{\mathbf{y}} - \mathbf{L}\tilde{\mathbf{x}})||^2 + ||(\mathbf{I} - \mathbf{QQ}^T)\tilde{\mathbf{y}} - \underbrace{(\mathbf{Q} - \mathbf{Q})}_{ = 0}\mathbf{L}\tilde{\mathbf{x}}||^2 \\ &= ||\mathbf{Q}^T\tilde{\mathbf{y}} - \mathbf{L}\tilde{\mathbf{x}}||^2 + \underbrace{||(\mathbf{I} - \mathbf{QQ}^T)\tilde{\mathbf{y}}||^2}_{\text{unabh\"angig von }\tilde{\mathbf{x}}}
	\end{align*}		 
	\item[$\Rightarrow $] $ \underset{\tilde{\mathbf{x}} \in A^{2N_T}}{\text{argmin}} ||\tilde{\mathbf{y}} - \tilde{\mathbf{H}}\tilde{\mathbf{x}}||^2 = \underset{\tilde{\mathbf{x}} \in A^{2N_T}}{\text{argmin}} ||\underbrace{\mathbf{Q}^T\tilde{\mathbf{y}}}_{\bar{\mathbf{y}}} - \mathbf{L}\tilde{\mathbf{x}}||^2$\\
	 where $A^{2N_T} $ contains all $ M^{N_T} $ possible vectors $\tilde{\mathbf{x}}$
	\item Observation:
	\begin{itemize}
		\item with
		\begin{align*}
			\mathbf{L} &= 
			\begin{bmatrix}
				l_{11} & 0 & \cdots & 0 \\
				l_{21} & l_{22} & \ddots &	\vdots \\
				\vdots & & \ddots & 0\\
				l_{2N_T\,1} & \cdots & & l_{2N_T\,2N_T}
			\end{bmatrix}\\
			\bar{\mathbf{y}} &= 
			 \begin{bmatrix}
			 	\bar{y}_1 & \cdots	& \bar{y}_{2N_T}
			 \end{bmatrix}^T \, ;\quad	
			 \tilde{\mathbf{x}} =
			 \begin{bmatrix}
			 	\tilde{x}_1 & \cdots	 & \tilde{x}_{2N_T}
			 \end{bmatrix}		 
		\end{align*}
		\item we have 
		\begin{align}\label{eq:Sphere_Decoding_01}
			||\bar{\mathbf{y}} - \mathbf{L}\tilde{\mathbf{x}}||^2 = (\bar{y} - l_{11}\tilde{x}_1)^2 + (\bar{y}_2 - l_{21}\tilde{x}_1 - l_{22}\tilde{x}_2)^2 + (\bar{y}_3 - l_{31}\tilde{x}_1 - l_{32}\tilde{x}_2 - l_{33}\tilde{x}_3)^2 + \ldots
		\end{align}
		\end{itemize}
		\item[$\rightarrow$] the $n$-th term in the above sum contains only $ \tilde{x}_1, \tilde{x}_2, \ldots,\tilde{x}_n $
\end{itemize}
\paragraph{Step 2}
Sphere decoding algorithm\\ 
\underline{\textbf{Define:}}
\begin{align*}
	 d(\tilde{\mathbf{x}}) &= \sum_{n = 1}^{2N_T} f_n(\tilde{\mathbf{x}}_n) \ ;\quad \text{with}\ f_n(\tilde{\mathbf{x}}_n) = \bigl (\bar{y}_n - \sum_{i = 1}^{n}l_{ni}\tilde{x}_i\bigr)^2\ ;\quad \tilde{\mathbf{x}}_n = 
	 \begin{bmatrix}
	 	\tilde{x}_1 & \ldots & \tilde{x}_n
	 \end{bmatrix}^T \\
	 d_n(\tilde{x}_n) &= \sum_{m = 1}^{n} f_n(\tilde{\mathbf{x}}_m)
\end{align*}
\underline{\textbf{Main Idea:}}
\begin{itemize}
	\item Assume we know that $ d(\tilde{\mathbf{x}}) \leq R $ holds for some $\tilde{\mathbf{x}} $, where $ R $ is the so called ``sphere radius" \quad	$\rightarrow$ any $ \tilde{\mathbf{x}} $ with $ d(\tilde{\mathbf{x}}) \geq R $ cannot be the ML solution and can be discarded
	\item since $ d_{n + 1}(\tilde{\mathbf{x}}_n^\prime, \tilde{x}_{n + 1}) \geq d_n(\tilde{\mathbf{x}}_n^\prime) $, we can easily discard $ \mathbf{x}_n^\prime $ and all \\ possible $ \tilde{\mathbf{x}} = \begin{bmatrix}
	\tilde{\mathbf{x}}_n^{\prime} & \tilde{x}_{n+1} & \tilde{x}_{n+2} & \ldots & \tilde{x}_{2N_T} \end{bmatrix}^T $ if we find $ d_n(\tilde{\mathbf{x}}_n^\prime) > R \quad $ \\ $ \Rightarrow $ we can exclude many possible vectors $\tilde{\mathbf{x}} $ without evaluating  any metrics for them 
	\item How to find a suitable $R$?
	\begin{itemize}
		\item Initial $R$: $R = d(\tilde{\mathbf{x}}_{\text{subopt}}) $ where $ \tilde{\mathbf{x}}_{\text{subopt}} $ was obtained with some suboptimum receiver
		\item $R $ is updated as $ R = R_{\text{new}} = d(\tilde{\mathbf{x}}_{\text{cand}}) $ where $ \tilde{\mathbf{x}}_{\text{cand}} $ is an $ \tilde{\mathbf{x}} $ which \\ yields  $ d(\tilde{\mathbf{x}}_{\text{cand}}) < R_{\text{old}} = R $
	\end{itemize}
	\item Use tree structure to represent all possible $\tilde{\mathbf{x}} $ 
\end{itemize}
\subparagraph{Example:}	

% Optionen für Stil, der im Bild verwendet werden kann
% Verwendung: \node [baumnode]
%
%\tikzset{
% 	baumnode/.style = {fill = gray, circle,draw}
% } 
% 
%\newpage
\begin{figure}[h]

\scalebox{0.8}[0.8]{ 
\begin{tikzpicture}[->,>=stealth']
\tikzstyle{every node} = [font = \small]
	\node [circle , draw]  (Start) {Start} 
		child {node [ circle, draw] (links) {links}
			edge from parent
			node[left] {$-1$}				
		}
		child {node [ circle, draw] (rechts) {rechts}
			edge from parent
			node[right] {$+1$}				
		}
	;
\end{tikzpicture}
}
\begin{center}
\begin{tikzpicture}%[rotate = 90]
\tikzstyle{level 1}=[sibling distance = 77mm]
\tikzstyle{level 2}=[sibling distance = 40mm]
\tikzstyle{level 3}=[sibling distance = 20mm]
\tikzstyle{level 4}=[sibling distance = 10mm]
%\tikzstyle{level 5}=[sibling distance = 30mm]
\tikzstyle{every node} = [font = \small]
		\node [circle, draw] (0) {0}
			child {node [circle,draw] (1) {1}
				child {node (0) [circle,draw] (2) {2}
					child {node [circle,draw] (4) {4}
						child {node [circle,draw] (8) {8}
							edge from parent
							node[above ] {$f_4(\ldots) = 4$}
						}
						child {node [circle,draw] (9) {9}
							child[grow=down] {node (x_subopt) {$\hat{x}_{\text{subopt}} = [-1,-1,-1,+11]^T$} edge from parent[->,>=stealth']}					
	%						child[missing] {node (Platzhalter){Platzhalter}}
							edge from parent
							node[below ] {$f_4(\ldots) = 2$}						
						}	
						edge from parent
						node[above ] {$f_3(\ldots) = 2$}		
					}
					child {node [circle,draw] (5) {5}
						child {node [circle,draw] (10) {10}
%							child[missing] {node (Platzhalter){Platzhalter}}
%							child  {node (x_ML) {$\hat{x}_{\text{ML}} = [-1,-1,+1,-11]^T$} edge from parent[->,>=stealth']}	
							edge from parent
							node[above ] {$f_4(\ldots) = 1$}						
						}
						child {node [circle,draw] (11) {11}
							edge from parent
							node[below ] {$f_4(\ldots) = 3$}						
						}			
						edge from parent
						node[below ] {$f_3(\ldots) = 1$}				
					}
					edge from parent
					node[left ] {$	f_2(-1,-1) = 2$}
				}
				child {node [circle,draw] (3) {3}
					child {node [circle,draw] (6) {6}
						child {node [circle,draw] (12) {12}}
						child {node [circle,draw] (13) {13}}		
						edge from parent
						node[above] {$f_3(\ldots) = 6$}			
					}
					child {node [circle,draw] (7) {7}
						child {node [circle,draw] (14) {14}
							child[grow=down]  {node (x_ML) {$\hat{x}_{\text{ML}} = [-1,-1,+1,-11]^T$} edge from parent[draw=none] }									}
						child {node [circle,draw] (15) {15}}	
						edge from parent
						node[below] {$f_3(\ldots) = 7$}	
					}				
					edge from parent
					node[right ] {$f_2(-1,+1) = 1$}				
				}	
				edge from parent
				node[left ] {$	f_1(-1) = 1$}		
			}
			child {node [circle,draw] (16) {16}
				child {node [circle,draw] (17) {17}
					child {node [circle,draw] (19) {19}
						child {node [circle,draw] (23) {23}}
						child {node [circle,draw] (24) {24}}						
						}
					child {node [circle,draw] (20) {20}
						child {node [circle,draw] (25) {25}}
						child {node [circle,draw] (26) {26}}						
						}
					edge from parent
					node[above] {$f_2(+1,-1) = 4$}		
					}
				child {node [circle,draw] (18) {18}
					child {node [circle,draw] (21) {21}
						child {node [circle,draw] (27) {27}}
						child {node [circle,draw] (28) {28}}						
					}
					child {node [circle,draw] (22) {22}
						child {node [circle,draw] (29) {29}}
						child {node [circle,draw] (30) {30}}						
					}
					%child[grow=down] {node (Punkte) {$\vdots$} edge from parent[draw=none]}
					edge from parent
					node[below] {$f_2(+1,+1) = 3$}						
				}	
				edge from parent
				node[right] {$f_1(+1) = 5$}				
			}
			;
			\draw[->,>=stealth'] (10)--(x_ML)	;

\end{tikzpicture}
	\caption{$\text{BPSK} = 2,\ N_T = 2$}
	\label{fig:tikz_Sphere_Decoding}
\end{center}
\end{figure}



\begin{itemize}
	\item Siehe dazu Abbildung \ref{fig:tikz_Sphere_Decoding} auf S. \pageref{fig:tikz_Sphere_Decoding}
	\item Assume $\tilde{\mathbf{x}}_{\text{subopt}} = \begin{bmatrix} -1 & -1 & -1 & 1	\end{bmatrix}^T $ \\ $ \quad \Rightarrow d(\tilde{x}_{\text{subopt}} = 1 + 2 + 2 + 2 = 7 = R $
	\item For $\tilde{x}_1 = 1 $ we find $ d_2(1,1) = 5 + 3 = 8 > R $ and $ d(1,-1) = 5 + 4 > R $ \\ $ \quad\Rightarrow $ we don't have to calculate metrics for remaining branches, i.e., for nodes $19, \ldots ,31 $
	\item For $ \tilde{x}_1 = -1 $ and $ \tilde{x}_2 = 1$ , we find that $d_3(-1, 1, -1) = 8 > R $ and $ d_3(-1, 1, 1) = 9 > R$ \\ $ \quad\Rightarrow $ remaining branches emerging from nodes 6 and 7 can be discarded
	\item For the ML solution we find $d(-1, -1, 1, -1) = 5 < R $
	\item Different strategies exist, regarding in which order the nodes in the tree (points in the lattice) are investigated
	\begin{itemize}
		\item Pohst strategy
		\item Schnoir\,-\,Enchner strategy
		\item rich literature on lattice decoding algorithms
	\end{itemize}
	\item Complexity of sphere decoding
	\begin{itemize}
		\item worst-case complexity is still exponential in $N_T$
		\item (for practical case:) for sufficiently high SNR, the average complexity is only polynomial in $N_T \Rightarrow $ efficient ML decoding
	\end{itemize}		
	\item sphere decoding has found application in many fields:
	\begin{itemize}
		\item ML detection in MIMO and multiuser systems
		\item precoding
		\item source coding
		\item multiple symbol differential detection
	\end{itemize}
\end{itemize}
\newpage
\section{Multiuser MIMO}
\begin{itemize}
	\item We distinguish two cases:
	\begin{itemize}
		\item multipoint\,-\,to\,-\,point transmission
		\item point\,-\,to\,-\,multipoint transmission
	\end{itemize}
	\item Multipoint\,-\,to\,-\,point transmission
	\begin{itemize}
		\item typical uplink scenario in cellular systems
		\item information theoretical channel model: Multiple Access Channel (MAC)
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.8]{MAC}
			\caption{Multiple Access Channel}
			\label{fig:MAC}		
		\end{figure}
%		\begin{tikzpicture}
%			\node (user1) at (0,0) [draw] {User 1};
%			\node (user2) at (1,-1)[draw] {User 2};
%			\node (user3) at (2,-2)[draw] {User 3};		
%			\draw[-<] (user1.east) --(1,0) --(1,0.5);        
%			\draw[-<] (user2.east) --(2,-1) --(2,-0.5); 
%			\draw[-<] (user3.east) --(3,-2) --(3,-1.5); 
%			\node[draw, minimum width = 2cm, minimum height = 2.5cm, font = \bfseries\large] (Joint_Receiver) at (10,-1) {Joint Receiver};
%			\path[draw,-<] ([xshift = 0cm, yshift = 0.5cm] Joint_Receiver.west)--([xshift = -1cm, yshift = 0.5cm] Joint_Receiver.west);
%			\path[draw,-<] (Joint_Receiver.west)--([xshift = -1cm, yshift = 0cm] Joint_Receiver.west) ;
%			\path[draw,-<] ([xshift = 0cm, yshift = -0.5cm] Joint_Receiver.west)--([xshift = -1cm, yshift = -0.5cm] Joint_Receiver.west);
%			\path[draw,->,>=stealth'] ([xshift = 0.6cm,yshift = 0.4cm] user1.east)--([xshift = -2.5cm, yshift = 0.5cm] Joint_Receiver.west);
%			\path[draw,->,>=stealth'] ([xshift = 0.6cm,yshift = 0.4cm] user2.east)--([xshift = -2.5cm, yshift = 0cm] Joint_Receiver.west);
%			\path[draw,->,>=stealth'] ([xshift = 0.6cm,yshift = 0.4cm] user3.east)--([xshift = -2.5cm, yshift = -0.5cm] Joint_Receiver.west);
%		\end{tikzpicture}
	\end{itemize}
	\item Point\,-\,to\,-multipoint transmission
	\begin{itemize}
		\item typical downlink scenarion in cellular systems
		\item information theoretical channel model: Broadcast Channel (BC)
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.8]{BC}
			\caption{Broadcast Channel}		
			\label{fig:BC}
		\end{figure}	
	\end{itemize}
	\item Advantage of multiuser MIMO compared to point\,-\,to\,-\,point MIMO
	\begin{itemize}
		\item multiplexing gain can be exploited even if users have only single antenna
		\item users are spatially distributed in cell $\rightarrow $ channels to different users are independent
	\end{itemize}
\end{itemize}

\subsection{Multiple Access Channel (MAC)}
We consider two aspects:
\begin{itemize}
	\item Detector structures
	\item Rate region
\end{itemize}
\subsubsection{Detector structures}
\paragraph*{Channel model:}
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.8]{Detector_Structures_Channel_Model}
	\caption{Block Diagramm of Channel Model}
	\label{fig:Detector_Structures_Channel_Model}
\end{figure}
$\rightarrow  $ general MAC: $\mathbf{y} = \sum\limits_{k = 1}^{K}\mathbf{H}_k\mathbf{x}_k + \mathbf{n} $\\
with: \begin{itemize}
	\item K users
	\item user $k $ has $N_{T,k} $ transmit antennas
	\item $N_R $ receive antennas
	\item $\mathbf{H}_k \in \mathbb{C}^{N_R\times N_{T,k}} $ 
\end{itemize}
\begin{align*}
	\mathbf{y} = 
	\underbrace{
	\begin{bmatrix}\mathbf{H}_1 & \mathbf{H}_2 & \ldots & \mathbf{H}_k 	
	\end{bmatrix}
	}_{\mathbf{H}}\cdot
	\underbrace{
	\begin{bmatrix}\mathbf{x}_1 \\ \vdots \\ \mathbf{x}_k		
	\end{bmatrix}}_{\mathbf{x}} + \mathbf{n}
\end{align*}
\paragraph*{Observation:}
\begin{itemize}
	\item same equivalent channel model as for a point\,-\,to\,-\,point MIMO system transmitting $N_T = \sum_{k = 1}^{K} N_{T,k} $ independent signal streams \quad \textit{(Anmerkung: kein Unterschied f\"ur Empf\"anger, ob Signale von einem Nutzer oder von mehreren)}
	\item the receiver (e.g. base station) can use detection schemes as for point\,-\,to\,-\,point MIMO systems
	\begin{itemize}
		\item linear receiver
		\item DFG
		\item sphere decoder
	\end{itemize}
\end{itemize}
\paragraph*{Typical problems in uplink multiuser MIMO}
For given receiver structure:
\begin{itemize}
	\item calculate $\text{SNR}_k $ for all users $k $ based on the expressions developed in Chapter 2.4
	\item optimize transmit power of users, $E_k = \mathcal{E}\bigl\{||x_k||^2\bigr\} $ for maximization of the sumrate or maximization of the minimum $\text{SNR}_k $ \quad \textit{(Anmerkung: Maximierung der \textit{sumrate} kann durch Maximierung des SNR des Users mit bestem Kanal erfolgen, aber: unfair anderen Usern gegen\"uber $\Rightarrow $ \textit{starving})}
\end{itemize}
\subsubsection{Rate region}
For point\,-\,to\,-\,point links, we can decode error free, if the rate, $R$, meets
  \begin{itemize}
     \item[a)] SISO	$R < \log_2\bigl(1+\frac{\mathcal{E}_s}{\sigma_n^2}	\bigr) $
     \item[b)] MIMO $R < \log_2\underbrace{\bigl|\mathbf{I} + \frac{\mathcal{E}_s}{N_T\sigma_n^2} \mathbf{HH}^H\bigr|}_{\text{det}} $
	\end{itemize} 
Questions: What happens if there are multiple users?
\paragraph{Rate Region for Single Antenna Users and Receivers}
\begin{itemize}
	\item Gaussian channel
	\item $N_R = N_{T,k} = 1 \forall\quad k $
	\item received signal: 
	\begin{align*}
		y &= \sum\limits_{k = 1}^{K}x_k + n\\ * \mathcal{E}_k &= \mathcal{E}\bigl\{||x_k||^2\bigr\} \\ *\sigma_n^2 &= \bigl\{||n||^2\bigr\}
	\end{align*}
\end{itemize}
\subparagraph*{Example: 2 Users}
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{Rate_Region_Example}
	\caption{Block Diagramm}
	\label{fig:Rate_Region_Example}
\end{figure}
\begin{itemize}
	\item How should we choose $R_1 $ and $R_2 $ to ensure error free decoding of \underline{both} signal streams?
	\item It is no longer sufficient to maximizie a single rate. Instead we have to consider rate pairs $(R_1, R_2)$
	\item All possible rate points, that allows error free decoding, define the rate region $\underline{C} $
	\item Possible desing goals of the system:
	\begin{itemize}
		\item maximized sumrate $R_{\text{sum}} = \underset{(R_1, R_2) \in \underline{C}}{\text{max}} R_1 + R_2 $ 
		\item maximize minimum user rate: $R_{\text{max-min}} = \underset{(R_1, R_2) \in \underline{C}}{\text{max}} \, \underset{i \in \{1, 2\}}{\text{min}} R_i $ 
	\end{itemize}
	\item Rate Region of two user Gaussian MAC \quad\textit{Anmerkung: Einschr\"ankungen}
	\begin{align}
		R_1 &< \log_2\bigl(1 + \frac{\mathcal{E}_1}{\sigma_n^2}\bigr) \label{eq:Formel_1}\\ 
		R_2 &< \log_2\bigl(1 + \frac{\mathcal{E}_2}{\sigma_n^2}\bigr)\label{eq:Formel_2} \\ 
		R_1 + R_2 &< \log_2\bigl(1 + \frac{\mathcal{E}_1 + \mathcal{E}_2}{\sigma_n^2}\bigr) 	\label{eq:Formel_3}	
	\end{align}
	\item Interpretation:
	\begin{itemize}
		\item \eqref{eq:Formel_1} and \eqref{eq:Formel_2}  (= single\,-to\,user constraint) are the ``single\,-\,user bounds´´, i.e., the maximum rates of user 1 and 2, if the other user was not there
		\item \eqref{eq:Formel_3} can be interpreted as the maximum rate if streams of users 1 and 2 were jointly encoded. The separate encoding in the MAC cannot yield a better performance
		\item Graphical represantation: \\
		\begin{tikzpicture}
		\tikzstyle{every node} = [font = \small]
			\path[draw,->,>=stealth'](0,-0.3)--
				(0,3) node [left] {$\log_2(1+\frac{\mathcal{E}_2}{\mathcal{E}_1 + \sigma_n^2})$}--
				(0,4) node [left] {$\log_2(1+\frac{\mathcal{E}_2}{\sigma_n^2})$}--
				(0,2) node [right] {achievable rate}--		
				(0,6) node [left] {$R_2$};
			\path[draw,->,>=stealth'](-0.3,0)--
				(3,0) node [above] {$\log_2(1+\frac{\mathcal{E}_1}{\mathcal{E}_2 + \sigma_n^2})$}--
				(4,0) node [below] {$\log_2(1+\frac{\mathcal{E}_1}{\sigma_n^2})$}--
				(6,0) node [below] {$R_1$};
			\path[draw](4,0)--
				(4,0.2) node [right] {D}--
				(4,2.8) node [left] {C}--
				(4,3) node {};
			\path[draw,dashed](3,0)--
				(3,4) node [above] {B};
			\path[draw](0,4)--
				(0.2,4) node [above] {A}--
				(3,4) node {};
			\path[draw,dashed](0,3)--
				(4,3) node {};
			\path[draw](3,4)--
				(4,3) node [above] {dominant face};
			\path[draw,dashed](1,6)--
				(6,1) node [above] {$R_1 + R_2 = \log_2(1 + \frac{\mathcal{E}_1 + \mathcal{E}_2}{\sigma_n^2}) $};
		\end{tikzpicture}
	\end{itemize}
	
	\item Observations:
	\begin{itemize}
		\item A\,-\,B is defined by \eqref{eq:Formel_2}
		\item C\,-\,D is defined by \eqref{eq:Formel_1}
		\item B\,-\,C is defined by \eqref{eq:Formel_3}
		\item A\,-\,B suggests that even if user 2 transmits with the same max. rate as in the single user case, user 1 can transmit with non-zero rate! $\rightarrow $ Multiuser communication enables ``free´´ rate gains!
		\item Which point on A\,-\,B\,-\,C,-\,D we choose, depends on the design criterion 	
	\end{itemize}		
	\item How do we achieve points on 	A\,-\,B\,-\,C,-\,D? 
	\begin{itemize}
		\item Both user use Gaussian codebooks
		\item B: 
		\begin{itemize}
			\item signal of user 1, $x_1$, is decoded first and $x_2 $ is treated as noise:
			\begin{align*}
				y &= x_1 + \underbrace{x_2 + n}_{\text{treat as noise}}\\ \rightarrow R_1 &< \log_2\bigl(1 + \frac{\mathcal{E}_1}{\mathcal{E}_2 + \sigma_n^2}\bigr)
			\end{align*}
			\item once $x_1 $ is known, we form 
			\begin{align*}
				y -x_1 &= x_2 + n\\ \rightarrow R_2 &< \log_2\bigl(1 + \frac{\mathcal{E}_s}{\sigma_n^2}\bigr)
			\end{align*}
			\item this approach is referred to as successive interference cancellation (SIC) and is a direct result of the chain rule in information theory:\\ $I(X_1, X_2, Y) = I(X_1, Y) + I(X_2; Y|X_1) $
		\end{itemize}
		\item C: same as B, but $X_1 $ and $X_2 $ change rules
		\item Points on A\,-\,B, C\,-\,D can be achieved by decreasing the rate of users 1 and 2 respectively (not desirable)
		\item Points on B\,-\,C (dominant face): Achievable by ``time-sharing´´, i.e., $\theta\cdot 100\% $ of the time we decode user 1 first and $(1 - \theta)100\% $ of the time we decode user 2 first, $ 0\leq \theta \leq 1 $ 
		\begin{align*}
			R_1 &< \theta\log_2\bigl(1 + \frac{\mathcal{E}_1}{\mathcal{E}_2 + \sigma_n^2}\bigr) + \bigl(1-\theta\bigr)\log_2\bigl(1 + \frac{\mathcal{E}_1}{\sigma_n^2}\bigr)\\
			R_2 &< \theta\log_2\bigl(1 + \frac{\mathcal{E}_2}{\sigma_n^2}\bigr) + \bigl(1-\theta\bigr)\log_2\bigl(1 + \frac{\mathcal{E}_2}{\mathcal{E}_1 + \sigma_n^2}\bigr)\\
			\rightarrow R_1 + R_2 &< \theta\Bigl(\log_2\bigl( 1 + \frac{\mathcal{E}_1}{\mathcal{E}_2 + \sigma_n^2}\bigr) + \log_2\bigl(1 + \frac{\mathcal{E}_2}{\sigma_n^2}\bigr)\Bigr) + \\ &+ \bigl( 1- \theta\bigr) \Bigl(\log_2\bigl(1 + \frac{\mathcal{E}_1}{\sigma_n^2}\bigr) + \log_2\bigl(1 + \frac{\mathcal{E}_2}{\mathcal{E}_1 + \sigma_n^2}\bigr)\Bigr) = \\ &= \theta\log_2\Bigl(\frac{\mathcal{E}_1 + \mathcal{E}_2 + \sigma_n^2}{\mathcal{E}_2 + \sigma_n^2}\cdot\frac{\mathcal{E}_2 + \sigma_n^2}{\sigma_n^2}\Bigr) \cdot\bigl(1 - \theta\bigr)\log_2\Bigl(\frac{\mathcal{E}_1 + \sigma_n^2}{\sigma_n^2} \cdot\frac{\mathcal{E}_1 + \mathcal{E}_2 + \sigma_n^2}{\mathcal{E}_1 + \sigma_n^2} = \\ &= \log_2\Bigl( 1 + \frac{\mathcal{E}_1 + \mathcal{E}_2}{\sigma_n^2}\Bigr)
		\end{align*}
	\end{itemize}
	\item Comparison with orthogonal transmission
	\begin{itemize}
		\item User 1 transmits for $\theta\cdot 100\% $ of the time and user 2 transmits for $(1-\theta)\cdot 100\%  $ of the time, $0\leq \theta\leq 1 $
		\item to keep average transmit power independent of $\theta$, the users transmit with powers $\frac{\mathcal{E}_1}{\theta} $ and $\frac{\mathcal{E}_2}{1 - \theta} $ 
		\item Rates: 
		\begin{align*}
			R_1 &< \theta\log_2\Bigl( 1 + \frac{\mathcal{E}_1}{\theta\sigma_n^2}\Bigr)\\
			R_2 &< \bigl(1 - \theta\bigr)\log_2\Bigl( 1 + \frac{\mathcal{E}_2}{(1 - \theta)\sigma_n^2}\Bigr)
		\end{align*}
		\item[] multiuser:\\
			\begin{tikzpicture}
				\path[draw,<->,>=stealth'] node[left] {}(0,5)  -- node[sloped, above] {$\mathcal{E}_1$} (10,5) node[right]{ user 1};
				\path[draw,<->,>=stealth'] (0,4)  -- node[sloped, above] {$\mathcal{E}_2$} (10,4) node[right]{ user 2};
			\end{tikzpicture}
		\item[] orthogonal:\\
			\begin{tikzpicture}
				\path[draw,<->,>=stealth']node[left] {} (0,3)  -- node[sloped, above] {$\frac{\mathcal{E}_1}{\theta}$}(4,3) ;
				\path[draw,<->,>=stealth']node{}(4,3) --node[sloped,above]{non-transmission period: $1 -\theta$}(10,3) node[right]{ user 1} ;
				\path[draw,<->,>=stealth']node{}(0,2)--node[sloped,above]{non-transmission period: $\theta$}(4,2);
				\path[draw,<->,>=stealth'] (4,2)  -- node[sloped, above] {$\frac{\mathcal{E}_2}{1 - \theta}$} (10,2)  node[right]{ user 2};
			\end{tikzpicture}
			\item sumrate:
			\begin{align*}
				R_1 + R_2 < \theta\log_2\Bigl(1 + \frac{\mathcal{E}_1}{\theta\sigma_n^2}\Bigr) + \bigl(1-\theta\bigr)\log_2\Bigl(1 + \frac{\mathcal{E}_2}{(1-\theta)\sigma_n^2}\Bigr) = R_{\text{sum}} 
			\end{align*}			
			\item Which $\theta $ maximizes sumrate?
			\begin{align*}
				\frac{\delta R_{\text{sum}}}{\delta\theta} \overset{!}{=}  0 \text{ leads to } \theta_{\text{opt}} = \frac{\mathcal{E}_1}{\mathcal{E}_1 + \mathcal{E}_2}	
			\end{align*}			 
			\item Maximum sumrate
			\begin{align*}
				R_{\text{sum}} &= \frac{\mathcal{E}_1}{\mathcal{E}_1 + \mathcal{E}_2}\log_2\Bigl(1 + \frac{\mathcal{E}_1 + \mathcal{E}_2}{\sigma_n^2}\Bigr) + \frac{\mathcal{E}_2}{\mathcal{E}_1 + \mathcal{E}_2}\log_2\Bigl(1 + \frac{\mathcal{E}_1 + \mathcal{E}_2}{\sigma_n^2}\Bigr) = \\ &= \log_2\Bigl(1 + \frac{\mathcal{E}_1 + \mathcal{E}_2}{\sigma_n^2}\Bigr)\\
				&\rightarrow \text{ same value as for general non-orthogonal transmission!}
			\end{align*}
			\item \underline{But:} In general, orthogonal transmission is suboptimal!\\
			\begin{tikzpicture}
			\tikzstyle{every node} = [font = \small]
				\path[draw,->,>=stealth'] (0,-0.3)--
					(0,4) node [left] {$R_2$};
				\path[draw,->,>=stealth'] (-0.3,0)--
					(5,0) node [below] {$R_1$};
				\path[draw] (0, 3)--
					(2,3) node {} --
					(4,2) node {}--
					(4,1.5) node [right] {non-orthogonal}--
					(4,0) node {};			
				\path[draw,dashed] (0,3) --
					(1.5,11/4) node[below] {orthogonal}--
					(3,2.5) node {}--
					(4,0) node{};
				\node[right] at (3,2.5) {optimal $\theta = \theta_{\text{opt}}$};
					\pgfsetplotmarksize{0.3ex}
					\pgfplothandlermark{\pgfuseplotmark{*}}
					\pgfplotstreamstart
					\pgfplotstreampoint{\pgfpoint{3cm}{2.5cm}}
					\pgfplotstreamend	
			\end{tikzpicture}	
	\end{itemize}
	\item 3 users case:
	\begin{align*}
		R_1 &< \log_2\Bigl(1 + \frac{\mathcal{E}_1}{\sigma_n^2}\Bigr)\\
		R_2 &< \log_2\Bigl(1 + \frac{\mathcal{E}_2}{\sigma_n^2}\Bigr)\\
		R_3 &< \log_2\Bigl(1 + \frac{\mathcal{E}_3}{\sigma_n^2}\Bigr)\\
		R_i + R_j &< \log_2\Bigl(1 + \frac{\mathcal{E}_i + \mathcal{E}_j}{\sigma_n^2}\Bigr),\quad i\neq j\\
		R_1 + R_2 + R_3 &< \log_2\Bigl(1 + \frac{\mathcal{E}_1 + \mathcal{E}_2 + \mathcal{E}_3}{\sigma_n^2}\Bigr)\\
		&\rightarrow \text{rate region } \mathbf{\mathcal{C}} \text{ has } 3! = 6 \text{ corner points}
	\end{align*}
	\item general case of K users
	\begin{itemize}
		\item define all non-empty subsets of $\mathbf{K} = \bigl\{1,\ldots, K\bigr\} $ as $ \mathbf{S} \in \mathbf{K}$,\\  e.g. $K = 2$: $\mathbf{K} = \bigl\{1, 2\bigr\}, \mathbf{S} = \bigl\{\{1\}, \{2\}, \{1, 2\}\bigr\} $ 
	\end{itemize}
	\item rate region $\mathbf{\mathcal{C}} $  is defined by
	\begin{align*}
		\sum_{k \in \mathbf{S}}R_k < \log_2\Bigl(1 + \frac{\sum_{k\in\mathbf{S}\mathcal{E}_k}}{\sigma_n^2}\Bigr)\quad\forall\, \mathbf{S}
	\end{align*}
	\item[$\rightarrow$] $\mathbf{\mathcal{C}} $  has $ K! $ corner points which can all be achieved by successive interference cancellation (SIC) 
\end{itemize}
\paragraph{Rate region for MIMO Users and Receivers}
\begin{itemize}
	\item Channel Model: $ \mathbf{y} = \sum_{k = 1}^{K}\mathbf{H}_k\mathbf{x}_k + \mathbf{n} $, with:
	\begin{itemize}
		\item User k has $N_{T,k} $ transmit antennas
		\item $N_R $ receive antennas
		\item $\mathbf{n} $: AWGN vector $\mathcal{N}(\mathbf{0},\sigma_n^2\mathbf{I})$
	\end{itemize}
	\item 2 Users case: 
	\begin{equation}
		\mathbf{y} = \mathbf{H}_1\mathbf{x}_1 + \mathbf{H}_2\mathbf{x}_2 + \mathbf{n}  \label{eq:Formel_7}
	\end{equation}	 
	\begin{itemize}
		\item Covariance matrix of the TX signal of user k: $\mathbf{Q}_k = \mathcal{E}\{\mathbf{x}_k\mathbf{x}_k^H\} $
		\item transmit power: $\mathcal{E}_k = \text{tr}\{\mathbf{Q}_k\} $
	\end{itemize}
	\item rate region for 2 user case and given $\mathbf{Q}_k$
	\begin{itemize}
		\item $\mathbf{Q}_k $ given, for example 
		\begin{itemize}
			\item[a)] $\mathbf{Q}_k$ optimal for single user case $\rightarrow \mathbf{Q}_k = \mathbf{U}_k\mathbf{\Lambda}_k\mathbf{U}_k^H $, where:
			\begin{itemize}
				\item $\mathbf{U}_k $ is an unitary matrix 
				\item obtained from $\mathbf{H}_k = \mathbf{U}_k\mathbf{\Sigma}_k\mathbf{V}_k^H $
				\item $\mathbf{\Lambda}_k = \text{diag}\{\mathcal{E}_{k,1},\mathcal{E}_{k,2},\ldots,\mathcal{E}_{k,N_T}\} $ with $\mathcal{E}_{l,i} $ obtained from waterfilling and $\sum_{i = 1}^{N_{Tk}}\mathcal{E}_{k,i} = \mathcal{E}_k $
			\end{itemize}
			\item[b)] $\mathbf{Q}_k = \frac{\mathcal{E}_k}{N_{T,k}}\mathbf{I}_{N_{T,k}} $ if $\mathbf{H}_k $ is not known at transmitter
		\end{itemize}
		\item for given $\mathbf{Q}_1 $ and $\mathbf{Q}_2 $ we can obtain the rate region as direct extension of the SISO case
		\begin{align}
			R_1 &< \log_2\bigl|\mathbf{I} + \frac{1}{\sigma_n^2}\mathbf{H}_1\mathbf{Q}_1\mathbf{H}_1^H\bigr|\label{eq:Formel_4}\\
			R_2 &< \log_2\bigl|\mathbf{I} + \frac{1}{\sigma_n^2}\mathbf{H}_2\mathbf{Q}_2\mathbf{H}_2^H\bigr|\label{eq:Formel_5}\\
			R_1 + R_2 &< \log_2\bigl|\mathbf{I} + \frac{1}{\sigma_n^2}\sum_{i = 1}^{2}\mathbf{H}_i\mathbf{Q}_i\mathbf{H}_i^H\bigr|\label{eq:Formel_6}			
		\end{align}
		\begin{itemize}
			\item equation \ref{eq:Formel_4} and equation  \ref{eq:Formel_5} are the single user bounds,
			\item equation \ref{eq:Formel_6} is the bound for the joint encoding of both users
		\end{itemize}
		\item graphical represantation\\
		\begin{tikzpicture}
			\tikzstyle{every node} = [font = \small]
				\path[draw,->,>=stealth'] (0,-0.3)--
					(0,4) node [left] {$R_2$};
				\path[draw,->,>=stealth'] (-0.3,0)--
					(5,0) node [above] {$R_1$};
				\path[draw] (-0.1, 3)--
					(0,3) node[left]{$\log_2|\mathbf{I} + \frac{1}{\sigma_n^2}\mathbf{H}_2 \mathbf{Q}_2\mathbf{H}_2^H|$}--
					(0.3,3) node[above] {A}--
					(2,3) node[above] {B} --
					(3,2.5) node[right]{$R_1 + R_2$}--
					(4,2) node[right] {C}--
					(4,0.3) node [right] {D}--
					(4,-0.1) node[below] {$\log_2|\mathbf{I} + \frac{1}{\sigma_n^2}\mathbf{H}_1 \mathbf{Q}_1\mathbf{H}_1^H|$};		
		\end{tikzpicture}
	\item Points on A\,-\,B\,-\,C\,-\,D can be achieved in a similar manner as for SISO case	
	\item e.g. bound C can be achieved by SIC
	\item At B we have 
	\begin{align*}
		R_2 &= \log_2\bigl|\mathbf{I} + \frac{1}{\sigma_n^2}\mathbf{H}_2\mathbf{Q}_2\mathbf{H}_2^H\bigr|\\
		R_1 &= \log_2\bigl|\mathbf{I} + \frac{1}{\sigma_n^2}\sum_{i = 1}^{2}\mathbf{H}_i\mathbf{Q}_i\mathbf{H}_i^H\bigr| - R_2\\
		\rightarrow &\text{ user 1 transmits with rate}\\
		R_1 &= \log_2\bigl|\mathbf{I} + \frac{1}{\sigma_n^2}(\mathbf{I} + \frac{1}{\sigma_n^2}\mathbf{H}_2\mathbf{Q}_2\mathbf{H}_2^H)^{-1}\mathbf{H}_1\mathbf{Q}_1\mathbf{H}_1^H\bigr|
	\end{align*}
	\item How to achieve rates at B? $\rightarrow $ Treat $\mathbf{H}_2\mathbf{x}_2 + \mathbf{n} $ in equation  \ref{eq:Formel_7} as noise with covariance matrix $ \mathbf{Q}_N = \mathbf{H}_2\mathbf{Q}_2\mathbf{H}_2^H + \sigma_n^2\mathbf{I} $
	\item[$\rightarrow$] equivalent channel matrix with white noise: $\mathbf{r} = \mathbf{Q}_N^{-\frac{1}{2}}\mathbf{y} = 	\mathbf{Q}_N^{-\frac{1}{2}}\mathbf{H}_1\mathbf{x}_1 + \tilde{\mathbf{n}} $ where $ \tilde{\mathbf{n}} $ is white noise with covariance $\mathbf{I}$. \\ \textit{Anmerkung: Rauschen war vorher farbig, muss ``gewei"st"\  werden.} 
	\begin{align*}
		\rightarrow R_1 &= \log_2\bigl|\mathbf{I} + \underbrace{\mathbf{Q}_N^{-\frac{1}{2}} \mathbf{H}_1}_{\mathbf{H}}\mathbf{Q}_1\underbrace{\mathbf{H}_1^H\mathbf{Q}_N^{-\frac{1}{2}}}_{\mathbf{H_{eq}^H}}\bigr|  = \log_2\Bigl(\bigl|\mathbf{Q}_N^{\frac{1}{2}} + \mathbf{Q}_N^{-\frac{1}{2}}\mathbf{H}_1\mathbf{Q}_1\mathbf{H}_1^H\bigr|\cdot\bigl|\mathbf{Q}_N^{-\frac{1}{2}}\bigr|\Bigr)\\
		&= \log_2\bigl|\mathbf{I} + \mathbf{Q}_N^{-1}\mathbf{H}_1\mathbf{Q}_1\mathbf{H}_1^H\bigr| = \\
		&= \log_2\bigl|\mathbf{I} + \frac{1}{\sigma_n^2}\bigl(\mathbf{I} + \frac{1}{\sigma_n^2}\mathbf{H}_2\mathbf{Q}_2\mathbf{H}_2^H\bigr)^{-1}\mathbf{H}_1\mathbf{Q}_1\mathbf{H}_1^H\bigr|
	\end{align*}
	\item[$\rightarrow$] we can achieve $R_1 $ in B by treating user 2 as noise
	\item[$\rightarrow$] once user 1 is detected, we can subtract its contribution from the received signal and detect user 2
	\begin{itemize}
		\item[$\Rightarrow$] user 2 can transmit with maximum single user rate
	\end{itemize}
	\item[$\rightarrow$] bound C can be achieved by SIC similar to SISO case
	\item points on B\,-\,C are achieved through time sharing
	\end{itemize}
	\item extension to K user case $\rightarrow $ analogous to SISO case
	\item Note: Different choices for $Q_k $ will lead to different rate regions
	\item Example: $ K = 2$
	\begin{tikzpicture}
		\tikzstyle{every node} = [font = \small]
				\path[draw,->,>=stealth'] (0,-0.3)--
					(0,5) node [left] {$R_2$};
				\path[draw,->,>=stealth'] (-0.3,0)--
					(7,0) node [below] {$R_1$};
				\path[draw] (0, 4)--
					(1.5,4) node[above] {$\mathbf{Q}_1^a, \mathbf{Q}_2^a$}--
					(3,4) node{}--
					(5,2) node{}--
					(5,0) node{};
				\path[draw,dashed](0,3)--
					(4.5,3) node[right]{$\mathbf{Q}_1^b, \mathbf{Q}_2^b$}--
					(6.5,1.5) node {}--
					(6.5,0) node{};
	\end{tikzpicture}
	\item[$\rightarrow$] $\mathbf{Q}_1 $ and $ \mathbf{Q}_2 $ can be optimized to achieve desired trade-off between  performance of users 1 and 2
\end{itemize}

\subsection{Broadcast Channel}
We consider:
\begin{itemize}
	\item uplink\,-\,downlink duality
	\item rate region
\end{itemize}
\subsubsection{Multiplexing Gain\,-\,Degrees of freedom}
\paragraph{Downlink scenarios:}
\begin{itemize}
	\item $N_R $ antennas at transmitter, single antennas at the users
	\item user k receives: $ \mathbf{y}_k = \mathbf{h}_k^H\mathbf{x} + \mathbf{n}_k $, with:
	\begin{itemize}
		\item $N_R $ dimensional channel vector of user k: $\mathbf{h}_k^H $
		\item $n_k$: AWGN at user k
		\item $\mathbf{x} $ transmit vector  
	\begin{figure}[ht]
		\centering
		\psfrag{sender}[bl][bl][3]{$\underline{\text{x}}$}
		\psfrag{CH1}[br][br]{$h_1^H$}
		\psfrag{CH2}[br][br]{$h_2^H$}
		\includegraphics[scale=0.8]{BroadcastChannel_scenario}
		\caption{Scenario for Broadcast Channel}
		\label{fig:BroadcastChannel_scenario}
	\end{figure}
	\end{itemize}
	\item How many independent signal streams can we transmit?
	\begin{itemize}
		\item Consider transmit signal: $\mathbf{x} = \sum\limits_{k = 1}^{K}\mathbf{h}_k\mathbf{x}_k $, with TX transmit vector $\mathbf{n}_k $ and symbol $x_k $ is intended for user k
	\end{itemize}
	\item received signal of user k: $y_k = \sum\limits_{k = 1}^{K}(\mathbf{h}_k^H\mathbf{n}_i)\mathbf{x}_i + n_k $
	\item if all $\mathbf{h}_k $ were orthogonal and we chose $\mathbf{n}_k = \mathbf{h}_k $, the received signal would be: \\ $y_k = ||h_k||^2\mathbf{x}_k + n_k $
	\item if $N_R \geq K$, we can transmit simultaneously and interference free to all K users\\ $\Rightarrow $ multiplexing gain $= \text{min}\{K,N_k\} $
\item In practice, the $\mathbf{h}_k $ will not be orthogonal
\begin{itemize}
	\item[$\rightarrow$] choose $\mathbf{n}_k $ such that it lies in the null space of $\begin{bmatrix}
	\mathbf{h}_1 & \ldots & \mathbf{h}_{k-1} & \mathbf{h}_{k+1} & \ldots & \mathbf{h}_K \end{bmatrix}	 $
	\item[$\rightarrow$] always possible if $ \mathbf{h}_1,\ldots, \mathbf{h}_K $ are linearly independent
	\item[$\rightarrow$] multiplexing gain ( $= $ degrees of freedom) is equal to $ \text{min}\{K, N_R\} $
\end{itemize}
\item we can transmit interference free to $K\leq N_R $ users \\ \quad \textit{Anmerkung: Falls TX viele Antennen, aber RX nur eine hat $\Rightarrow $ begrenzter Nutzen: SNR Verbesserung, kein Multiplexing Gain; falls TX viele Antennen und viele RX vorhanden sind $\Rightarrow $ RX erscheinen als Antennenarray $\rightarrow $ hoher Multiplexing Gain}
\end{itemize}

\subsubsection{Uplink\,-\,Downlink  Duality}
\begin{itemize}
	\item How should we choose signature vectors to achieve a certain SNR at users?
	\item Difficult problem since optimal (in the SINR sense) $\mathbf{u}_k $ are not orthogonal $\rightarrow $ signature of user $k$, $\mathbf{u}_k $, influences SINR at all other users!
	\item On the other hand, the uplink problem was much easier to solve, since the receive filter of user $k$, $\mathbf{f}_k$, was not affected by receive filters of other users! (vgl. Point\,-\,to\,-\,Point $\rightarrow $ detection problem
	\item[$\Rightarrow$] We establish a duality between the uplink and downlink, that allows us to solve the more challenging downlink problem by solving an equivalent uplink problem.
\end{itemize}
\paragraph{Downlink:}
\begin{itemize}
	\item transmit signal
	\begin{align*}
		\mathbf{x}_{dl} = \sum\limits_{k = 1}^{K}\mathbf{u}_kx_{dl,k}
	\end{align*}
	\item received signal at user $k$
	\begin{align*}
		y_{dl,k} = \mathbf{h}_k^H\mathbf{u}_kx_{dl,k} + \sum\limits_{j \neq k}\mathbf{h}_k^H\mathbf{u}_jx_{dl,j} + u_{dl,k}
	\end{align*}
	\item SINR of user $k$
	\begin{align*}
		\text{SINR}_k^{dl} = \frac{\mathcal{E}_{dl,k}|\mathbf{u}_k^H\mathbf{h}_k|^2}{\sigma_n^2 + \sum\limits_{j \neq k}\mathcal{E}_{dl,j}|\mathbf{h}_j^H\mathbf{h}_k|^2},\quad 1\leq k \leq K
	\end{align*}
	\item where: $\mathcal{E}_{dl,k} = \mathcal{E}\bigl\{|x_{dl,k}|^2 \bigr\}; \quad \sigma_n^2 = \mathcal{E}\bigl\{|n_{dl,k}|^2\bigr\} $
	\item Using: 
	\begin{align*}
		a_k = \frac{\text{SINR}_k^{dl}}{(1 + \text{SINR}_k^{dl})|\mathbf{h}_k^H\mathbf{u}K|^2}
	\end{align*}
	we can rewrite the the SINR expressions as:
	\begin{align*}
		\boxed{\bigl(\mathbf{I}_K - \text{diag}\{a_1, \ldots, a_k\}\mathbf{A}\bigr)\mathbf{p}_{dl} = \sigma_n^2\mathbf{a}}
	\end{align*}
	where:
	\begin{align*}
		\mathbf{a} &= \begin{bmatrix} a_1,\ldots,a_k\end{bmatrix}^T \\
		\mathbf{A} &= 
		\begin{bmatrix}
			|u_1^Hh_1|^2 & |u_2^Hh_1|^2 & \ldots & |u_K^Hh_1|^2	\\
			\vdots \\
			|u_1^Hh_K|^2 & \ldots & & |u_K^Hh_K|^2		
		\end{bmatrix} \\
		\mathbf{p}_{dl} &= 
		\begin{bmatrix}
			\mathcal{E}_{dl,1},\ldots, \mathcal{E}_{dl,K}	
		\end{bmatrix}^T
	\end{align*}
	\item[$\rightarrow$] We can easily calculate transmit powers $\mathcal{E}_{dl,k} $ required to achieve desired $\text{SINR}_k^{dl}, 1\leq k_1\leq K, $ for given signature (precoding) vectores $u_k, 1\leq k\leq K $ 
	\begin{figure}[ht]
		\centering	
		\psfrag{Eingang1}[bl][bl][1]{$x_{dl,1} $}
		\psfrag{Eingang2}[bl][bl][1]{$x_{dl,K} $}
		\psfrag{Nutzer1}[bl][bl][1]{$\mathbf{u}_1 $}
		\psfrag{Nutzer2}[bl][bl][1]{$\mathbf{u}_2 $}
		\psfrag{Kanalmatrix}[bl][bl][1]{$\mathbf{H}^* $}
		\psfrag{Rauschen1}[bl][bl][1]{$n_{dl,1} $}
		\psfrag{Rauschen2}[bl][bl][1]{$n_{dl,k} $}
		\psfrag{Ausgang1}[bl][bl][1]{$y_{dl,1} $}
		\psfrag{Ausgang2}[bl][bl][1]{$y_{dl,K} $}
		\includegraphics[scale=0.8]{Downlink_MIMO}	
		\caption{Block Diagramm of MIMO in Downlink}	
		\label{fig:Downlink_MIMO}
	\end{figure}
\end{itemize}
\paragraph{Uplink:} 
Use downlink signatures, $\mathbf{h}_k$, as receive filters, $\mathbf{f}_k$
\begin{itemize}
	\item block diagramm: 
	\begin{figure}[ht]
		\centering	
		\psfrag{Eingang1}[bl][bl][1]{$x_{ul,1} $}
		\psfrag{Eingang2}[bl][bl][1]{$x_{ul,K} $}
		\psfrag{Nutzer1}[bl][bl][1]{$\mathbf{u}_1 $}
		\psfrag{Nutzer2}[bl][bl][1]{$\mathbf{u}_2 $}
		\psfrag{Kanalmatrix}[bl][bl][1]{$\mathbf{H}^* $}
		\psfrag{Rauschen1}[bl][bl][1]{$n_{ul} $}
		\psfrag{Ausgang1}[bl][bl][1]{$y_{ul,1} $}
		\psfrag{Ausgang2}[bl][bl][1]{$y_{ul,K} $}
		\includegraphics[scale=0.8]{Uplink_MIMO}	
		\caption{Block Diagramm of MIMO in Uplink}
		\label{fig:Uplink_MIMO}	
	\end{figure}
	\item Signal model:
	\begin{align*}
		y_{ul,k} &= \mathbf{u}_k^H\bigl(\mathbf{Hx}_{ul} + \mathbf{u}_{ul}\bigr) 
	\end{align*}	
		with 
	\begin{align*}
		\mathbf{H} &= 
		\begin{bmatrix}
			\mathbf{h}_1 & \ldots & \mathbf{h}_K
		\end{bmatrix} ,\quad \mathbf{x}_{ul} = 
		\begin{bmatrix}
			x_{ul,1} & \ldots & x_{ul,K}
		\end{bmatrix}^T , \quad \mathbf{u}_{ul} = 
		\begin{bmatrix}
			u_{ul,1} & \ldots & u_{ul,K}
		\end{bmatrix}^T 
	\end{align*}		
	\begin{align}
		\rightarrow y_{ul,k} &= \mathbf{u}_k^H\mathbf{h}_kx_{ul,k} + \sum\limits_{j \neq k}\mathbf{u}_k\mathbf{h}_jx_{ul,j} + \mathbf{n}_k^H\mathbf{u}_{ul}\\
		\rightarrow \text{SINR}_k^{ul} &= \frac{\mathcal{E}_{ul,k}|\mathbf{u}^H\mathbf{h}_k|^2}{\sigma_n^2 + \sum\limits_{j\neq k}\mathcal{E}_{ul,j}|\mathbf{u}_k^H\mathbf{h}_j|^2}\label{eq:Formel_8}
	\end{align}
	where we used $\mathbf{u}_k^H\mathbf{u}_k = 1 $ and $ \mathcal{E}_{ul,K} = \mathcal{E}\bigl\{|x_{ul,K}|^2\bigr\} $ 
	\item we define: $ b_k = \frac{\text{SINR}_k^{ul}}{(1 + \text{SINR}_k^{ul})|\mathbf{u}_k^H\mathbf{h}_k|^2} $
	\item we can rewrite SINR epression \eqref{eq:Formel_8} as : 
	\begin{align*}
		\sigma_n^2 + \sum\limits_{j\neq k}\mathcal{E}_{ul,j}\bigl|\mathbf{u}_k^H\mathbf{h}_j\bigr|^2 = \frac{1}{\text{SINR}_k^{ul}} \mathcal{E}_{ul,k}\bigl|\mathbf{u}_k^H\mathbf{h}_k\bigr|^2
	\end{align*}
	\begin{align*}
		\underbrace{\bigl(1 + \frac{1}{\text{SINR}_k^{ul}}\bigr)\bigl|\mathbf{u}_k^H\mathbf{h}_k\bigr|^2}_{\frac{1}{b_k}}\mathcal{E}_{ul,k} - \sum\limits_{j = 1}^{K}\mathcal{E}_{ul,k}\bigl|\mathbf{u}_k^H\mathbf{h}_k\bigr|^2 = \sigma_n^2
	\end{align*}
	\begin{align*}
		\rightarrow \mathcal{E}_{ul,k} - b_k\sum\limits_{j = 1}^{K}\mathcal{E}_{ul,j}\bigl|\mathbf{u}_k^H\mathbf{h}_j\bigr|^2 = b_k\sigma_n^2
	\end{align*}
	\item matrix notation:
	\begin{align*}
		\begin{bmatrix}
			\mathbf{I}_K - \text{diag}\bigl\{b_1,\ldots, b_K\bigr\}\mathbf{A}^T\mathbf{p}_{ul} = \sigma_n^2\cdot \mathbf{b}
		\end{bmatrix}
	\end{align*}
	\item where: $\mathbf{A} $ was defined for downlink case
	\begin{align*}
		\mathbf{p}_{ul} &= \begin{bmatrix} \mathcal{E}_{ul,1} & \ldots & \mathcal{E}_{ul,K}\end{bmatrix}^T \\
		\mathbf{b} &= \begin{bmatrix}b_1 & \ldots & b_K\end{bmatrix}^T				
	\end{align*}
	\item we can calculate power allocation vector $\mathbf{p}_{ul} $ for given $\text{SINR}_1^{ul} $ and $ \mathbf{u}_k,\quad 1\leq k \leq K $ 
\end{itemize} 
\paragraph{Comparison:} Assume, we want to achieve same SINR in uplink and downlink
\begin{itemize}
	\item[$\rightarrow$] $\text{SINR}_k^{ul}  = \text{SINR}_k^{dl} \quad \forall\quad k $ or equivalently $ a_k = b_k,\quad \forall\quad k $ !
	\item[] What sum power do we need in both uses?
	\begin{align*}
		\mathbf{p}_{dl} &= \sigma_n^2\bigl(\mathbf{I} - \text{diag}\{a_1, \ldots, a_K\}\mathbf{A}\bigr)^{-1}\mathbf{a} = \\
		&= \sigma_n^2\bigl(\mathbf{D}_a - \mathbf{A}\bigr)^{-1}\cdot\mathbf{1}
	\end{align*}
	where: $\mathbf{D}_a = \text{diag}\{\frac{1}{a_1}, \ldots, \frac{1}{a_K}\} $ and $ \mathbf{1} = \begin{bmatrix}	1 & 1 & 1 & \ldots & 1 \end{bmatrix}^T $ 
	\begin{align*}
		\mathbf{p}_{ul} = \sigma_n^2(\mathbf{D}_b - \mathbf{A}^T)^{-1}\cdot \mathbf{1}
	\end{align*}
	where: $\mathbf{D}_b = \text{diag}\{\frac{1}{b_1}, \ldots ,\frac{1}{b_K}\} $
	\begin{align*}
		\sum\limits_{k = 1}^{K}\mathcal{E}_{dl,k} &= \mathbf{1}^T\mathbf{p}_{dl} = \sigma_n^2\mathbf{1}^T\bigl(\mathbf{D}_a - \mathbf{A}\bigr)^{-1}\cdot\mathbf{1}\\
		&= \sigma_n^2\mathbf{1}^T\bigl(\mathbf{D}_b - \mathbf{A}\bigr)^{-1}\mathbf{1} \\
		&= \sigma_n^2\bigl[\mathbf{1}^T\bigl(\mathbf{D}_b - \mathbf{A}\bigr)^{-1}\mathbf{1}\bigr]^T \\
		&= \sigma_n^2\mathbf{1}^T\bigl[\bigl(\mathbf{D}_b - \mathbf{A}\bigr)^{-1}\bigr]^T\mathbf{1} \\
		&= \sigma_n^2\mathbf{1}^T\bigl(\underbrace{\mathbf{D}_b^T}_{\mathbf{D}_b} - \mathbf{A}^T\bigr)^{-1}\\cdot\mathbf{1} = \sum\limits_{k = 1}^{K}\mathcal{E}_{ul,k}
	\end{align*}
\end{itemize}
\paragraph{Conclusions:}
\begin{itemize}
	\item We can achieve any desired $\text{SINR}_k^{dl},\forall\, k,$ in the downlink by using filters optimized for uplink transmission as signature (precoding) vectors and the same sum power as in the uplink
	\item suitable filters may be MMSE or ZF filters $ \mathbf{u}_k = \mathbf{f}_k, \quad \forall \, k $
	\item Note that, in general, $\mathcal{E}_{ul,k} \neq \mathcal{E}_{dl,k} $, only the sum powers are equal!
	\item[$\rightarrow$] \fbox{\parbox{\textwidth}{For linear precoding, we can solve the more challenging problem via solving an equivalent uplink problem!}}
\end{itemize}
\paragraph{Extension:} This concept can be extended to nonlinear receivers as well. In this case the DFE receiver in the uplink is dual to a nonlinear Tamlinson\,-\,Harashima (TH) precoder in the downlink.

\subsubsection{Rate Region (only SISO)}
\begin{itemize}
	\item two user case: $ y_k = h_kx + n_k\,,\quad k \in \{1,2\} $
	\item Capacity can be achieved with so\,-\,called \textit{Costa Cor (dirty paper)} precoding
	\item In this scheme, the signal of ine user is processed such that it does not impair the receiver of the other user, while the signal of the other user is treated as Gaussian noise at the receiver of the first user.
	\item[$\rightarrow$] Thus, the achievable rates would be
	\begin{align*}
		R_1 &< \log_2\bigl(1 + \frac{|h_1|^2\mathcal{E}_1}{\sigma_n^2}\bigr)\\
		R_2 &< \log_2\bigl(1 + \frac{|h_2|^2\mathcal{E}_2}{\sigma_n^2 + |h_1|^2\mathcal{E}_1}\bigr)			
	\end{align*}	 
	\item $\mathcal{E}_1 $ and $\mathcal{E}_2 $ can be optimized as they both occure at the same transmitter
\end{itemize}
\newpage
\section{Distributed MIMO}

\begin{itemize}
	\item This research topic emerged ca. 10 years ago and is still a very active area of research
	\item Simple relaying schemes have been included in recent standards such as IEEE 802.16 (WiMAX) and LTE\,-\,Advanced
	\item Advantages: relay\,-\,assisted communications:
	\begin{itemize}
		\item Relays can help to reduce the effective overall pathloss
		\item Relays can also combat small\,-\,scale fading effects
		\item Relays can help to realize MIMO gains with single\,-\,antenna nodes
	\end{itemize}
	\item Challenges in relays\,-\,assisted communication:
	\begin{itemize}
		\item Network architectures are becoming more complex
		\item Synchronization across different nodes may be necessary \textit{(Anm.: untersch. Tr\"agerfrequenzen der Relays $\rightarrow $ Offset, Fehler, etc.)}
		\item Exchange of channel state information (CSI) across nodes may be required
	\end{itemize}
\end{itemize}

\subsection{Half\,-\,Duplex One\,-\,Way Relaying}
\paragraph{Basic Relay Network}
\begin{figure}[ht]
	\centering
	\psfrag{h_SR}[bl][bl][1]{$h_{SR}$}
	\psfrag{h_RD}[bl][bl][1]{$h_{RD}$}
	\psfrag{h_SD}[bl][bl][1]{$h_{SD}$}
	\includegraphics[scale=2]{Basic_Relay_Network}	
	\caption{Basic Relay Network}
	\label{fig:basic_relay_network}
\end{figure}
\begin{itemize}
	\item Relay R assists source S in communication with destination D
	\item Two basic nodes of transmission (at the relay):
\end{itemize}
\paragraph{Full\,-\,Duplex relaying:} 
R can receive and transmit at the same time and in the same frequency band \textit{(Anm.: effizient, da restliche Zeit und restliche Frequenzband von anderen genutzt werden kann)}
\begin{itemize}
	\item[$\rightarrow$] Since the TX signal power is orders of magnitude larger than the RX power, there is self\,-\,interference (at the relay) 
		\begin{figure}[ht]
			\centering
			\psfrag{y_R}[bl][bl][1]{$y_{R}$}
			\psfrag{s_R}[bl][bl][1]{$s_{R}$}
			\includegraphics[scale=1]{Relay_self_interference}	
			\caption{Relay with self-interference}
			\label{fig:relay_self_interference}
		\end{figure}	
	\item[$\rightarrow$] Full\,-\,duplex relays are difficult to implement. The design of full\,-\,duplex relays is an active area of research.
	\item[$\rightarrow$] Majority of existing literature assumes half\,-\,duplex relaying.
\end{itemize}
\paragraph{Half\,-\,duplex relaying:} 
R transmits and receives in different time slots and/\,or different frequency bands. Typically, a two\,-\,phase protocoll is used:
\begin{itemize}
	\item[] \textbf{Phase 1}: S transmits, R and D receive 
	\begin{figure}[ht]
		\centering
		\includegraphics[scale=1.3]{Relay_HalfDuplex_1}	
		\caption{Half-duplex Relaying: Phase 1}
		\label{fig:relay_halfduplex_1}
	\end{figure}
	\item[] \textbf{Phase 2}: R transmits, D receives, S may or may not transmit 
	\begin{figure}[ht]
		\centering
		\includegraphics[scale=1.3]{Relay_HalfDuplex_2}	
		\caption{Half-duplex Relaying: Phase 2}
		\label{fig:relay_halfduplex_2}
	\end{figure}
\end{itemize}
There are different relaying strategies that differ in the processing applied at the relay. The most popular are:
\begin{itemize}
	\item Decode\,-\,and\,-\,Forward
	\item Amplify\,-\,and\,-\,Forward
	\item (Compress\,-\,and\,-\,Forward)
\end{itemize}

\subsubsection{Decode\,-\,and\,-\,Forward (DF) Relaying}
In DF relaying, the relay detects and decodes the signal received from the source before encoding it and  forwarding it to the destination.
	\begin{figure}[ht]
		\centering
		\psfrag{h_SR}[bl][bl][1]{$h_{SR}$}
		\psfrag{h_RD}[bl][bl][1]{$h_{RD}$}
		\psfrag{h_SD}[bl][bl][1]{$h_{SD}$}
		\psfrag{y_R}[bl][bl][1]{$y_R$}
		\psfrag{y_D1}[bl][bl][1]{$y_{D,1}$}
		\psfrag{y_D2}[bl][bl][1]{$y_{D,2}$}
		\psfrag{x_R}[bl][bl][1]{$x_R$}		
		\includegraphics[scale=1.5]{DF_Relaying}	
		\caption{Block diagramm Decode\,-\,and\,-\,forward Relaying}
		\label{fig:df_relaying}
	\end{figure}
\paragraph{Phase 1:}
\begin{itemize}
	\item R receives: $ y_R = h_{SR}x + n_R $
	\item D receives: $ y_{D1} = h_{SD}x + n_{D1} $
	\item with: 
	\begin{itemize}
		\item transmit signal $x, \mathcal{E}_s = \mathcal{E}\{|x|^2\} $
		\item AWGN $n_R $ and $ n_{D1}$, $\sigma_n^2 = \mathcal{E}\{|n_R|^2\} = \mathcal{E}\{|n_{D1}|^2\} $
	\end{itemize}
\end{itemize}
\paragraph{Phase 2:}
\begin{itemize}
	\item R decodes and forwards $x_R$ (estimate of $x$)
	\item D receives: $ y_{D2} = h_{RD}x_R + n_{D2} $
	\begin{itemize}
		\item $x_R$ is estimate of $x $ after decoding at R
		\item $\sigma_n^2 = \mathcal{E}\{|n_{D2}|^2\}\,;\quad \mathcal{E}_R = \mathcal{E}\{|x_R|^2\} $
		\item weassume: S is silent in Phase 2		 
	\end{itemize}
\end{itemize}
\paragraph{}
\begin{itemize}
	\item The capacity at the three node relay channel is not known!
	\item We provide an achievable rate under the following simplifying assumption: The direct source\,-\,relay link is not used/\,exploited.
	\item Achievable rate without S\,-\,D link:
	\begin{align*}
		\boxed{C_{DF} = \frac{1}{2}\text{min}\bigl\{\log_2\bigl(1 + \frac{\mathcal{E}_S|h_{SR}|^2}{\sigma_n^2}\bigr),\,\log_2\bigl(1 + \frac{\mathcal{E}_R|h_{RD}|^2}{\sigma_n^2}\bigr)\bigr\} }	
	\end{align*}	 
	\begin{itemize}
		\item factor $\frac{1}{2} $ is due to the fact that we use two time slots to transmit one packet
		\item $\text{min}\{\ldots\} $ means we are limited by the weaker link (bottle\,-\,neck)
		\item If power allocation is possible, the total power $ \mathcal{E} = \mathcal{E}_S + \mathcal{E}_R $ should be divided between S and R to guarantee:
		\begin{align*}
			&\frac{\mathcal{E}_S|h_{SR}|^2}{\sigma_n^2} = \frac{\mathcal{E}_R|h_{RD}|^2}{\sigma_n^2},\\
			&\mathcal{E}_R = \frac{|h_{SR}|^2}{|h_{SR}|^2 + |h_{RD}|^2}\cdot\mathcal{E},\\
			&\mathcal{E}_S = \frac{|h_{RD}|^2}{|h_{SR}|^2 + |h_{RD}|^2}\cdot\mathcal{E}
		\end{align*}
	\end{itemize}
	\item Outage\,-\,probability in fading:
	\begin{itemize}
		\item We transmit with fixed rate R
		\item An outage occurs, if:
		\begin{align*}
			\frac{1}{2}\log_2\Bigl(1 + \underbrace{\frac{\mathcal{E}_S|h_{SR}|^2}{\sigma_n^2}}_{= \gamma_{SR}}\Bigr) &< R \quad	\text{or} \\
			\frac{1}{2}\log_2\Bigl(1 + \underbrace{\frac{\mathcal{E}_R|h_{RD}|^2}{\sigma_n^2}}_{= \gamma_{RD}}\Bigr) &< R
		\end{align*}
		\begin{align*}
			\boxed{\gamma_{SR} < \underbrace{2^{2R} - 1}_{\gamma_T} \quad \text{or}\quad \gamma_{RD} < 2^{2R} - 1}
		\end{align*}
		\begin{align*}
			P_{\text{out}} &= \text{Pr}\bigl\{\gamma_{SR} < \gamma_T \vee \gamma_{RD} < \gamma_T\bigr\}
			= \text{Pr}\bigl\{\underbrace{\text{min}\bigl\{\gamma_{SR},\gamma_{RD}\bigr\}}_{ = \gamma_{eq}} < \gamma_T\bigr\} \\ &= 1- \text{Pr}\bigl\{\gamma_{SR} > \gamma_T \wedge \gamma_{RD} > \gamma_T\bigr\} = 1 - \text{Pr}\bigl\{\gamma_{SR} > \gamma_T\bigr\}\,\text{Pr}\bigl\{\gamma_{RD} > \gamma_T\bigr\} = \\ &= 1 - \bigl(1 - F_{\gamma_{SR}}(\gamma_T)\bigr)\bigl(1 - F_{\gamma_{RD}}(\gamma_T)\bigr) = \\ &= \underline{F_{\gamma_{SR}}(\gamma_T) + F_{\gamma_{RD}}(\gamma_T) - F_{\gamma_{SR}}(\gamma_T)\cdot F_{\gamma_{RD}}(\gamma_T)}
		\end{align*}
		\item[] with CDFs: $F_{\gamma_{SR}}(\cdot) $ and $F_{\gamma_{RD}}(\cdot) $ 
		\item Rayleigh Fading: 
		\begin{align*}
		\rightarrow	F_{\gamma_{SR}}(\gamma) &= 1-\exp\bigl(\frac{-\gamma}{\bar{\gamma}_{SR}}\bigr)\,;\quad \bar{\gamma}_{SR} = \mathcal{E}\{\gamma_{SR}\} \\
			F_{\gamma_{RD}}(\gamma) &= 1-\exp\bigl(\frac{-\gamma}{\bar{\gamma}_{RD}}\bigr)\,;\quad \bar{\gamma}_{RD} = \mathcal{E}\{\gamma_{RD}\}\\
		\rightarrow P_{\text{out}} &= 1-\exp\bigl(\frac{-\gamma_T}{\bar{\gamma}_{SR}}\bigr) + 1-\exp\bigl(\frac{-\gamma_T}{\bar{\gamma}_{RD}}\bigr) - \Bigl(1-\exp\bigl(\frac{-\gamma_T}{\bar{\gamma}_{SR}}\bigr)\Bigr)\Bigl( 1-\exp\bigl(\frac{-\gamma_T}{\bar{\gamma}_{RD}}\bigr)\Bigr) = \\
		&= 1 - \exp\Bigl(-\frac{\bar{\gamma}_{SR} + \bar{\gamma}_{RD}}{\bar{\gamma}_{SR}\bar{\gamma}_{RD}}	\cdot\gamma_T\Bigr)
		\end{align*}
		\item[$\rightarrow$] equivalent SNR $\gamma_{eq} = \min\bigl\{\gamma_{SR}, \gamma_{RD}\bigr\} $ is also exponentially distributed with \\ $\bar{\gamma}_{eq} = \frac{\bar{\gamma}_{SR}\bar{\gamma}_{RD}}{\bar{\gamma}_{SR} + \bar{\gamma}_{RD}} $ 
		\item Diversity gain: Assume $\bar{\gamma}_{SR} = \alpha\bar{\gamma} $
		\begin{align*}
			\rightarrow P_{out} & \xrightarrow{\bar{\gamma} \rightarrow \inf} 1 - \Bigl(1-\frac{\alpha + \beta}{\alpha\beta}\cdot\frac{\gamma_T}{\bar{\gamma}}\Bigr) + \mathcal{O}\bigl(\bar{\gamma}^{-1}\bigr) = \\ 
			&= \frac{\alpha + \beta}{\alpha\beta}\cdot\frac{\gamma_T}{\bar{\gamma}} + \mathcal{O}\bigl(\bar{\gamma}^{-1}\bigr) = \\
			& \rightarrow \boxed{G_d = 1}
		\end{align*}
	\end{itemize}
	\item Bit error rate (BER) of BPSK (uncoded)
	\begin{itemize}
		\item $\text{BER}\bigl(\gamma_{SR},\gamma_{RD}\bigr) = \Bigl(1 - \text{BER}_{SR}\bigl(\gamma_{SR}\bigl)\Bigl)\text{BER}_{RD}\bigl(\gamma_{RD}\bigr) + \Bigl(1 - \text{BER}_{RD}\bigl(\gamma_{RD}\bigl)\Bigl)\text{BER}_{SR}\bigl(\gamma_{SR}\bigr) $ 
		\begin{itemize}
			\item with BER of the S\,-\,R link, $\text{BER}_{SR}(\gamma_{SR}) $ and BER of the R\,-\,D link $\text{BER}_{RD}(\gamma_{RD}) $ 
			\item for sufficiently high SNR $\leadsto \text{BER}_{SR}(\gamma_{SR}), \text{BER}_{RD}(\gamma_{RD}) \ll \text{BER}_{SR}(\gamma_{SR}) + \text{BER}_{RD}(\gamma_{RD}) $
			\item[$\rightarrow$] $\underline{\text{BER}(\gamma_{SR},\gamma_{RD}) \approx \text{BER}_{SR}(\gamma_{SR}) + \text{BER}_{RD}(\gamma_{RD})} $
			\item to average BER (Rayleigh Fading):
			\begin{align*}
				\text{BER} = \mathcal{E}_{\gamma_{SR},\gamma_{RD}}\Bigl\{\text{BER}\bigl(\gamma_{SR},\gamma_{RD}\bigr)\Bigr\} = \frac{1}{2}\Bigl(1-\sqrt{\frac{1}{1+\frac{1}{\bar{\gamma}_{SR}}}} + \frac{1}{2}\Bigl(1-\sqrt{\frac{1}{1+\frac{1}{\bar{\gamma}_{RD}}}}\Bigr)
			\end{align*}
			\item high SNR: 
			\begin{align*}
				\text{BER} &\approx \frac{1}{2}\bigl(1 - 1 + \frac{1}{2}\frac{1}{\bar{\gamma}_{SR}}\bigr) + \frac{1}{2}\bigl(1 - 1 + \frac{1}{2}\frac{1}{\bar{\gamma}_{RD}}\bigr) = \\ 
				&= \frac{1}{4}\bigl(\frac{1}{\bar{\gamma}_{SR}} + \frac{1}{\bar{\gamma}_{RD}}\bigr)
			\end{align*}
			$\leadsto $ also indicates diversity gain $ G_d = 1 $
		\end{itemize}		 
	\end{itemize}
\end{itemize}
\subsubsection{Amplify\,-\,and\,-\,Forward (AF) Relaying}
\begin{itemize}
	\item Relay does not decode signal received from source but only amplifies it before forwarding it to the destination
	\begin{figure}[ht]
		\centering
		\psfrag{y_R}[bl][bl]{$y_R$}
		\psfrag{Ay_R}[bl][bl]{$Ay_R$}
		\psfrag{h_SR}[bl][bl]{$h_{SR}$}
		\psfrag{h_SD}[bl][bl]{$h_{SD}$}
		\psfrag{h_RD}[bl][bl]{$h_{RD}$}	
		\psfrag{y_D2}[bl][bl]{$y_{D,2}$}
		\psfrag{y_D1}[bl][bl]{$y_{D,1}$}
		\includegraphics[scale=1.5]{AF_Relaying}
		\caption{Block diagramm AF\,-\,Relaying}	
		\label{fig:af_relaying}
	\end{figure}
	\item Amplification gain $A $ may be constant on channel dependent and ensures a certain (average) transmit power
	\item[] \textbf{Phase 1}:
	\begin{itemize}
		\item[-] R receives: $y_D = h_{SR}x + n_R $
		\item[-] D receives: $y_{D,1} = h_{SD}x + n_{D,1} $
	\end{itemize}
	\item[] \textbf{Phase 2}:
	\begin{itemize}
		\item[-] R transmits: $s_R = Ay_R = A(h_{SR}x + n_R) $
		\item[-] D receives: $y_{D,2} = h_{RD}Ay_R + n_{D,2} $  
	\end{itemize}
	\item We can use MRC to combine $y_{D,1} $ and $ y_{D,2} $ at D: $ y_{D,2} = Ah_{RD}h_{SR}x + h_{RD}An_R + n_{D,2} $,\quad where: $ h_{RD}An_R + n_{D,2} $ is effective noise $n_{\text{eff}} $ with variance $\sigma_{n_\text{eff}}^2 = \sigma_n^2\bigl(|h_{RD}|^2A^2 + 1\bigr) $
	\item[$\rightarrow$] make noise variances of both branches equal
	\begin{align*}
		\bar{y}_{D,2} &= \frac{1}{\sqrt{|h_{RD}|^2A^2+1}}\cdot y_{D,2} = \frac{Ah_{RD}h_{SR}}{\sqrt{A^2|h_{RD}|^2 + 1}} \cdot x + \tilde{n}_{\text{eff}} \\
		\text{MRC:}\quad r &= h_{SD}^*y_{D,1} + \frac{Ah_{RD}^*h_{SR}^*}{\sqrt{A^2|h_{RD}|^2+1}}\cdot\bar{y}_{D,2} = \underbrace{h_{SD}^*y_{D,1} + \frac{Ah_{RD}^*h_{SR}^*}{A^2|h_{RD}|^2 + 1}\cdot y_{D,2}}_{ = \text{decision variable!}}		
	\end{align*}
	\item Choice of A: The goal is to ensure en (average) transmit power of $\mathcal{E}_R $	
\end{itemize}
\paragraph{a) Variable gain relaying:}
In this case we introduce an instantaneous power constraint. \textit{Anm.: A muss abh\"angig von $h_{SR} $ sein, um es kompensieren zu k\"onnen.}
\begin{align*}
	\mathcal{E}_{x,n}\bigl\{|S_R|^2\bigr\} &= \mathcal{E}_{x,n}\bigl\{A^2\bigl(|h_{SR}|^2|x|^2 + |n_R|^2\bigr)\bigr\} = \\ &= A^2\bigl(|h_{SR}|^2\mathcal{E}_S + \sigma_n^2\bigr) \overset{!}{=} \mathcal{E}_R \\ \rightarrow A^2 &= \frac{\mathcal{E}_R}{|h_{SR}|^2\mathcal{E}_S + \sigma_n^2}	 
\end{align*}
\begin{itemize}
	\item A is channel dependent
	\item Instantaneous transmit power is \underline{not} channel dependent 
\end{itemize}
\paragraph{b) Fixed gain relaying:} In this case, we introduce an average (with respect to the channel) power constraint
\begin{align*}
	\mathcal{E}\bigl\{|S_R|^2\bigr\} &= \mathcal{E}\bigl\{A^2\bigl(|h_{SR}|^2|x|^2 + |n_R|^2\bigr)\bigr\} = \\ &= A^2\bigl( \underbrace{\mathcal{E}\bigl\{|h_{SR}|^2\bigr\}}_{\sigma_{SR}^2}\mathcal{E}_S + \sigma_n^2\bigr) \overset{!}{=} \mathcal{E}_R \\ \rightarrow A^2 &= \frac{\mathcal{E}_R}{\mathcal{E}_S\sigma_{SR}^2 + \sigma_n^2}	 
\end{align*}
\begin{itemize}
	\item A is not channel dependent
	\item Instantaneous power of $S_R $ depends on channel and may actually vary widely
\end{itemize}
Equivalent SNR for variable gain AF relaying (inly relayed link)
\begin{align*}
	y_{D,2} = Ah_{RD}h_{SR}x + h_{RD}An_R + n_{D,2}
\end{align*}
\begin{align*}
	\text{SNR: }\quad \gamma_{eq}^{AF} &= \frac{A^2|h_{SR}|^2|h_{RD}|^2\mathcal{E}_S}{A^2|h_{RD}|^2\sigma_n^2 + \sigma_n^2} = \frac{\frac{\mathcal{E}_S}{\sigma_n^2}|h_{SR}|^2|h_{RD}|^2}{|h_{RD}|^2 + \frac{1}{\mathcal{E}_R}(|h_{SR}|^2\mathcal{E}_S + \sigma_n^2)} = \\ &= \frac{\frac{\mathcal{E}_S}{\sigma_n^2}|h_{SR}|^2\cdot\frac{\mathcal{E}_R}{\sigma_n^2}|h_{RD}|^2}{\frac{\mathcal{E}_R}{\sigma_n^2}|h_{RD}|^2 + \frac{\mathcal{E}_S}{\sigma_n^2}|h_{SR}|^2 + 1} = \\ &= \frac{\gamma_{SR}\gamma_{RD}}{\gamma_{SR} + \gamma_{RD} + 1}
\end{align*}
high SNR: $\gamma_{SR},\,\gamma_{RD} \gg 1 $
\begin{align}
	\boxed{\gamma_{eq}^{AF} = \frac{\gamma_{SR}\gamma_{RD}}{\gamma_{SR} + \gamma_{RD}}} \label{eq:Formel_01}
\end{align}
\textit{Anm.: Vgl. Formel \ref{eq:Formel_01} mit Berechnung zweier paralleler Widerst\"ande.}
Comparison with equivalent SNF of DF:
\begin{align}
	\boxed{\gamma_{eq}^{DF} = \min\bigl\{\gamma_{SR},\,\gamma_{RD}\bigr\}}\label{eq:Formel_02}
\end{align}
\underline{3 cases:}
\begin{align}
	&\text{a)}\quad \gamma_{SR} = \gamma_{RD} = \gamma\quad \rightarrow \quad \gamma_{eq}^{AF} = \frac{1}{2}\gamma = \frac{1}{2}\gamma_{eq}^{DF}\label{eq:Formel_03} \\
	&\text{b)}\quad \gamma_{SR} \gg \gamma_{RD} \quad \rightarrow \quad \gamma_{eq}^{AF} = \gamma_{RD} = \gamma_{eq}^{DF}\label{eq:Formel_04} \\
	&\text{c)}\quad \gamma_{SR} \ll \gamma_{RD} \quad \rightarrow \quad \gamma_{eq}^{AF} = \gamma_{SR} = \gamma_{eq}^{DF}\label{eq:Formel_05}
\end{align}
\textit{Anm.: F\"alle \ref{eq:Formel_04} und \ref{eq:Formel_05} sind am wahrscheinlichsten.}
Decision errors mostly occur if one of the two link SNRs is much smaller than the other. The probability, that both SNRs are small at the same time is much smaller, than the probability, that just one link SNR is small. 
\begin{itemize}
	\item[$\rightarrow$] $\gamma_{eq}^{AF} = \gamma_{eq}^{DF} $ holds most of the time
	\item[$\rightarrow$] AF relaying with variable gain has the same performance as DF relaying in high SNR, vgl Plot vom 07.02.13
\end{itemize}
\subsubsection{Buffer\,-\,aided DF Relaying}
\begin{itemize}
	\item For conventional relaying, the performance is always limited by the weaker (bottleneck) link since the relay has to immediately retransmit
	\begin{figure}[ht]
		\centering
		\psfrag{gamma_SR}[bl][bl]{$\gamma_{SR}$}
		\psfrag{gamma_RD}[bl][bl]{$\gamma_{RD}$}
		\psfrag{d_i_0}[bl][bl]{$d(i) = 0$}
		\psfrag{d_i_1}[bl][bl]{$d(i) = 1$}
		\psfrag{Phase 1}[bl][bl]{Phase 1}
		\psfrag{Phase 2}[bl][bl]{Phase 2}
		\includegraphics[scale=1.4]{Buffer_Aided_DF_Relaying}
		\caption{Buffer\,-\,aided DF Relaying}
		\label{fig:Buffer_Aided_DF_Relaying}
	\end{figure}
	\item In practice, the nodes in the network have buffers. Thus, we can use the stronger link and wait until the channel conditions of the weaker link have sufficiently improved.
	\item To avoid buffer over\,- or underflow at the relay, we demand that the average rate of the source relay channel (S\,-\,R) is equal to the average rate of the R\,-\,D channel
	\item We introduce a binary selection variable $d(i) $ for time slot $i = \{1,\,2,\,\ldots\} $, where:
	\begin{align*}
		d(i) = 0 \quad &\Rightarrow \quad \text{S transmits, R receives} \\
		d(i) = 1 \quad &\Rightarrow \quad \text{R transmits, D receives}
	\end{align*}
	\item Note: For conventional relaying we have $ d(1) = 0,\, d(2) = 1,\, d(3) = 0,\, d(4) = 1, \ldots $
	\item The average rate in the S\,-\,R link is:
	\begin{align*}
		R_{SR} = \frac{1}{N}\sum_{i=1}^{N}\bigl(1 - d(i)\bigr)\log_2\bigl(1 + \gamma_{SR}(i)\bigr)
	\end{align*}
	and that of the R\,-\,D link is:
	\begin{align*}
		R_{RD} = \frac{1}{N}\sum_{i=1}^{N}d(i)\log_2\bigl(1 + \gamma_{RD}(i)\bigr)
	\end{align*}
	where N denotes the total number of time slots.
	\item $\gamma_{SR}(i) $ and $ \gamma_{RD}(i) $ change from one time slot to the next following e.g. a Rayleigh distribution
	\item At the relay, we have the constraint $ R_{SR} = R_{RD} $ to avoid buffer over\,-\,/\,underflow
	\item To maximize the achievable throughput, we formulate an optimization problem:
	\begin{align*}
		&\underset{d(i) \forall i}{\max}\, R_{RD} \\ 
		\text{subject to:}\, &\text{C1:}\quad R_{RD} = R_{SR} \\
		&\text{C2:}\quad d(i) \in \{0,\,1\}
	\end{align*}
	\item For finite N, this problem is very difficult to solve.
	\item For infinte N, a simple solution exists.
	\item[$\rightarrow$] Solution can be found by Langrange method
	\item Solution $(\text{for }\, N \rightarrow \inf )$: The optimal $d(i) $ is given by:
	\begin{align*}
		d(i) = 
		\begin{cases}
			&1\quad \text{if}\quad \log_2\bigl( 1 + \gamma_{RD}(i)\bigr) \geq \rho \log_2\bigl( 1 + \gamma_{SR}(i)\bigr) \\
			&0\quad \text{otherwise} 
		\end{cases}
\end{align*}	 
	where $\rho $\, is a constant, that only depends on the statistics of $\gamma_{SR}(i) $ and $ \gamma_{RD}(i) $ and can be obtained from (numerical search needed):
	\begin{align*}
		\mathcal{E}_{\gamma_{SR}(i)}\bigl\{\bigl(1 - d(i)\bigr)\log_2\bigl(1 + \gamma_{SR}(i)\bigr)\bigr\} = \mathcal{E}_{\gamma_{RD}(i)}\bigl\{d(i)\cdot\log_2\bigl(1 + \gamma_{RD}(i)\bigr)\bigr\}
	\end{align*}
	\item Since always: the \glqq best\grqq\ of two links is selected, this scheme can achieve a diversity gain of $G_d = 2 $ (Rayleigh Fading)
\end{itemize}
\vspace*{3cm}
\begin{figure}[ht]
	\centering
	\includegraphics[scale=1.4]{Ende}
\end{figure}
\end{document}
